{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Total PNB using XGBoost and AGBoost Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we aim to build predictive models for total PNB using XGBoost and AGBoost algorithms. We will preprocess the data, handle outliers, split the data into training and testing sets, and compare the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "First, we need to install the necessary packages. If they are already installed, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AGBoost (requires authentication)\n",
    "!pip install agboost --no-cache-dir --extra-index-url https://${ARTIFACTORY_USERNAME}:${ARTIFACTORY_TOKEN}@repo.artifactory-dogen.group.echonet/artifactory/api/pypi/ap12261-pypi-local/simple\n",
    "\n",
    "# Install other required packages\n",
    "!pip install xgboost matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import agboost as agb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "We load the dataset and perform initial preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../df_er.csv', sep=',', encoding='latin-1')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['CCLIKPI', 'flux_annuel', 'MFTCREP', 'nb_op_annuel', 'code_sous_marche',\n",
    "              'pnb_annuel', 'pnb_an_rep', 'MACMPROF', 'CNOUVSEG', 'CTYPCLI', 'CENSEIGNE', 'section'], axis=1)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(['total_pnb'], axis=1)\n",
    "y = df['total_pnb']\n",
    "\n",
    "# Get feature names\n",
    "feat = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers and Data Cleaning\n",
    "\n",
    "We identify and remove outliers based on the Interquartile Range (IQR) method. We also transform certain variables to categorical types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "target = 'total_pnb'\n",
    "\n",
    "# Calculate the IQR to identify outliers\n",
    "Q1 = df[target].quantile(0.25)\n",
    "Q3 = df[target].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the bounds for identifying outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter outliers and negative target values\n",
    "outliers_negative = df[(df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0)]\n",
    "non_outliers_og = df[~((df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0))]\n",
    "\n",
    "# Remove entries with AGE less than 18\n",
    "non_outliers_og = non_outliers_og[non_outliers_og['AGE'] >= 18]\n",
    "\n",
    "# Convert certain numerical variables to categorical\n",
    "non_outliers_og['CMOTENT1'] = non_outliers_og['CMOTENT1'].astype(str) + '_cat'\n",
    "non_outliers_og['Cartes de Paiement'] = non_outliers_og['Cartes de Paiement'].astype(str) + '_cat'\n",
    "non_outliers_og['carte_business'] = non_outliers_og['carte_business'].astype(str) + '_cat'\n",
    "non_outliers_og['crédit équipement'] = non_outliers_og['crédit équipement'].astype(str) + '_cat'\n",
    "non_outliers_og['CUODR'] = non_outliers_og['CUODR'].astype(str) + '_cat'\n",
    "\n",
    "# Update the dataset\n",
    "df = non_outliers_og.copy()\n",
    "X = df.copy()\n",
    "y = df['total_pnb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_df, test_df, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Filter out entries with non-positive total_pnb in training set\n",
    "print(f\"Training set size before filtering: {len(train_df)}\")\n",
    "train_df = train_df[train_df['total_pnb'] > 0]\n",
    "print(f\"Training set size after filtering: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing with AGBoost\n",
    "\n",
    "We preprocess the data using AGBoost's `DataPreprocessing` class, which includes imputing missing values and target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "data_proc = agb.DataPreprocessing(\n",
    "    label_name='total_pnb',\n",
    "    feature_names=feat,\n",
    "    impute_na=True,\n",
    "    threshold_na=0.05,\n",
    "    target_encode=True,\n",
    "    threshold_cat=0,\n",
    "    encoding_algo=\"rank\"\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_proc = data_proc.fit_transform(data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training set\n",
    "print(\"Missing values in the training set:\")\n",
    "print(train_proc.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Table and Features Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View imputation table\n",
    "data_proc.imputation_table\n",
    "\n",
    "# Get updated feature names\n",
    "data_proc.features_cache\n",
    "feature_names = data_proc.features_cache['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "test_proc = data_proc.transform(data=test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model: XGBoost\n",
    "\n",
    "We train an XGBoost model as a benchmark to compare against the AGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df = train_proc\n",
    "df_ohe = pd.get_dummies(df[feature_names], prefix_sep=\"_ohe_\")\n",
    "df_xgb = xgb.DMatrix(data=df_ohe.values, label=df['total_pnb'].values, feature_names=list(df_ohe.columns))\n",
    "train_xgb = df_xgb\n",
    "\n",
    "df = test_proc\n",
    "df_ohe = pd.get_dummies(df[feature_names], prefix_sep=\"_ohe_\")\n",
    "df_xgb = xgb.DMatrix(data=df_ohe.values, label=df['total_pnb'].values, feature_names=list(df_ohe.columns))\n",
    "test_xgb = df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation to Find Optimal Number of Boosting Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "e, s, md, b = 0.02, 0.2, 3, 1000  # Hyperparameters found by AGBoost\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': e,\n",
    "    'seed': 1989,\n",
    "    'subsample': s,\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': md,\n",
    "    'colsample_bytree': 1\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "xgb_cv = xgb.cv(\n",
    "    params=xgb_params,\n",
    "    dtrain=train_xgb,\n",
    "    num_boost_round=b,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=0\n",
    ")\n",
    "\n",
    "# Plot RMSE vs. number of boosting rounds\n",
    "xgb_cv.iloc[:, [0, 2]].plot(xlabel='Number of Boosting Rounds', ylabel='RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Optimized XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal number of trees\n",
    "best_ntrees = agb.find_best_ntrees(eval_history_df=xgb_cv, maximize=False)\n",
    "print(f\"Optimal number of trees: {best_ntrees}\")\n",
    "\n",
    "# Train the final XGBoost model\n",
    "xgb_final = xgb.train(params=xgb_params, dtrain=train_xgb, num_boost_round=best_ntrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "df = train_df\n",
    "y_actual_train = df['total_pnb']\n",
    "y_pred_train = xgb_final.predict(data=train_xgb)\n",
    "rmse_xgb_train = mean_squared_error(y_actual_train, y_pred_train, squared=False)\n",
    "\n",
    "# Evaluate on test set\n",
    "df = test_df\n",
    "y_actual_test = df['total_pnb']\n",
    "y_pred_test = xgb_final.predict(data=test_xgb)\n",
    "rmse_xgb_test = mean_squared_error(y_actual_test, y_pred_test, squared=False)\n",
    "r2_xgb_test = r2_score(y_actual_test, y_pred_test)\n",
    "\n",
    "# Compile model scores\n",
    "score = pd.DataFrame({'model': ['XGBoost'], 'RMSE train': [rmse_xgb_train], 'RMSE test': [rmse_xgb_test], 'r2 test:': [r2_xgb_test]})\n",
    "model_scores = score\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Residuals for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_actual_test - y_pred_test\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "data = {'res': residuals, 'Actual': y_actual_test, 'Predicted': y_pred_test}\n",
    "dff = pd.DataFrame(data)\n",
    "\n",
    "# Plot residuals and predictions\n",
    "plt.figure(figsize=(28, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot(dff['res'], shade=True)\n",
    "plt.title('Residuals Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"res\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Residuals vs. Actual')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"Predicted\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Predicted vs. Actual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize AGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGBoost hyperparameters\n",
    "e, s, md, b = 0.02, 0.8, 3, 1000\n",
    "\n",
    "params_xgbooster = {\n",
    "    'eta': e,\n",
    "    'seed': 1989,\n",
    "    'subsample': s,\n",
    "    'booster': 'gbtree',\n",
    "    'colsample_bytree': 1\n",
    "}\n",
    "\n",
    "# Initialize AGBoost model\n",
    "agbooster = agb.AGBooster(\n",
    "    label_name='total_pnb',\n",
    "    feature_names=feature_names,\n",
    "    params_xgbooster=params_xgbooster,\n",
    "    prefix_sep=\"_ohe_\",\n",
    "    target_encoding_dict=data_proc.target_encoding_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with AGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select individual features\n",
    "agbooster.select_features(\n",
    "    data=train_proc,\n",
    "    cv_imp=True,\n",
    "    threshold=5,  # Note: Threshold is set high for illustration\n",
    "    num_boost_round_min=51,\n",
    "    num_boost_round_max=200,\n",
    "    num_boost_round_start=200,\n",
    "    early_stopping_rounds=10,\n",
    "    eta_start=0.1\n",
    ")\n",
    "\n",
    "# View feature importance\n",
    "agbooster.feature_importance['importance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the list of selected features with a new threshold\n",
    "agbooster.update_selected_features(threshold=1.5)\n",
    "agbooster.selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Univariate GAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit univariate GAM\n",
    "agbooster.fit_univariate_gam(\n",
    "    data=train_proc,\n",
    "    num_boost_round_min=51,\n",
    "    num_boost_round_max=200,\n",
    "    num_boost_round_start=200,\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate AGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "df = test_proc\n",
    "y_actual = df['total_pnb']\n",
    "y_pred = agbooster.gam_univariate_model.predict(data=df)\n",
    "\n",
    "# Calculate RMSE and R2\n",
    "rmse_gam_univar_test = mean_squared_error(y_actual, y_pred, squared=False)\n",
    "r2_gam_univar_test = r2_score(y_actual, y_pred)\n",
    "\n",
    "# Update model scores\n",
    "score = pd.DataFrame({'model': ['GAM Univariate'], 'RMSE train': [np.nan], 'RMSE test': [rmse_gam_univar_test], 'r2 test:': [r2_gam_univar_test]})\n",
    "model_scores = pd.concat([model_scores, score], ignore_index=True)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Residuals for AGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_actual - y_pred\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "data = {'res': residuals, 'Actual': y_actual, 'Predicted': y_pred}\n",
    "dff = pd.DataFrame(data)\n",
    "\n",
    "# Plot residuals and predictions\n",
    "plt.figure(figsize=(28, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot(dff['res'], shade=True)\n",
    "plt.title('Residuals Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"res\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Residuals vs. Actual')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"Predicted\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Predicted vs. Actual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Model Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the GAM model\n",
    "model = agbooster.gam_univariate_model\n",
    "\n",
    "# Create a table of coefficients\n",
    "table = pd.DataFrame([{\n",
    "    'variable': 'Intercept',\n",
    "    'group': '',\n",
    "    'coef': model.gam_model['intercept'],\n",
    "    'nb': '',\n",
    "    'min': '',\n",
    "    'max': ''\n",
    "}])\n",
    "\n",
    "# Append coefficients for each feature\n",
    "for feature in model.gam_model[\"coefs_stump\"].keys():\n",
    "    table_int = pd.DataFrame(model.gam_model[\"coefs_stump\"][feature][['group', 'coef', 'nb', 'min', 'max']])\n",
    "    table_int['variable'] = feature\n",
    "    table = pd.concat([table, table_int], axis=0)\n",
    "\n",
    "# Rearrange columns\n",
    "table = table[['variable', 'group', 'coef', 'nb', 'min', 'max']]\n",
    "\n",
    "print(f\"Total number of coefficients: {table.shape[0]}\")\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
