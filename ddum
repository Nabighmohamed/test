import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# Sample DataFrame
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Select the target variable with missing values
target_variable = 'Target'

# Drop other variables with missing values
df = df.drop(columns=['Feature_2'])

# Create dataset for training (without missing target) and predicting (with missing target)
train_data = df.dropna(subset=[target_variable])
predict_data = df[df[target_variable].isnull()]

# Split features and target
X_train = train_data.drop(columns=[target_variable])
y_train = train_data[target_variable]

# One-hot encode categorical variables
X_train = pd.get_dummies(X_train, drop_first=True)
predict_features = pd.get_dummies(predict_data.drop(columns=[target_variable]), drop_first=True)

# Align columns of prediction set to match the training set
predict_features = predict_features.reindex(columns=X_train.columns, fill_value=0)

# Add constant term for intercept
X_train_sm = sm.add_constant(X_train)

# Train the regression model and get summary
model = sm.OLS(y_train, X_train_sm)
results = model.fit()
print(results.summary())

# Select features with p-value < 0.10
significant_features = results.pvalues[results.pvalues < 0.10].index.tolist()
if 'const' in significant_features:
    significant_features.remove('const')

# Train a new regression model using significant features
X_train_sig = X_train[significant_features]
X_train_sig_sm = sm.add_constant(X_train_sig)
model_sig = sm.OLS(y_train, X_train_sig_sm)
results_sig = model_sig.fit()
print(results_sig.summary())

# Predict missing values using the significant features model
predict_features_sig = predict_features[significant_features]
predict_features_sig_sm = sm.add_constant(predict_features_sig)
y_pred = results_sig.predict(predict_features_sig_sm)
df.loc[df[target_variable].isnull(), target_variable] = y_pred

# Print the DataFrame with imputed values
print("\nDataFrame with Imputed Values:\n", df)



























import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# Sample DataFrame
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Select the target variable with missing values
target_variable = 'Target'

# Drop other variables with missing values
df = df.drop(columns=['Feature_2'])

# Create dataset for training (without missing target) and predicting (with missing target)
train_data = df.dropna(subset=[target_variable])
predict_data = df[df[target_variable].isnull()]

# Split features and target
X_train = train_data.drop(columns=[target_variable])
y_train = train_data[target_variable]

# One-hot encode categorical variables
X_train = pd.get_dummies(X_train, drop_first=True)
predict_features = pd.get_dummies(predict_data.drop(columns=[target_variable]), drop_first=True)

# Align columns of prediction set to match the training set
predict_features = predict_features.reindex(columns=X_train.columns, fill_value=0)

# Train the regression model
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predict missing values
y_pred = regressor.predict(predict_features)
df.loc[df[target_variable].isnull(), target_variable] = y_pred

# Model summary using statsmodels
X_train_sm = sm.add_constant(X_train)  # Add constant term for intercept
model = sm.OLS(y_train, X_train_sm)
results = model.fit()

# Print the summary
print(results.summary())

# Print the DataFrame with imputed values
print("\nDataFrame with Imputed Values:\n", df)














































import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame (replace with your actual dataset)
data = {
    'Category_1': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Category_2': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z', 'X'],
    'Value': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    'Target': [15.1, 25.3, 35.2, 45.5, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Value']),
        ('cat', OneHotEncoder(), ['Category_1', 'Category_2'])
    ])

# Define the model pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))
])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Model MSE: {mse:.4f}')
print(f'Model R-squared: {r2:.4f}')
