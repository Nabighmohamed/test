Bonjour Sabrina,

Merci encore pour le temps que vous m'avez accordé aujourd'hui. J'ai apprécié notre discussion et l'opportunité de partager mon parcours. Comme demandé, je vous envoie ci-joint mon CV.

Cordialement,



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score
from xgboost import XGBClassifier
from deap import base, creator, tools, algorithms

# Generate a synthetic dataset
np.random.seed(0)
data = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
data['target'] = np.random.randint(0, 2, 1000)  # Binary target variable

# Split the data into features and target
X = data.drop(columns='target')
y = data['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define the fitness function to maximize F1 score
def fitness_function(individual):
    n_estimators, max_depth, learning_rate = individual
    n_estimators = int(n_estimators)
    max_depth = int(max_depth)
    learning_rate = float(learning_rate)
    
    # Ensure hyperparameters are within valid ranges
    if n_estimators < 1 or max_depth < 1 or learning_rate <= 0 or learning_rate > 1:
        return -1.0,  # Invalid fitness score for invalid parameter ranges

    model = XGBClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=0
    )
    
    # Evaluate the model using cross-validation with F1 score
    try:
        scores = cross_val_score(model, X_train, y_train, cv=3, scoring='f1')
        return scores.mean(),
    except Exception as e:
        # Handle cases where the model fails to train
        return -1.0,  # Invalid fitness score

# Define the GA components
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_int", np.random.randint, 50, 200)  # Range for n_estimators
toolbox.register("attr_int2", np.random.randint, 1, 10)   # Range for max_depth
toolbox.register("attr_float", np.random.uniform, 0.01, 1.0)  # Range for learning_rate
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_int, toolbox.attr_int2, toolbox.attr_float), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", fitness_function)
toolbox.register("mate", tools.cxBlend, alpha=0.5)

# Correct mutation function to ensure valid values
def mutate_individual(individual, low, up, indpb):
    size = len(individual)
    for i in range(size):
        if np.random.rand() < indpb:
            if i == 2:  # learning_rate
                individual[i] = np.clip(np.random.uniform(0.01, 1.0), low[i], up[i])
            else:
                individual[i] = np.clip(int(np.random.randint(low[i], up[i])), low[i], up[i])
    return individual,

toolbox.register("mutate", mutate_individual, low=[50, 1, 0.01], up=[200, 10, 1.0], indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# Run the Genetic Algorithm
population = toolbox.population(n=20)  # Small population for simplicity
NGEN = 10
CXPB, MUTPB = 0.5, 0.2
f1_scores = []

for gen in range(NGEN):
    offspring = algorithms.varAnd(population, toolbox, CXPB, MUTPB)
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    
    population = toolbox.select(offspring, k=len(population))
    
    # Record the best F1 score for this generation
    best_f1 = tools.selBest(population, k=1)[0].fitness.values[0]
    f1_scores.append(best_f1)

# Extract the best individual
best_individual = tools.selBest(population, k=1)[0]
best_params = [int(best_individual[0]), int(best_individual[1]), best_individual[2]]
print("Best hyperparameters found:", best_params)

# Train the final model with the best hyperparameters
xgb = XGBClassifier(
    n_estimators=best_params[0],
    max_depth=best_params[1],
    learning_rate=best_params[2],
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=0
)
xgb.fit(X_train, y_train)

# Predict the target values for the test set
y_pred = xgb.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate and print the classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Plot the evolution of the F1 score through generations
plt.plot(range(1, NGEN+1), f1_scores, marker='o')
plt.xlabel('Generation')
plt.ylabel('F1 Score')
plt.title('Evolution of F1 Score through Generations')
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score
from xgboost import XGBClassifier
from deap import base, creator, tools, algorithms

# Generate a synthetic dataset
np.random.seed(0)
data = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
data['target'] = np.random.randint(0, 2, 1000)  # Binary target variable

# Split the data into features and target
X = data.drop(columns='target')
y = data['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define the fitness function to maximize F1 score
def fitness_function(individual):
    n_estimators, max_depth, learning_rate = individual
    n_estimators = int(n_estimators)
    max_depth = int(max_depth)
    learning_rate = float(learning_rate)
    
    # Ensure hyperparameters are within valid ranges
    if n_estimators < 1 or max_depth < 1 or learning_rate <= 0 or learning_rate > 1:
        return -1.0,  # Invalid fitness score for invalid parameter ranges

    model = XGBClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=0
    )
    
    # Evaluate the model using cross-validation with F1 score
    try:
        scores = cross_val_score(model, X_train, y_train, cv=3, scoring='f1')
        return scores.mean(),
    except Exception as e:
        # Handle cases where the model fails to train
        return -1.0,  # Invalid fitness score

# Define the GA components
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_int", np.random.randint, 50, 200)  # Range for n_estimators
toolbox.register("attr_int2", np.random.randint, 1, 10)   # Range for max_depth
toolbox.register("attr_float", np.random.uniform, 0.01, 1.0)  # Range for learning_rate
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_int, toolbox.attr_int2, toolbox.attr_float), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", fitness_function)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutPolynomialBounded, low=[50, 1, 0.01], up=[200, 10, 1.0], eta=0.1, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# Run the Genetic Algorithm
population = toolbox.population(n=20)  # Small population for simplicity
NGEN = 10
CXPB, MUTPB = 0.5, 0.2
f1_scores = []

for gen in range(NGEN):
    offspring = algorithms.varAnd(population, toolbox, CXPB, MUTPB)
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    
    population = toolbox.select(offspring, k=len(population))
    
    # Record the best F1 score for this generation
    best_f1 = tools.selBest(population, k=1)[0].fitness.values[0]
    f1_scores.append(best_f1)

# Extract the best individual
best_individual = tools.selBest(population, k=1)[0]
best_params = [int(best_individual[0]), int(best_individual[1]), best_individual[2]]
print("Best hyperparameters found:", best_params)

# Train the final model with the best hyperparameters
xgb = XGBClassifier(
    n_estimators=best_params[0],
    max_depth=best_params[1],
    learning_rate=best_params[2],
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=0
)
xgb.fit(X_train, y_train)

# Predict the target values for the test set
y_pred = xgb.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate and print the classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Plot the evolution of the F1 score through generations
plt.plot(range(1, NGEN+1), f1_scores, marker='o')
plt.xlabel('Generation')
plt.ylabel('F1 Score')
plt.title('Evolution of F1 Score through Generations')
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from itertools import combinations
import seaborn as sns

# Sample data for demonstration
Y = np.random.rand(100) * 10  # Random float numbers between 0 and 10
Y.sort()
min_number_per_seg = 25
N = 4  # Number of segments

# Create a DataFrame with the sample data
df = pd.DataFrame({'value': Y})

def is_valid_segmentation(segments, min_count):
    # Count numbers in each segment
    segment_counts = [np.sum((Y >= segments[i]) & (Y < segments[i+1])) for i in range(len(segments) - 1)]
    segment_counts.append(np.sum(Y >= segments[-1]))  # For the last segment
    return all(count >= min_count for count in segment_counts)

# Create a list of valid segmentations and store in DataFrame
valid_segments = []
segmentation_results = pd.DataFrame({'value': Y})

for idx, split_points in enumerate(combinations(Y, N-1)):  # We need N-1 split points for N segments
    if is_valid_segmentation(split_points, min_number_per_seg):
        valid_segments.append(split_points)
        # Create a new column for each valid segmentation
        segmentation_results[f'segment_{idx}'] = np.digitize(Y, bins=split_points, right=False)

# If no valid segments, exit early
if not valid_segments:
    print("No valid segmentations found. Exiting.")
    exit()

# Set up the plot
fig, ax = plt.subplots()
sns.set(style="whitegrid")

# Initialize the plot elements
def init():
    ax.clear()
    ax.set_xlim(np.min(Y), np.max(Y))
    ax.set_ylim(0, 20)  # Adjust ylim based on data
    return []

# Animation function
def animate(i):
    ax.clear()
    segments = valid_segments[i]
    df['segment'] = np.digitize(df['value'], bins=segments, right=False)
    
    # Plot each segment as a histogram using Seaborn
    sns.histplot(df, x='value', hue='segment', element="step", palette='tab10', ax=ax)
    
    ax.set_title(f'Valid Segmentation {i+1}')
    return []

# Create the animation
ani = FuncAnimation(fig, animate, init_func=init, frames=len(valid_segments), interval=200, blit=False)

# Save the animation as a GIF file
ani.save('segmented_histogram_animation.gif', writer='pillow')

plt.show()

# Save the segmentation results DataFrame to a CSV file
segmentation_results.to_csv('segmentation_results.csv', index=False)





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
from xgboost import XGBClassifier
from deap import base, creator, tools, algorithms

# Load your dataset
np.random.seed(0)
data = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
data['target'] = np.random.randint(0, 2, 1000)  # Binary target variable

# Split the data into features and target
X = data.drop(columns='target')
y = data['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define the fitness function
def fitness_function(individual):
    n_estimators, max_depth, learning_rate = individual
    n_estimators = int(n_estimators)
    max_depth = int(max_depth)
    learning_rate = float(learning_rate)
    
    model = XGBClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=0
    )
    
    # Evaluate the model using cross-validation
    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
    return scores.mean(),

# Define the GA components
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_int", np.random.randint, 50, 200)  # Range for n_estimators
toolbox.register("attr_int2", np.random.randint, 1, 10)   # Range for max_depth
toolbox.register("attr_float", np.random.uniform, 0.01, 0.3)  # Range for learning_rate
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_int, toolbox.attr_int2, toolbox.attr_float), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", fitness_function)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutPolynomialBounded, low=[50, 1, 0.01], up=[200, 10, 0.3], eta=0.1, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# Run the Genetic Algorithm
population = toolbox.population(n=10)  # Smaller population for simplicity
NGEN = 5
CXPB, MUTPB = 0.5, 0.2

for gen in range(NGEN):
    offspring = algorithms.varAnd(population, toolbox, CXPB, MUTPB)
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    
    population = toolbox.select(offspring, k=len(population))

# Extract the best individual
best_individual = tools.selBest(population, k=1)[0]
best_params = [int(best_individual[0]), int(best_individual[1]), best_individual[2]]
print("Best hyperparameters found:", best_params)

# Train the final model with the best hyperparameters
xgb = XGBClassifier(
    n_estimators=best_params[0],
    max_depth=best_params[1],
    learning_rate=best_params[2],
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=0
)
xgb.fit(X_train, y_train)

# Predict the target values for the test set
y_pred = xgb.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate and print the classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(xgb.feature_importances_)), xgb.feature_importances_)
plt.yticks(range(len(X.columns)), X.columns)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance in XGBClassifier")
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score
from xgboost import XGBClassifier
from deap import base, creator, tools, algorithms

# Load your dataset
np.random.seed(0)
data = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
data['target'] = np.random.randint(0, 2, 1000)  # Binary target variable

# Split the data into features and target
X = data.drop(columns='target')
y = data['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define the fitness function
def fitness_function(params):
    n_estimators, max_depth, learning_rate, subsample, colsample_bytree = params
    n_estimators = int(n_estimators)
    max_depth = int(max_depth)
    
    model = XGBClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=0
    )
    
    # Evaluate the model using cross-validation
    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
    return scores.mean(),

# Define the GA components
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_float", np.random.rand)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=5)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", fitness_function)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutPolynomialBounded, low=0, up=1, eta=0.1, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# Run the Genetic Algorithm
population = toolbox.population(n=50)
NGEN = 20
CXPB, MUTPB = 0.5, 0.2

for gen in range(NGEN):
    offspring = algorithms.varAnd(population, toolbox, CXPB, MUTPB)
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    
    population = toolbox.select(offspring, k=len(population))

# Extract the best individual
best_individual = tools.selBest(population, k=1)[0]
best_params = [int(best_individual[0]), int(best_individual[1]), best_individual[2], best_individual[3], best_individual[4]]
print("Best hyperparameters found:", best_params)

# Train the final model with the best hyperparameters
xgb = XGBClassifier(
    n_estimators=best_params[0],
    max_depth=best_params[1],
    learning_rate=best_params[2],
    subsample=best_params[3],
    colsample_bytree=best_params[4],
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=0
)
xgb.fit(X_train, y_train)

# Predict the target values for the test set
y_pred = xgb.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate and print the classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(xgb.feature_importances_)), xgb.feature_importances_)
plt.yticks(range(len(X.columns)), X.columns)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance in XGBClassifier")
plt.show()






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from deap import base, creator, tools, algorithms
import random

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters and range for min_samples_split
num_clusters = 10

# Define the fitness function
def fitness_function(individual):
    min_samples_split = int(individual[0])
    
    # Initialize list to store weighted F1 scores for each feature
    weighted_f1_scores = []

    # Iterate over each feature to segment it using the target variable
    for feature_index in range(data.shape[1] - 1):  # Exclude the target column
        feature_name = data.columns[feature_index]
        X_feature = data[[feature_name]].values.reshape(-1, 1)
        y = data['target'].values.reshape(-1, 1)

        # Segment the feature using the target variable
        dt = DecisionTreeRegressor(min_samples_split=min_samples_split, max_leaf_nodes=num_clusters, random_state=0)
        dt.fit(y, X_feature)
        predicted_X = dt.predict(y)

        # Convert the continuous segment predictions into discrete labels
        label_encoder = LabelEncoder()
        segment_labels = label_encoder.fit_transform(predicted_X)

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(
            data.drop(columns='target'), segment_labels, test_size=0.3, random_state=0
        )

        # Use all features (excluding the target variable) to predict these segments
        xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
        xgb.fit(X_train, y_train)

        # Predict the segments on the test set
        predicted_segments = xgb.predict(X_test)

        # Generate a classification report
        report = classification_report(y_test, predicted_segments, output_dict=True)
        weighted_f1_scores.append(report['weighted avg']['f1-score'])

    # Return the average weighted F1 score as the fitness score (negated to minimize)
    return -np.mean(weighted_f1_scores),

# Define the GA settings
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)
toolbox = base.Toolbox()
toolbox.register("attr_int", random.randint, 2, 20)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_int, n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", fitness_function)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutPolynomialBounded, low=2, up=20, eta=0.1, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# Run the Genetic Algorithm
population = toolbox.population(n=10)
NGEN = 20
CXPB, MUTPB = 0.5, 0.2

for gen in range(NGEN):
    offspring = algorithms.varAnd(population, toolbox, CXPB, MUTPB)
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    population = toolbox.select(offspring, k=len(population))

# Extract and print the best solution
best_individual = tools.selBest(population, k=1)[0]
best_min_samples_split = int(best_individual[0])
print("Best min_samples_split is:", best_min_samples_split)





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from itertools import combinations
import seaborn as sns

# Sample data for demonstration
Y = np.random.rand(100) * 10  # Random float numbers between 0 and 10
Y.sort()
min_number_per_seg = 5
N = 4  # Number of segments

# Create a DataFrame with the sample data
df = pd.DataFrame({'value': Y})

def is_valid_segmentation(segments, min_count):
    # Count numbers in each segment
    segment_counts = [np.sum((Y >= segments[i]) & (Y < segments[i+1])) for i in range(len(segments) - 1)]
    segment_counts.append(np.sum(Y >= segments[-1]))  # For the last segment
    return all(count >= min_count for count in segment_counts)

# Create a list of valid segmentations
valid_segments = []
for split_points in combinations(Y, N-1):  # We need N-1 split points for N segments
    if is_valid_segmentation(split_points, min_number_per_seg):
        valid_segments.append(split_points)

# If no valid segments, exit early
if not valid_segments:
    print("No valid segmentations found. Exiting.")
    exit()

# Set up the plot
fig, ax = plt.subplots()
sns.set(style="whitegrid")

# Initialize the plot elements
def init():
    ax.clear()
    ax.set_xlim(np.min(Y), np.max(Y))
    ax.set_ylim(0, 20)  # Adjust ylim based on data
    return []

# Animation function
def animate(i):
    ax.clear()
    segments = valid_segments[i]
    df['segment'] = np.digitize(df['value'], bins=segments, right=False)
    
    # Plot each segment as a histogram using Seaborn
    sns.histplot(df, x='value', hue='segment', element="step", palette='tab10', ax=ax)
    
    ax.set_title(f'Valid Segmentation {i+1}')
    return []

# Create the animation
ani = FuncAnimation(fig, animate, init_func=init, frames=len(valid_segments), interval=200, blit=False)

# Save the animation as a GIF file
ani.save('segmented_histogram_animation.gif', writer='pillow')

plt.show()







import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Sample data for demonstration
Y = np.random.rand(100) * 10  # Random float numbers between 0 and 10
Y.sort()
min_number_per_seg = 25


def is_valid_segmentation(S1, S2, min_count):
    # Count numbers in each segment
    count1 = np.sum(Y < S1)
    count2 = np.sum((Y >= S1) & (Y < S2))
    count3 = np.sum(Y >= S2)
    return (count1 >= min_count) and (count2 >= min_count) and (count3 >= min_count)

# Create a list of valid segmentations
valid_segments = []
for S1 in Y:
    for S2 in Y:
        if S1 < S2:
            if is_valid_segmentation(S1, S2, min_number_per_seg):
                valid_segments.append((S1, S2))

# Set up the plot
fig, ax = plt.subplots()
line1, = ax.plot([], [], 'r-', label='Segment 1')
line2, = ax.plot([], [], 'g-', label='Segment 2')
line3, = ax.plot([], [], 'b-', label='Segment 3')
ax.legend()

# Initialize the plot elements
def init():
    ax.set_xlim(np.min(Y), np.max(Y))
    ax.set_ylim(0, len(Y))
    line1.set_data([], [])
    line2.set_data([], [])
    line3.set_data([], [])
    return line1, line2, line3

# Animation function
def animate(i):
    S1, S2 = valid_segments[i]
    Y1 = Y[Y < S1]
    Y2 = Y[(Y >= S1) & (Y < S2)]
    Y3 = Y[Y >= S2]
    line1.set_data(Y1, np.zeros_like(Y1) + 1)
    line2.set_data(Y2, np.zeros_like(Y2) + 2)
    line3.set_data(Y3, np.zeros_like(Y3) + 3)
    return line1, line2, line3

# Create the animation
ani = FuncAnimation(fig, animate, init_func=init, frames=len(valid_segments), interval=200, blit=True)

plt.show()



# Using conda
conda install -c conda-forge imagemagick

# Using pip (might require ImageMagick to be installed on your system)
pip install imagemagick

pip install pillow


import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# This line is for static plots in Jupyter Notebooks
%matplotlib inline

# Set up the figure, the axis, and the plot element we want to animate
fig, ax = plt.subplots()
x = np.linspace(0, 2 * np.pi, 200)
line, = ax.plot(x, np.sin(x))

# Initialization function: plot the background of each frame
def init():
    line.set_ydata([np.nan] * len(x))
    return line,

# Animation function: this is called sequentially
def animate(i):
    line.set_ydata(np.sin(x + i / 10))  # Update the data
    return line,

# Call the animator
ani = FuncAnimation(fig, animate, init_func=init, frames=100, interval=20, blit=True)

# Save the animation as a GIF file
ani.save('sine_wave_animation.gif', writer='pillow')

plt.show()





import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# This line is for static plots in Jupyter Notebooks
%matplotlib inline

# Set up the figure, the axis, and the plot element we want to animate
fig, ax = plt.subplots()
x = np.linspace(0, 2 * np.pi, 200)
line, = ax.plot(x, np.sin(x))

# Initialization function: plot the background of each frame
def init():
    line.set_ydata([np.nan] * len(x))
    return line,

# Animation function: this is called sequentially
def animate(i):
    line.set_ydata(np.sin(x + i / 10))  # Update the data
    return line,

# Call the animator
ani = FuncAnimation(fig, animate, init_func=init, frames=100, interval=20, blit=True)

# Save the animation as an MP4 file or GIF (optional)
ani.save('sine_wave_animation.mp4', writer='ffmpeg')

plt.show()




import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# This line is important for Jupyter Notebooks
%matplotlib notebook

# Set up the figure, the axis, and the plot element we want to animate
fig, ax = plt.subplots()
x = np.linspace(0, 2 * np.pi, 200)
line, = ax.plot(x, np.sin(x))

# Initialization function: plot the background of each frame
def init():
    line.set_ydata([np.nan] * len(x))
    return line,

# Animation function: this is called sequentially
def animate(i):
    line.set_ydata(np.sin(x + i / 10))  # Update the data
    return line,

# Call the animator
ani = FuncAnimation(fig, animate, init_func=init, frames=100, interval=20, blit=True)

# Display the animation
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Set up the figure, the axis, and the plot element we want to animate
fig, ax = plt.subplots()
x = np.linspace(0, 2 * np.pi, 200)
line, = ax.plot(x, np.sin(x))

# Initialization function: plot the background of each frame
def init():
    line.set_ydata([np.nan] * len(x))
    return line,

# Animation function: this is called sequentially
def animate(i):
    line.set_ydata(np.sin(x + i / 10))  # Update the data
    return line,

# Call the animator
ani = FuncAnimation(fig, animate, init_func=init, frames=100, interval=20, blit=True)

# Display the animation
plt.show()







Objet : Demande d'approbation pour l'email à Sabrina

Bonjour Solène,

J'espère que vous allez bien.

Je souhaite reporter la discussion RH avec Sabrina et j'ai préparé deux options d'email. Pourriez-vous s'il vous plaît examiner ces messages et me dire lequel vous semble le plus approprié ?

Option 1 :

Objet : Report de la discussion RH

Bonjour Sabrina,

J'espère que vous allez bien.

Pourrions-nous reporter la discussion RH à lundi prochain ?

Merci beaucoup !

Cordialement,

Mohamed

Option 2 :

Bonjour Sabrina,

J'espère que vous allez bien.

Je vous serais très reconnaissant si nous pouvions reporter la discussion RH à la semaine prochaine, à un moment qui vous conviendrait. Merci beaucoup pour votre compréhension et votre flexibilité.

Merci encore pour votre aide.

Cordialement,

Mohamed






# Create a continuous float variable y in the DataFrame
df_float = pd.DataFrame({'y': np.linspace(1.0, 100.0, 100)})

# Adjusting the function to handle float values in y
def create_segmentation_float(y, boundaries):
    y_seg = np.zeros_like(y, dtype=int)
    for i, boundary in enumerate(boundaries):
        y_seg[(y > boundaries[i-1]) & (y <= boundary)] = i if i > 0 else 0
    y_seg[y > boundaries[-1]] = len(boundaries)
    return y_seg

def general_segmentations_float(df, var_name, num_segs, min_number):
    y = df[var_name].values
    max_y = y[-1]
    initial_segments = list(np.linspace(min(y), max_y, num_segs, endpoint=False)[1:])
    
    segmentations = pd.DataFrame({var_name: y})
    
    def plot_segmentation(y, segmentation, boundaries, step):
        plt.figure(figsize=(10, 6))
        plt.scatter(y, segmentation, c=segmentation, cmap='viridis', alpha=0.7, edgecolors='w')
        plt.title(f'Segmentation at Step {step}: Boundaries {boundaries}')
        plt.xlabel(var_name)
        plt.ylabel('Segment')
        plt.grid(True)
        plt.show()
    
    step = 1
    
    for idx in reversed(range(num_segs - 1)):
        for i in np.linspace(initial_segments[idx], max_y, 50):  # finer steps for float
            boundaries = initial_segments[:]
            boundaries[idx] = i
            segmentation = create_segmentation_float(y, boundaries)
            if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
                column_name = f'{var_name}_seg_{"_".join(map(lambda x: f"{x:.2f}", boundaries))}'
                segmentations[column_name] = segmentation
                plot_segmentation(y, segmentation, boundaries, step)
                step += 1
    
    return segmentations

# Generate general segmentations for float values
general_segmentations_float_result = general_segmentations_float(df_float, 'y', num_segs, min_number)
tools.display_dataframe_to_user(name="General Segmentations with Float Values", dataframe=general_segmentations_float_result)





def general_segmentations(df, var_name, num_segs, min_number):
    y = df[var_name].values
    max_y = y[-1]
    initial_segments = list(np.linspace(min(y), max_y, num_segs, endpoint=False, dtype=int)[1:])
    
    segmentations = pd.DataFrame({var_name: y})
    
    def plot_segmentation(y, segmentation, boundaries, step):
        plt.figure(figsize=(10, 6))
        plt.scatter(y, segmentation, c=segmentation, cmap='viridis', alpha=0.7, edgecolors='w')
        plt.title(f'Segmentation at Step {step}: Boundaries {boundaries}')
        plt.xlabel(var_name)
        plt.ylabel('Segment')
        plt.grid(True)
        plt.show()
    
    step = 1
    
    for idx in reversed(range(num_segs - 1)):
        for i in range(initial_segments[idx] + 1, max_y + 1):
            boundaries = initial_segments[:]
            boundaries[idx] = i
            segmentation = create_segmentation(y, boundaries)
            if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
                column_name = f'{var_name}_seg_{"_".join(map(str, boundaries))}'
                segmentations[column_name] = segmentation
                plot_segmentation(y, segmentation, boundaries, step)
                step += 1
    
    return segmentations

# Example usage with a dataframe and 10 segments
df = pd.DataFrame({'y': np.arange(1, 101)})
num_segs = 10
min_number = 5

# Generate general segmentations
general_segmentations_result = general_segmentations(df, 'y', num_segs, min_number)
tools.display_dataframe_to_user(name="General Segmentations with 10 Segments", dataframe=general_segmentations_result)
i



mport matplotlib.pyplot as plt

def create_segmentations_with_corrected_steps(y, initial_segments, min_number):
    segmentations = pd.DataFrame({'y': y})
    max_y = y[-1]
    initial_s1, initial_s2, initial_s3 = initial_segments
    
    # Function to plot segmentations
    def plot_segmentation(y, segmentation, boundaries, step):
        plt.figure(figsize=(10, 6))
        plt.scatter(y, segmentation, c=segmentation, cmap='viridis', alpha=0.7, edgecolors='w')
        plt.title(f'Segmentation at Step {step}: Boundaries {boundaries}')
        plt.xlabel('y')
        plt.ylabel('Segment')
        plt.grid(True)
        plt.show()
    
    step = 1
    
    # Step 1: Fix S1, S2, S3; change S4
    for s4 in range(initial_s3 + 1, max_y + 1):
        boundaries = [initial_s1, initial_s2, initial_s3, s4]
        segmentation = create_segmentation(y, boundaries)
        if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
            column_name = f'y_seg_{"_".join(map(str, boundaries))}'
            segmentations[column_name] = segmentation
            plot_segmentation(y, segmentation, boundaries, step)
            step += 1
    
    # Step 2: Fix S1, S2; change S3, then S4
    for s3 in range(initial_s2 + 1, max_y):
        for s4 in range(s3 + 1, max_y + 1):
            boundaries = [initial_s1, initial_s2, s3, s4]
            segmentation = create_segmentation(y, boundaries)
            if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
                column_name = f'y_seg_{"_".join(map(str, boundaries))}'
                segmentations[column_name] = segmentation
                plot_segmentation(y, segmentation, boundaries, step)
                step += 1
    
    # Step 3: Fix S1; change S2, then S3, then S4
    for s2 in range(initial_s1 + 1, max_y):
        for s3 in range(s2 + 1, max_y):
            for s4 in range(s3 + 1, max_y + 1):
                boundaries = [initial_s1, s2, s3, s4]
                segmentation = create_segmentation(y, boundaries)
                if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
                    column_name = f'y_seg_{"_".join(map(str, boundaries))}'
                    segmentations[column_name] = segmentation
                    plot_segmentation(y, segmentation, boundaries, step)
                    step += 1
    
    return segmentations

# Generate segmentations with corrected steps and plotting
corrected_segmentations_with_plots = create_segmentations_with_corrected_steps(y, initial_segments, min_number)
tools.display_dataframe_to_user(name="Corrected Segmentations with Plots", dataframe=corrected_segmentations_with_plots)






# Redefine the function to include the steps as specified
def create_segmentations_with_steps(y, initial_segments, min_number):
    segmentations = pd.DataFrame({'y': y})
    max_y = y[-1]
    initial_s1, initial_s2, initial_s3 = initial_segments
    
    # Step 1: Fix S1, S2, S3; change S4
    for s4 in range(initial_s3 + 1, max_y + 1):
        boundaries = [initial_s1, initial_s2, initial_s3, s4]
        segmentation = create_segmentation(y, boundaries)
        if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
            column_name = f'y_seg_{"_".join(map(str, boundaries))}'
            segmentations[column_name] = segmentation
    
    # Step 2: Fix S1, S2; change S3, then S4
    for s3 in range(initial_s2 + 1, max_y):
        for s4 in range(s3 + 1, max_y + 1):
            boundaries = [initial_s1, initial_s2, s3, s4]
            segmentation = create_segmentation(y, boundaries)
            if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
                column_name = f'y_seg_{"_".join(map(str, boundaries))}'
                segmentations[column_name] = segmentation
    
    # Step 3: Fix S1; change S2, then S3, then S4
    for s2 in range(initial_s1 + 1, max_y):
        for s3 in range(s2 + 1, max_y):
            for s4 in range(s3 + 1, max_y + 1):
                boundaries = [initial_s1, s2, s3, s4]
                segmentation = create_segmentation(y, boundaries)
                if all(np.sum(segmentation == j) >= min_number for j in range(len(boundaries) + 1)):
                    column_name = f'y_seg_{"_".join(map(str, boundaries))}'
                    segmentations[column_name] = segmentation
    
    return segmentations

# Generate segmentations with the corrected steps
corrected_segmentations = create_segmentations_with_steps(y, initial_segments, min_number)
tools.display_dataframe_to_user(name="Corrected Segmentations", dataframe=corrected_segmentations)





import matplotlib.pyplot as plt

# Define a function to plot segmentations
def plot_segmentations(segmentations, y, step_boundaries):
    for column in segmentations.columns[1:]:  # Skip the 'y' column
        boundaries = list(map(int, column.split('_')[2:]))
        fig, ax = plt.subplots()
        for i, boundary in enumerate(boundaries):
            ax.axvline(boundary, color='r', linestyle='--', label=f'Boundary {i+1}: {boundary}')
        ax.scatter(y, segmentations[column], c=segmentations[column], cmap='viridis', label='Segmentation')
        ax.set_title(f'Segmentation: {column}')
        ax.set_xlabel('y')
        ax.set_ylabel('Segment')
        ax.legend(loc='best')
        plt.show()

# Plot the segmentations
plot_segmentations(segmentations, y, initial_segments)




import numpy as np
import pandas as pd

# Define the initial continuous variable y
y = np.arange(1, 101)  # y = [1, 2, 3, ..., 100]

# Define initial segment boundaries and minimum number of observations
initial_segments = [25, 50, 75]
min_number = 5

# Create an empty DataFrame to store the segmentations
segmentations = pd.DataFrame({'y': y})

# Function to create a segmentation based on given boundaries
def create_segmentation(y, boundaries):
    y_seg = np.zeros_like(y)
    for i, boundary in enumerate(boundaries):
        y_seg[(y > boundaries[i-1]) & (y <= boundary)] = i if i > 0 else 0
    y_seg[y > boundaries[-1]] = len(boundaries)
    return y_seg

# Iterate over each boundary, starting with the last one and moving backwards
for i in range(len(initial_segments) - 1, -1, -1):
    boundary_name = f'S{i+1}'
    max_value = y[-1] if i == len(initial_segments) - 1 else initial_segments[i+1]
    min_value = 1 if i == 0 else initial_segments[i-1] + 1
    
    for new_boundary in range(initial_segments[i], max_value + 1):
        new_boundaries = initial_segments.copy()
        new_boundaries[i] = new_boundary
        segmentation = create_segmentation(y, new_boundaries)
        
        # Check if the segmentation is valid
        valid = all(np.sum(segmentation == j) >= min_number for j in range(len(new_boundaries) + 1))
        if valid:
            column_name = f'y_seg_{"_".join(map(str, new_boundaries))}'
            segmentations[column_name] = segmentation

# Display the generated segmentations
import ace_tools as tools; tools.display_dataframe_to_user(name="Generated Segmentations", dataframe=segmentations)





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters and range for min_samples_split
num_clusters = 10
min_samples_split_values = range(2, 21)  # Testing values from 2 to 20

# Initialize variables to store metrics
metrics = []

# Iterate over different values of min_samples_split
for min_samples in min_samples_split_values:
    # Store F1 scores for each min_samples_split value
    current_metrics = {'min_samples_split': min_samples, 'weighted_f1': [], 'cluster_f1': {}}

    # Iterate over each feature to segment it using the target variable
    for feature_index in range(data.shape[1] - 1):  # Exclude the target column
        feature_name = data.columns[feature_index]
        X_feature = data[[feature_name]].values.reshape(-1, 1)
        y = data['target'].values.reshape(-1, 1)
        
        # Segment the feature using the target variable
        dt = DecisionTreeRegressor(min_samples_split=min_samples, max_leaf_nodes=num_clusters, random_state=0)
        dt.fit(y, X_feature)
        predicted_X = dt.predict(y)
        
        # Convert the continuous segment predictions into discrete labels
        label_encoder = LabelEncoder()
        segment_labels = label_encoder.fit_transform(predicted_X)
        
        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(
            data.drop(columns='target'), segment_labels, test_size=0.3, random_state=0
        )
        
        # Use all features (excluding the target variable) to predict these segments
        xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
        xgb.fit(X_train, y_train)
        
        # Predict the segments on the test set
        predicted_segments = xgb.predict(X_test)
        
        # Generate a classification report
        report = classification_report(y_test, predicted_segments, output_dict=True)
        
        # Store the weighted average F1 score and individual cluster F1 scores
        current_metrics['weighted_f1'].append(report['weighted avg']['f1-score'])
        for label, metrics in report.items():
            if isinstance(metrics, dict):  # Only store scores for each cluster
                if label not in current_metrics['cluster_f1']:
                    current_metrics['cluster_f1'][label] = []
                current_metrics['cluster_f1'][label].append(metrics['f1-score'])
    
    # Store metrics for the current min_samples_split value
    metrics.append(current_metrics)

# Process and plot the results
min_samples_split_vals = [m['min_samples_split'] for m in metrics]
weighted_f1_scores = [np.mean(m['weighted_f1']) for m in metrics]

plt.figure(figsize=(12, 6))
plt.plot(min_samples_split_vals, weighted_f1_scores, marker='o', label='Weighted F1 Score')
plt.xlabel('min_samples_split')
plt.ylabel('F1 Score')
plt.title('Weighted F1 Score Evolution')
plt.legend()
plt.grid(True)
plt.show()

# Plot the F1 scores for each cluster across different min_samples_split values
plt.figure(figsize=(12, 6))
for cluster_label in metrics[0]['cluster_f1'].keys():
    cluster_scores = [m['cluster_f1'][cluster_label] for m in metrics]
    mean_cluster_scores = [np.mean(scores) for scores in cluster_scores]
    plt.plot(min_samples_split_vals, mean_cluster_scores, marker='o', label=f'Cluster {cluster_label}')

plt.xlabel('min_samples_split')
plt.ylabel('F1 Score')
plt.title('Cluster F1 Scores Evolution')
plt.legend()
plt.grid(True)
plt.show()







import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters and range for min_samples_split
num_clusters = 10
min_samples_split_values = range(2, 21)  # Testing values from 2 to 20

# Initialize variables to store metrics
metrics = []

# Iterate over different values of min_samples_split
for min_samples in min_samples_split_values:
    # Store F1 scores for each min_samples_split value
    current_metrics = {'min_samples_split': min_samples, 'weighted_f1': [], 'cluster_f1': []}

    # Iterate over each feature to segment it using the target variable
    for feature_index in range(data.shape[1] - 1):  # Exclude the target column
        feature_name = data.columns[feature_index]
        X_feature = data[[feature_name]].values.reshape(-1, 1)
        y = data['target'].values.reshape(-1, 1)
        
        # Segment the feature using the target variable
        dt = DecisionTreeRegressor(min_samples_split=min_samples, max_leaf_nodes=num_clusters, random_state=0)
        dt.fit(y, X_feature)
        predicted_X = dt.predict(y)
        
        # Convert the continuous segment predictions into discrete labels
        label_encoder = LabelEncoder()
        segment_labels = label_encoder.fit_transform(predicted_X)
        
        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(
            data.drop(columns='target'), segment_labels, test_size=0.3, random_state=0
        )
        
        # Use all features (excluding the target variable) to predict these segments
        xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
        xgb.fit(X_train, y_train)
        
        # Predict the segments on the test set
        predicted_segments = xgb.predict(X_test)
        
        # Generate a classification report
        report = classification_report(y_test, predicted_segments, output_dict=True)
        
        # Store the weighted average F1 score and individual cluster F1 scores
        current_metrics['weighted_f1'].append(report['weighted avg']['f1-score'])
        for label, metrics in report.items():
            if isinstance(metrics, dict):  # Only store scores for each cluster
                current_metrics['cluster_f1'].append((label, metrics['f1-score']))
    
    # Store metrics for the current min_samples_split value
    metrics.append(current_metrics)

# Process and plot the results
min_samples_split_vals = [m['min_samples_split'] for m in metrics]
weighted_f1_scores = [np.mean(m['weighted_f1']) for m in metrics]

plt.figure(figsize=(12, 6))
plt.plot(min_samples_split_vals, weighted_f1_scores, marker='o', label='Weighted F1 Score')
plt.xlabel('min_samples_split')
plt.ylabel('F1 Score')
plt.title('Weighted F1 Score Evolution')
plt.legend()
plt.grid(True)
plt.show()

# Plot the F1 scores for each cluster across different min_samples_split values
plt.figure(figsize=(12, 6))
for i, cluster_f1 in enumerate(metrics[0]['cluster_f1']):
    cluster_label = cluster_f1[0]
    cluster_scores = [m['cluster_f1'][i][1] for m in metrics]
    plt.plot(min_samples_split_vals, cluster_scores, marker='o', label=f'Cluster {cluster_label}')

plt.xlabel('min_samples_split')
plt.ylabel('F1 Score')
plt.title('Cluster F1 Scores Evolution')
plt.legend()
plt.grid(True)
plt.show()






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Initialize variables to store the best feature and corresponding results
best_f1 = 0
best_feature_index = -1
best_segment_labels = None
best_report = None

# Iterate over each feature to segment it using the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude the target column
    feature_name = data.columns[feature_index]
    X_feature = data[[feature_name]].values.reshape(-1, 1)
    y = data['target'].values.reshape(-1, 1)
    
    # Segment the feature using the target variable
    dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
    dt.fit(y, X_feature)
    predicted_X = dt.predict(y)
    
    # Convert the continuous segment predictions into discrete labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(predicted_X)
    
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(columns='target'), segment_labels, test_size=0.3, random_state=0
    )
    
    # Use all features (excluding the target variable) to predict these segments
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X_train, y_train)
    
    # Predict the segments on the test set
    predicted_segments = xgb.predict(X_test)
    
    # Generate a classification report
    report = classification_report(y_test, predicted_segments, output_dict=True)
    
    # Get the weighted average F1 score
    weighted_f1 = report['weighted avg']['f1-score']
    
    # Check if this is the best F1 score
    if weighted_f1 > best_f1:
        best_f1 = weighted_f1
        best_feature_index = feature_index
        best_segment_labels = label_encoder.fit_transform(dt.predict(data['target'].values.reshape(-1, 1)))
        best_report = report
    
    # Clean up
    del data['temp_segments']

# Add the best segments to the DataFrame as a new column
best_feature_name = data.columns[best_feature_index]
data['best_segments'] = best_segment_labels

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Best Weighted F1 Score: {best_f1}")

# Output the detailed classification report for the best feature
print("\nDetailed Classification Report:")
for label, metrics in best_report.items():
    if isinstance(metrics, dict):  # Only print details for each class, not overall metrics
        print(f"Cluster {label}:")
        for metric_name, value in metrics.items():
            print(f"  {metric_name}: {value}")

# Plot the feature colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data[best_feature_name], c=data['best_segments'], cmap='viridis', marker='o')
plt.title(f'Feature {best_feature_name} Segmentation with Highest F1 Score')
plt.xlabel('Sample Index')
plt.ylabel(f'{best_feature_name} Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Initialize variables to store the best feature and corresponding results
best_f1 = 0
best_feature_index = -1
best_segment_labels = None
best_report = None

# Iterate over each feature to segment it using the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude the target column
    feature_name = data.columns[feature_index]
    X_feature = data[[feature_name]].values.reshape(-1, 1)
    y = data['target'].values.reshape(-1, 1)
    
    # Segment the feature using the target variable
    dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
    dt.fit(y, X_feature)
    predicted_X = dt.predict(y)
    
    # Convert the continuous segment predictions into discrete labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(predicted_X)
    
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(columns='target'), segment_labels, test_size=0.3, random_state=0
    )
    
    # Use all features (excluding the target variable) to predict these segments
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X_train, y_train)
    
    # Predict the segments on the test set
    predicted_segments = xgb.predict(X_test)
    
    # Generate a classification report
    report = classification_report(y_test, predicted_segments, output_dict=True)
    
    # Get the weighted average F1 score
    weighted_f1 = report['weighted avg']['f1-score']
    
    # Check if this is the best F1 score
    if weighted_f1 > best_f1:
        best_f1 = weighted_f1
        best_feature_index = feature_index
        best_segment_labels = label_encoder.fit_transform(dt.predict(data['target'].values.reshape(-1, 1)))
        best_report = report
    
    # Clean up
    del data['temp_segments']

# Add the best segments to the DataFrame as a new column
best_feature_name = data.columns[best_feature_index]
data['best_segments'] = best_segment_labels

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Best Weighted F1 Score: {best_f1}")

# Output the detailed classification report for the best feature
print("\nDetailed Classification Report:")
for label, metrics in best_report.items():
    if isinstance(metrics, dict):  # Only print details for each class, not overall metrics
        print(f"Cluster {label}:")
        for metric_name, value in metrics.items():
            print(f"  {metric_name}: {value}")

# Plot the feature colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data[best_feature_name], c=data['best_segments'], cmap='viridis', marker='o')
plt.title(f'Feature {best_feature_name} Segmentation with Highest F1 Score')
plt.xlabel('Sample Index')
plt.ylabel(f'{best_feature_name} Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Initialize variables to store the best feature and F1 score
best_f1 = 0
best_feature_index = -1
best_segment_labels = None

# Iterate over each feature to segment it using the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude the target column
    feature_name = data.columns[feature_index]
    X_feature = data[[feature_name]].values.reshape(-1, 1)
    y = data['target'].values.reshape(-1, 1)
    
    # Segment the feature using the target variable
    dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
    dt.fit(y, X_feature)
    predicted_X = dt.predict(y)
    
    # Convert the continuous segment predictions into discrete labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(predicted_X)
    
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(columns='target'), segment_labels, test_size=0.3, random_state=0
    )
    
    # Use all features (excluding the target variable) to predict these segments
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X_train, y_train)
    
    # Predict the segments and calculate the F1 score on the test set
    predicted_segments = xgb.predict(X_test)
    f1 = f1_score(y_test, predicted_segments, average='weighted')
    
    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segment_labels = label_encoder.fit_transform(dt.predict(data['target'].values.reshape(-1, 1)))
    
    # Clean up
    del data['temp_segments']

# Add the best segments to the DataFrame as a new column
best_feature_name = data.columns[best_feature_index]
data['best_segments'] = best_segment_labels

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Best F1 Score: {best_f1}")

# Plot the feature colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data[best_feature_name], c=data['best_segments'], cmap='viridis', marker='o')
plt.title(f'Feature {best_feature_name} Segmentation with Highest F1 Score')
plt.xlabel('Sample Index')
plt.ylabel(f'{best_feature_name} Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score, classification_report, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
X = np.random.randn(100, 5)  # 100 samples, 5 features
y = np.random.randn(100)     # Target variable

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(X.shape[1]):
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[:, feature_index].reshape(-1, 1), y)
    segments = dt.predict(X[:, feature_index].reshape(-1, 1))

    # Split the data into training and testing sets
    X_train, X_test, segments_train, segments_test = train_test_split(X, segments, test_size=0.2, random_state=0)

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X_train, segments_train)

    # Predict the segments on the test set and calculate the F1 score
    predicted_segments = xgb.predict(X_test)
    f1 = f1_score(segments_test, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segments

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index}, Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y)), y, c=best_segments, cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_index}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()

# Classification report and confusion matrix for the best feature
xgb.fit(X, best_segments)
predicted_segments = xgb.predict(X)
report = classification_report(best_segments, predicted_segments)
print("Classification Report:")
print(report)

cm = confusion_matrix(best_segments, predicted_segments)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_predict, StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
from xgboost import XGBClassifier

# Load your dataset
# For demonstration, we'll generate a synthetic dataset with multiple features and a binary target
np.random.seed(0)
data = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
data['target'] = np.random.randint(0, 2, 1000)  # Binary target variable

# Split the data into features and target
X = data.drop(columns='target')
y = data['target']

# Initialize the XGBClassifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

# Perform cross-validation and get predictions
y_pred = cross_val_predict(xgb, X, y, cv=cv)

# Compute the confusion matrix
cm = confusion_matrix(y, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate and print the classification report
report = classification_report(y, y_pred)
print("Classification Report:")
print(report)

# Plot feature importance
xgb.fit(X, y)
plt.figure(figsize=(10, 6))
plt.barh(range(len(xgb.feature_importances_)), xgb.feature_importances_)
plt.yticks(range(len(X.columns)), X.columns)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance in XGBClassifier")
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix
from xgboost import XGBClassifier

# Load your dataset
# For demonstration, we'll generate a synthetic dataset with multiple features and a binary target
np.random.seed(0)
data = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
data['target'] = np.random.randint(0, 2, 1000)  # Binary target variable

# Split the data into features and target
X = data.drop(columns='target')
y = data['target']

# Initialize the XGBClassifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

# Perform cross-validation
cv_scores = cross_val_score(xgb, X, y, cv=cv, scoring='accuracy')
print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean()}")

# Train the model on the entire dataset and evaluate
xgb.fit(X, y)
y_pred = cross_val_score(xgb, X, y, cv=cv, scoring='accuracy')

# Predict using cross-validation and calculate the confusion matrix
y_pred_all = cross_val_score(xgb, X, y, cv=cv, scoring='accuracy')
cm = confusion_matrix(y, y_pred_all)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate and print the classification report for the last fold
xgb.fit(X, y)
y_pred_last_fold = xgb.predict(X)
report = classification_report(y, y_pred_last_fold)
print("Classification Report:")
print(report)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(xgb.feature_importances_)), xgb.feature_importances_)
plt.yticks(range(len(X.columns)), X.columns)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance in XGBClassifier")
plt.show()






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
from xgboost import XGBClassifier

# Load your dataset
# For demonstration, we'll generate a synthetic dataset with multiple features and a binary target
np.random.seed(0)
data = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
data['target'] = np.random.randint(0, 2, 1000)  # Binary target variable

# Split the data into features and target
X = data.drop(columns='target')
y = data['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train the XGBClassifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)

# Predict the target values for the test set
y_pred = xgb.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Generate and print the classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(xgb.feature_importances_)), xgb.feature_importances_)
plt.yticks(range(len(X.columns)), X.columns)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance in XGBClassifier")
plt.show()






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Initialize variables to store the best feature and F1 score
best_f1 = 0
best_feature_index = -1
best_segment_labels = None

# Iterate over each feature to segment it using the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude the target column
    feature_name = data.columns[feature_index]
    X_feature = data[[feature_name]].values.reshape(-1, 1)
    y = data['target'].values.reshape(-1, 1)
    
    # Segment the feature using the target variable
    dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
    dt.fit(y, X_feature)
    predicted_X = dt.predict(y)
    
    # Convert the continuous segment predictions into discrete labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(predicted_X)
    
    # Add the segment labels to the DataFrame temporarily for classification
    data['temp_segments'] = segment_labels
    
    # Use all features (excluding the target variable) to predict these segments
    X_all_features = data.drop(columns=['target', 'temp_segments']).values
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X_all_features, segment_labels)
    
    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X_all_features)
    f1 = f1_score(segment_labels, predicted_segments, average='weighted')
    
    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segment_labels = segment_labels
    
    # Remove the temporary column
    data.drop(columns=['temp_segments'], inplace=True)

# Add the best segments to the DataFrame as a new column
best_feature_name = data.columns[best_feature_index]
data['best_segments'] = best_segment_labels

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Best F1 Score: {best_f1}")

# Plot the feature colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data[best_feature_name], c=data['best_segments'], cmap='viridis', marker='o')
plt.title(f'Feature {best_feature_name} Segmentation with Highest F1 Score')
plt.xlabel('Sample Index')
plt.ylabel(f'{best_feature_name} Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import f1_score

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame({
    'feature': np.random.randn(100),
    'target': np.random.randn(100)
})

# Desired number of clusters
num_clusters = 10

# Input feature and target variable
X = data['feature'].values.reshape(-1, 1)  # Single feature as 2D array
y = data['target']

# Segment the feature (X) using the target variable (y)
dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
dt.fit(y.values.reshape(-1, 1), X)

# Obtain the predicted values of X as segments
predicted_X = dt.predict(y.values.reshape(-1, 1))

# Convert these continuous values into discrete segment labels
label_encoder = LabelEncoder()
segment_labels = label_encoder.fit_transform(predicted_X)

# Use these segments as labels and predict using the target variable with XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(y.values.reshape(-1, 1), segment_labels)

# Predict the segments and calculate the F1 score
predicted_segments = xgb.predict(y.values.reshape(-1, 1))
f1 = f1_score(segment_labels, predicted_segments, average='weighted')

# Add the segments to the DataFrame as a new column
data['best_segments'] = segment_labels

# Output the F1 score
print(f"F1 Score: {f1}")

# Plot the feature colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['feature'], c=data['best_segments'], cmap='viridis', marker='o')
plt.title('Feature Segmentation Based on Target Variable')
plt.xlabel('Sample Index')
plt.ylabel('Feature Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import f1_score

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame({
    'feature': np.random.randn(100),
    'target': np.random.randn(100)
})

# Desired number of clusters
num_clusters = 10

# Input feature and target variable
X = data[['feature']]  # Single feature
y = data['target']

# Segment the target variable (y) directly using a Decision Tree Regressor
dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
dt.fit(X, y)

# Obtain the predicted values of y as segments
predicted_y = dt.predict(X)

# Convert these continuous values into discrete segment labels
label_encoder = LabelEncoder()
segment_labels = label_encoder.fit_transform(predicted_y)

# Use these segments as labels and predict using the single feature with XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X, segment_labels)

# Predict the segments and calculate the F1 score
predicted_segments = xgb.predict(X)
f1 = f1_score(segment_labels, predicted_segments, average='weighted')

# Add the segments to the DataFrame as a new column
data['best_segments'] = segment_labels

# Output the F1 score
print(f"F1 Score: {f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o')
plt.title('Target Variable Segmentation into Disjoint Clusters')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Input features and target variable
X = data.drop(columns='target')
y = data['target']

# Segment the target variable (y) directly using a Decision Tree Regressor
dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
dt.fit(X, y)

# Obtain the predicted values of y as segments
predicted_y = dt.predict(X)

# Convert these continuous values into discrete segment labels
label_encoder = LabelEncoder()
segment_labels = label_encoder.fit_transform(predicted_y)

# Use these segments as labels and predict using all features with XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X, segment_labels)

# Predict the segments and calculate the F1 score
predicted_segments = xgb.predict(X)
f1 = f1_score(segment_labels, predicted_segments, average='weighted')

# Add the segments to the DataFrame as a new column
data['best_segments'] = segment_labels

# Output the F1 score
print(f"F1 Score: {f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o')
plt.title('Target Variable Segmentation into Disjoint Clusters')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Input features and target variable
X = data.drop(columns='target')
y = data['target']

# Segment the target variable using a Decision Tree Regressor
dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
dt.fit(X, y)

# Assign each sample to a segment (leaf node)
segments = dt.apply(X)

# Convert the segments (leaf nodes) into categorical labels
label_encoder = LabelEncoder()
segment_labels = label_encoder.fit_transform(segments)

# Use these segments as labels and predict using all features with XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X, segment_labels)

# Predict the segments and calculate the F1 score
predicted_segments = xgb.predict(X)
f1 = f1_score(segment_labels, predicted_segments, average='weighted')

# Add the segments to the DataFrame as a new column
data['best_segments'] = segment_labels

# Output the F1 score
print(f"F1 Score: {f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o')
plt.title('Target Variable Segmentation into Disjoint Clusters')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Input features and target variable
X = data.drop(columns='target')
y = data['target']

# Segment the target variable using a Decision Tree Regressor
dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
dt.fit(X, y)

# Assign each sample to a segment (leaf node)
segments = dt.apply(X)

# Convert the segments (leaf nodes) into categorical labels
label_encoder = LabelEncoder()
data['best_segments'] = label_encoder.fit_transform(segments)

# Output the unique segments
unique_segments = data['best_segments'].unique()
print(f"Unique Segments: {len(unique_segments)}, Segments: {unique_segments}")

# Plot the target variable colored by the segments
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o')
plt.title('Target Variable Segmentation into Disjoint Clusters')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.show()



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude target column
    feature_name = data.columns[feature_index]
    X = data.drop(columns='target')
    y = data['target']
    
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
    dt.fit(X[[feature_name]], y)
    segments = dt.apply(X[[feature_name]])

    # Check for disjoint segments by ensuring non-overlapping intervals
    # This is a simplified example; more sophisticated checks and adjustments may be needed
    unique_segments = np.unique(segments)
    is_disjoint = len(unique_segments) <= num_clusters

    # Convert segments to categorical labels if disjoint
    if is_disjoint:
        label_encoder = LabelEncoder()
        segment_labels = label_encoder.fit_transform(segments)
    else:
        continue  # Skip if the segments are not disjoint

    # Check if this configuration is the best
    if len(unique_segments) > len(set(best_segments)) if best_segments is not None else 0:
        best_segments = segment_labels
        best_feature_index = feature_index

# Add the best segments to the DataFrame as a new column if disjoint segments were found
if best_segments is not None:
    data['best_segments'] = best_segments
    best_feature_name = data.columns[best_feature_index]
    print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Unique Segments: {len(set(best_segments))}")
else:
    print("No disjoint segments found with the given configuration.")

# Plot the target variable colored by the best segmentation if available
if best_segments is not None:
    plt.figure(figsize=(10, 6))
    plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o', label='Target variable')
    plt.title(f'Target Variable Segmentation by Feature {best_feature_name} with {num_clusters} Clusters')
    plt.xlabel('Sample Index')
    plt.ylabel('Target Variable Value')
    plt.colorbar(label='Segment Label')
    plt.grid(True)
    plt.legend()
    plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Example DataFrame with features and target
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.choice([0, 1], size=100)  # Binary target for classification

# Split the data into training and testing sets
X = data.drop(columns='target')
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train the XGBoost classifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X_train, y_train)

# Predict the target values for the test set
y_pred = xgb.predict(X_test)

# Calculate evaluation metrics
conf_matrix = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=xgb.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

# Print the evaluation metrics
print(f"F1 Score: {f1}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude target column
    feature_name = data.columns[feature_index]
    X = data.drop(columns='target')
    y = data['target']
    
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[[feature_name]], y)
    segments = dt.apply(X[[feature_name]])

    # Convert segments to categorical labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(segments)

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segment_labels)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segment_labels, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segment_labels

# Add the best segments to the DataFrame as a new column
data['best_segments'] = best_segments

# Output the best feature index and F1 score
best_feature_name = data.columns[best_feature_index]
print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_name}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
X = np.random.randn(100, 5)  # 100 samples, 5 features
y = np.random.randn(100)     # Target variable

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(X.shape[1]):
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[:, feature_index].reshape(-1, 1), y)
    segments = dt.apply(X[:, feature_index].reshape(-1, 1))

    # Convert segments to categorical labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(segments)

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segment_labels)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segment_labels, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segment_labels

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index}, Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y)), y, c=best_segments, cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_index}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import KBinsDiscretizer

# Generate a synthetic dataset for the example
np.random.seed(0)
X = np.random.randn(100, 5)  # 100 samples, 5 features
y = np.random.randn(100)     # Target variable

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(X.shape[1]):
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[:, feature_index].reshape(-1, 1), y)
    segments = dt.apply(X[:, feature_index].reshape(-1, 1))

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segments)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segments, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segments

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index}, Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y)), y, c=best_segments, cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_index}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
X = np.random.randn(100, 5)  # 100 samples, 5 features
y = np.random.randn(100)     # Target variable

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(X.shape[1]):
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[:, feature_index].reshape(-1, 1), y)
    segments = dt.predict(X[:, feature_index].reshape(-1, 1))

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segments)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segments, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segments

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index}, Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y)), y, c=best_segments, cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_index}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()





import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Generate a sample dataset with multiple features
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=0)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit the Decision Tree Regressor
tree = DecisionTreeRegressor(min_samples_split=5, random_state=0)
tree.fit(X_train, y_train)

# Predict using the decision tree
y_pred_train = tree.predict(X_train)
y_pred_test = tree.predict(X_test)

# Plot the original data and the decision tree segments
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# Training data plot
ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', label='Original data')
ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train, cmap='viridis', marker='x', label='Predicted segments')
ax[0].set_title('Training Data')
ax[0].set_xlabel('Feature 1')
ax[0].set_ylabel('Feature 2')
ax[0].legend()
ax[0].grid(True)

# Testing data plot
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', label='Original data')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, cmap='viridis', marker='x', label='Predicted segments')
ax[1].set_title('Testing Data')
ax[1].set_xlabel('Feature 1')
ax[1].set_ylabel('Feature 2')
ax[1].legend()
ax[1].grid(True)

plt.tight_layout()
plt.show()



import numpy as np
import matplotlib.pyplot as plt

# Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Set the minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Plot the entire data with each segment highlighted
for i, (start, end) in enumerate(segments):
    plt.figure()
    plt.plot(data, 'k-', alpha=0.3)  # Plot the full data in a lighter color
    plt.plot(range(start, end), data[start:end], 'b-', linewidth=2)  # Highlight the current segment
    plt.title(f'Segment {i + 1}: {start} to {end}')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.grid(True)
    plt.show()




import numpy as np
import matplotlib.pyplot as plt

# Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Set the minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Plotting the segments step-by-step
for i in range(len(segments)):
    plt.figure(figsize=(10, 5))
    plt.plot(data, label='Data', alpha=0.5)
    
    # Plot each segment up to the current iteration
    for j in range(i + 1):
        start, end = segments[j]
        plt.plot(range(start, end), data[start:end], marker='o', label=f'Segment {j + 1}')

    plt.title(f'Segmentation Step {i + 1}')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'segment_step_{i + 1}.png')  # Save each plot as an image file
    plt.show()




import numpy as np
import matplotlib.pyplot as plt

# Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Set the minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Plot each segment
for i, (start, end) in enumerate(segments):
    plt.figure()
    x = range(start, end)
    y = data[start:end]
    plt.plot(x, y, marker='o')
    plt.title(f'Segment {i + 1}: {start} to {end}')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.grid(True)
    plt.savefig(f'segment_{i + 1}.png')  # Save each plot as an image file
    plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Step 1: Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Step 2: Create segments based on a minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    # This function will generate the possible segmentations
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Step 3: Animate the plot
fig, ax = plt.subplots()
ax.set_xlim(0, len(data))
ax.set_ylim(np.min(data), np.max(data))
line, = ax.plot([], [], lw=2)
title = ax.set_title('')

def init():
    line.set_data([], [])
    return line,

def update(frame):
    start, end = segments[frame]
    x = range(start, end)
    y = data[start:end]
    line.set_data(x, y)
    title.set_text(f'Segment {frame + 1}/{len(segments)}: {start} to {end}')
    return line, title

ani = FuncAnimation(fig, update, frames=range(len(segments)), init_func=init, blit=True, repeat=False)
plt.show()






import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from itertools import combinations

def generate_possible_segments(y, num_segments, min_obs_per_seg):
    """
    Generates all possible segmentations for the target variable `y` with a given number of segments and minimum observations per segment.
    
    Parameters:
    - y: pd.Series - The target variable.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.
    
    Returns:
    - possible_segments: List of lists containing segment boundaries.
    """
    possible_segments = []
    sorted_values = np.sort(y.unique())
    n_values = len(sorted_values)
    
    for combination in combinations(range(1, n_values), num_segments - 1):
        segments = np.split(sorted_values, combination)
        if all(len(seg) >= min_obs_per_seg for seg in segments):
            segment_boundaries = [seg[-1] for seg in segments[:-1]]  # Get the last value in each segment except the last
            segment_boundaries = [sorted_values[0] - 1] + segment_boundaries + [sorted_values[-1] + 1]
            possible_segments.append(segment_boundaries)
    
    return possible_segments

def optimize_segments(df, target_var, num_segments, min_obs_per_seg):
    """
    Optimizes the segmentation of a target variable in a DataFrame to maximize the F1 score of an XGBoost model.

    Parameters:
    - df: pd.DataFrame - The input DataFrame.
    - target_var: str - The target variable column name.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.

    Returns:
    - df_with_segments: pd.DataFrame - The DataFrame with an added 'segment' column.
    - best_f1: float - The best F1 score achieved.
    """
    data = df.copy()
    X = data.drop(columns=[target_var])
    y = data[target_var]
    
    possible_segments = generate_possible_segments(y, num_segments, min_obs_per_seg)
    best_f1 = -np.inf
    best_segments = None

    for boundaries in possible_segments:
        labels = pd.cut(y, bins=boundaries, labels=False, include_lowest=True)
        data['segment'] = labels

        # Train an XGBoost model and evaluate F1 score
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
        model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_segments)
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        f1 = f1_score(y_test, predictions, average='weighted')

        if f1 > best_f1:
            best_f1 = f1
            best_segments = boundaries

    # Assign the best segments to the DataFrame
    data['segment'] = pd.cut(y, bins=best_segments, labels=False, include_lowest=True)

    # Plotting the target variable with segments
    plt.figure(figsize=(12, 6))
    plt.scatter(range(len(y)), y, c=data['segment'], cmap='viridis', label='Segments', alpha=0.6)
    plt.colorbar(label='Segment')
    plt.title(f'Target Variable Segmentation with Best F1 Score = {best_f1:.4f}')
    plt.xlabel('Index')
    plt.ylabel(target_var)
    plt.grid(True)
    plt.legend()
    plt.show()

    return data, best_f1

# Example usage:
# df = pd.read_csv('your_data.csv')
# df_with_segments, best_f1 = optimize_segments(df, 'target_variable', num_segments=5, min_obs_per_seg=50)
# print("Data with segments:\n", df_with_segments.head())
# print("Best F1 score:", best_f1)








import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def optimize_segments(df, target_var, num_segments, min_obs_per_seg):
    """
    Optimizes the segmentation of a target variable in a DataFrame to maximize the F1 score of an XGBoost model.

    Parameters:
    - df: pd.DataFrame - The input DataFrame.
    - target_var: str - The target variable column name.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.

    Returns:
    - df_with_segments: pd.DataFrame - The DataFrame with an added 'segment' column.
    - best_f1: float - The best F1 score achieved.
    """
    data = df.copy()
    X = data.drop(columns=[target_var])
    y = data[target_var]

    # Create initial segment boundaries
    initial_segment_boundaries = np.linspace(y.min(), y.max(), num_segments + 1)
    best_f1 = -np.inf
    best_segments = initial_segment_boundaries

    def evaluate_segments(boundaries):
        nonlocal best_f1, best_segments
        
        # Create segment labels based on boundaries
        labels = pd.cut(y, bins=boundaries, labels=False, include_lowest=True)
        data['segment'] = labels
        
        # Check if each segment has the minimum required observations
        segment_counts = data['segment'].value_counts()
        if (segment_counts < min_obs_per_seg).any():
            return -np.inf  # Return a very low F1 score if any segment is too small
        
        # Train an XGBoost model and evaluate F1 score
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
        model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_segments)
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        f1 = f1_score(y_test, predictions, average='weighted')
        
        if f1 > best_f1:
            best_f1 = f1
            best_segments = boundaries
        
        return f1

    # Optimization loop - this is a placeholder for a more sophisticated optimization approach
    for _ in range(100):  # Number of iterations, adjust based on needs
        new_boundaries = initial_segment_boundaries + np.random.normal(0, 0.1, size=initial_segment_boundaries.shape)
        new_boundaries = np.sort(np.clip(new_boundaries, y.min(), y.max()))
        evaluate_segments(new_boundaries)

    # Assign the best segments to the DataFrame
    data['segment'] = pd.cut(y, bins=best_segments, labels=False, include_lowest=True)

    # Plotting the target variable with segments
    plt.figure(figsize=(12, 6))
    plt.scatter(range(len(y)), y, c=data['segment'], cmap='viridis', label='Segments', alpha=0.6)
    plt.colorbar(label='Segment')
    plt.title(f'Target Variable Segmentation with F1 Score = {best_f1:.4f}')
    plt.xlabel('Index')
    plt.ylabel(target_var)
    plt.grid(True)
    plt.legend()
    plt.show()

    return data, best_f1

# Example usage:
# df = pd.read_csv('your_data.csv')
# df_with_segments, best_f1 = optimize_segments(df, 'target_variable', num_segments=5, min_obs_per_seg=50)
# print("Data with segments:\n", df_with_segments.head())
# print("Best F1 score:", best_f1)








import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def optimize_segments(df, target_var, num_segments, min_obs_per_seg):
    """
    Optimizes the segmentation of a target variable in a DataFrame to maximize the R² score of an XGBoost model.

    Parameters:
    - df: pd.DataFrame - The input DataFrame.
    - target_var: str - The target variable column name.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.

    Returns:
    - df_with_segments: pd.DataFrame - The DataFrame with an added 'segment' column.
    - best_r2: float - The best R² score achieved.
    """
    data = df.copy()
    X = data.drop(columns=[target_var])
    y = data[target_var]

    # Create initial segment boundaries
    initial_segment_boundaries = np.linspace(y.min(), y.max(), num_segments + 1)
    best_r2 = -np.inf
    best_segments = initial_segment_boundaries

    def evaluate_segments(boundaries):
        nonlocal best_r2, best_segments
        
        # Create segment labels based on boundaries
        labels = pd.cut(y, bins=boundaries, labels=False, include_lowest=True)
        data['segment'] = labels
        
        # Check if each segment has the minimum required observations
        segment_counts = data['segment'].value_counts()
        if (segment_counts < min_obs_per_seg).any():
            return -np.inf  # Return a very low R² if any segment is too small
        
        # Train an XGBoost model and evaluate R²
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
        model = xgb.XGBRegressor(objective='reg:squarederror')
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        r2 = r2_score(y_test, predictions)
        
        if r2 > best_r2:
            best_r2 = r2
            best_segments = boundaries
        
        return r2

    # Optimization loop - this is a placeholder for a more sophisticated optimization approach
    for _ in range(100):  # Number of iterations, adjust based on needs
        new_boundaries = initial_segment_boundaries + np.random.normal(0, 0.1, size=initial_segment_boundaries.shape)
        new_boundaries = np.sort(np.clip(new_boundaries, y.min(), y.max()))
        evaluate_segments(new_boundaries)

    # Assign the best segments to the DataFrame
    data['segment'] = pd.cut(y, bins=best_segments, labels=False, include_lowest=True)

    # Plotting the target variable with segments
    plt.figure(figsize=(12, 6))
    plt.scatter(range(len(y)), y, c=data['segment'], cmap='viridis', label='Segments', alpha=0.6)
    plt.colorbar(label='Segment')
    plt.title(f'Target Variable Segmentation with R² = {best_r2:.4f}')
    plt.xlabel('Index')
    plt.ylabel(target_var)
    plt.grid(True)
    plt.legend()
    plt.show()

    return data, best_r2

# Example usage:
# df = pd.read_csv('your_data.csv')
# df_with_segments, best_r2 = optimize_segments(df, 'target_variable', num_segments=5, min_obs_per_seg=50)
# print("Data with segments:\n", df_with_segments.head())
# print("Best R² score:", best_r2)






import xgboost as xgb
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def plot_top_k_features(model, feature_names, k=5, title="Feature Importance"):
    # Get feature importances
    feature_importances = model.feature_importances_
    feature_importances_percentage = 100 * (feature_importances / feature_importances.sum())

    # Create a DataFrame for visualization
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importances_percentage
    }).sort_values(by='Importance', ascending=False).head(k)

    # Plot feature importances
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=importance_df)
    plt.xlabel('Importance (%)')
    plt.ylabel('Feature')
    plt.title(title)
    plt.show()

# Generate a sample regression dataset
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train_reg, y_train_reg)

# Plot top k feature importances for regression
plot_top_k_features(regressor, [f'feature_{i}' for i in range(X_reg.shape[1])], k=5, title="Top 5 Features for XGBoost Regression")

# Generate a sample classification dataset
X_clf, y_clf = make_classification(n_samples=1000, n_features=10, random_state=42)
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

# Train an XGBoost classification model
classifier = xgb.XGBClassifier()
classifier.fit(X_train_clf, y_train_clf)

# Plot top k feature importances for classification
plot_top_k_features(classifier, [f'feature_{i}' for i in range(X_clf.shape[1])], k=5, title="Top 5 Features for XGBoost Classification")

# Generate a sample dataset for RandomForestRegressor
X_rf, y_rf = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)
feature_names_rf = [f'feature_{i}' for i in range(X_rf.shape[1])]

# Train a RandomForestRegressor
model_rf = RandomForestRegressor(random_state=42)
model_rf.fit(X_rf, y_rf)

# Plot top k feature importances for RandomForestRegressor
plot_top_k_features(model_rf, feature_names_rf, k=5, title="Top 5 Features for RandomForestRegressor")





import xgboost as xgb
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# Generate a sample regression dataset
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train, y_train)

# Get feature importance
importance = regressor.get_booster().get_score(importance_type='weight')
importance = {k: v / sum(importance.values()) for k, v in importance.items()}  # Calculate percentages

# Sort importance by feature
sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
features = [x[0] for x in sorted_importance]
importance_values = [x[1] * 100 for x in sorted_importance]  # Convert to percentages

# Plot feature importance
fig, ax = plt.subplots()
ax.barh(features, importance_values)
ax.set_title("Feature Importance for XGBoost Regression (Percentage)")
ax.set_xlabel("Percentage")
ax.set_ylabel("Features")
plt.show()






import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample classification dataset
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost classification model
classifier = xgb.XGBClassifier()
classifier.fit(X_train, y_train)

# Plot feature importance
fig, ax = plt.subplots()
plot_importance(classifier, ax=ax)
ax.set_title("Feature Importance for XGBoost Classification")
plt.show()
import xgboost as xgb
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample regression dataset
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train, y_train)

# Plot feature importance
fig, ax = plt.subplots()
plot_importance(regressor, ax=ax)
ax.set_title("Feature Importance for XGBoost Regression")
plt.show()





import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, precision_recall_curve

# Generate a synthetic dataset with class imbalance
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,
                           n_redundant=10, n_clusters_per_class=1, n_classes=10,
                           weights=[0.1]*9 + [0.1], flip_y=0, random_state=42)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize results dictionary
results = {}

# Approach 1: SMOTE (Oversampling)
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
classifier_smote = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
classifier_smote.fit(X_train_smote, y_train_smote)
y_pred_smote = classifier_smote.predict(X_test)
results['SMOTE'] = classification_report(y_test, y_pred_smote, output_dict=True)

# Approach 2: Random Undersampling
rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)
classifier_rus = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
classifier_rus.fit(X_train_rus, y_train_rus)
y_pred_rus = classifier_rus.predict(X_test)
results['Random Undersampling'] = classification_report(y_test, y_pred_rus, output_dict=True)

# Approach 3: Class Weight Adjustment
class_weights = {i: len(y_train) / (10 * sum(y_train == i)) for i in np.unique(y_train)}
classifier_weighted = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', scale_pos_weight=class_weights)
classifier_weighted.fit(X_train, y_train)
y_pred_weighted = classifier_weighted.predict(X_test)
results['Class Weight Adjustment'] = classification_report(y_test, y_pred_weighted, output_dict=True)

# Approach 4: Threshold Moving
classifier_threshold = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
classifier_threshold.fit(X_train, y_train)
y_probs = classifier_threshold.predict_proba(X_test)
thresholds = {}
best_thresholds = {}

for i in range(10):
    precision, recall, thr = precision_recall_curve(y_test == i, y_probs[:, i])
    f1_scores = 2 * (precision * recall) / (precision + recall)
    best_threshold = thr[np.argmax(f1_scores)]
    thresholds[i] = best_threshold
    best_thresholds[i] = best_threshold

y_pred_adjusted = np.zeros_like(y_test)
for i in range(len(y_test)):
    class_probs = y_probs[i]
    class_probs_thresholded = (class_probs >= [thresholds[j] for j in range(10)]).astype(int)
    y_pred_adjusted[i] = np.argmax(class_probs_thresholded)

results['Threshold Moving'] = classification_report(y_test, y_pred_adjusted, output_dict=True)

# Print results
for approach, metrics in results.items():
    print(f"Approach: {approach}")
    print(pd.DataFrame(metrics).transpose())
    print("\n")

# Evaluate the models
import matplotlib.pyplot as plt

approaches = ['SMOTE', 'Random Undersampling', 'Class Weight Adjustment', 'Threshold Moving']
f1_scores = [results[approach]['weighted avg']['f1-score'] for approach in approaches]

plt.figure(figsize=(10, 6))
plt.bar(approaches, f1_scores, color=['blue', 'green', 'red', 'purple'])
plt.xlabel('Approaches')
plt.ylabel('F1-Score')
plt.title('Comparison of Approaches for Handling Imbalanced Data')
plt.show()






import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, precision_recall_curve

# Generate a synthetic dataset with class imbalance
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,
                           n_redundant=10, n_clusters_per_class=1, n_classes=10,
                           weights=[0.05]*9 + [0.55], flip_y=0, random_state=42)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to oversample the minority classes
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Calculate class weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)
class_weights_dict = dict(enumerate(class_weights))

# Train an XGBoost classifier with class weights
classifier = XGBClassifier(scale_pos_weight=class_weights_dict)
classifier.fit(X_train_res, y_train_res)

# Predict probabilities
y_probs = classifier.predict_proba(X_test)

# Example of adjusting the threshold for one class (class 1)
precision, recall, thresholds = precision_recall_curve(y_test == 1, y_probs[:, 1])
f1_scores = 2 * (precision * recall) / (precision + recall)
best_threshold = thresholds[np.argmax(f1_scores)]

# Apply the best threshold
y_pred_adjusted = (y_probs[:, 1] >= best_threshold).astype(int)

# Evaluate the model
print("Classification Report with Adjusted Threshold:")
print(classification_report(y_test, y_pred_adjusted))

# Standard classification report for comparison
y_pred = classifier.predict(X_test)
print("Standard Classification Report:")
print(classification_report(y_test, y_pred))




from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# Generate a synthetic imbalanced dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,
                           n_redundant=10, n_clusters_per_class=1,
                           weights=[0.99], flip_y=0, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Train an XGBoost classifier with balanced class weights
classifier = XGBClassifier(scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))
classifier.fit(X_train_res, y_train_res)

# Evaluate the model
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))






import xgboost as xgb
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample regression dataset
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train, y_train)

# Plot feature importance
plot_importance(regressor)
plt.title("Feature Importance for XGBoost Regression")
plt.show()

import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample classification dataset
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost classification model
classifier = xgb.XGBClassifier()
classifier.fit(X_train, y_train)

# Plot feature importance
plot_importance(classifier)
plt.title("Feature Importance for XGBoost Classification")
plt.show()






import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

# Generate a sample dataset
X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)
feature_names = [f'feature_{i}' for i in range(X.shape[1])]
data = pd.DataFrame(X, columns=feature_names)
data['target'] = y

# Train a RandomForestRegressor
model = RandomForestRegressor(random_state=42)
model.fit(data[feature_names], data['target'])

# Get feature importances
feature_importances = model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance')
plt.show()





import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Sample data
data = pd.DataFrame({
    'x': range(10),
    'y1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'y2': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
    'y3': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
})

# Convert data to long format
data_melted = pd.melt(data, ['x'])

# Plot with seaborn
sns.lineplot(data=data_melted, x='x', y='value', hue='variable')

# Show plot
plt.show()





import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, f1_score, ConfusionMatrixDisplay
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Apply k-means clustering to segment the target variable
optimal_clusters = 3  # Example: Adjust based on your previous result
kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
df['Cluster'] = kmeans.fit_predict(df[['Target']])

# Combine the columns for stratification
df['Stratify'] = df['Age'].astype(str) + '_' + df['Income'].round(0).astype(str) + '_' + df['Category1'].astype(str) + '_' + df['Category2'].astype(str)

# Perform the train-test split with stratification
train_df, test_df = train_test_split(df, test_size=0.2, random_state=0, stratify=df['Stratify'])

# Drop the stratification column from the train and test sets
train_df = train_df.drop(columns=['Stratify']).reset_index(drop=True)
test_df = test_df.drop(columns=['Stratify']).reset_index(drop=True)

# Prepare training and testing sets
X_train = train_df.drop(columns=['Target', 'Cluster'])
y_train = train_df['Cluster']
X_test = test_df.drop(columns=['Target', 'Cluster'])
y_test = test_df['Cluster']

# Define classifiers and their hyperparameters
classifiers = {
    'XGBoost': XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='mlogloss'),
    'LightGBM': LGBMClassifier(random_state=0),
    'CatBoost': CatBoostClassifier(random_state=0, verbose=0)
}

param_grids = {
    'XGBoost': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]},
    'LightGBM': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'num_leaves': [31, 50]},
    'CatBoost': {'iterations': [50, 100], 'learning_rate': [0.01, 0.1], 'depth': [3, 5]}
}

# Train, evaluate and plot results for each classifier
results = {}

for clf_name, clf in classifiers.items():
    print(f"Training {clf_name}...")
    grid_search = GridSearchCV(clf, param_grids[clf_name], cv=5, scoring='f1_macro', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_clf = grid_search.best_estimator_
    y_pred = best_clf.predict(X_test)
    
    # Compute confusion matrix and F1 score
    conf_matrix = confusion_matrix(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    
    # Store results
    results[clf_name] = {
        'conf_matrix': conf_matrix,
        'f1_score': f1
    }
    
    # Plot confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=best_clf.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f'{clf_name} Confusion Matrix')
    plt.show()
    
    # Print F1 score
    print(f'{clf_name} F1 Score: {f1:.2f}')

# Plot F1 scores for all classifiers
f1_scores = {clf_name: res['f1_score'] for clf_name, res in results.items()}
plt.figure(figsize=(10, 6))
plt.bar(f1_scores.keys(), f1_scores.values())
plt.xlabel('Classifier')
plt.ylabel('F1 Score')
plt.title('F1 Scores for Different Classifiers')
plt.show()






import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Combine the columns for stratification
df['Stratify'] = df['Age'].astype(str) + '_' + df['Income'].round(0).astype(str) + '_' + df['Category1'].astype(str) + '_' + df['Category2'].astype(str)

# Perform the train-test split with stratification
train_df, test_df = train_test_split(df, test_size=0.2, random_state=0, stratify=df['Stratify'])

# Drop the stratification column from the train and test sets
train_df = train_df.drop(columns=['Stratify']).reset_index(drop=True)
test_df = test_df.drop(columns=['Stratify']).reset_index(drop=True)

# Check the distribution
print("Training set distribution:\n", train_df['Stratify'].value_counts(normalize=True))
print("\nTesting set distribution:\n", test_df['Stratify'].value_counts(normalize=True))

# Proceed with your further code (clustering, modeling, etc.)





import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Plot AIC and BIC for different number of clusters
n_clusters = range(1, 11)
aic_values = []
bic_values = []
silhouette_scores = []

for n in n_clusters:
    gmm = GaussianMixture(n_components=n, random_state=0)
    gmm.fit(df[['Target']])
    aic_values.append(gmm.aic(df[['Target']]))
    bic_values.append(gmm.bic(df[['Target']]))
    if n > 1:
        silhouette_scores.append(silhouette_score(df[['Target']], gmm.predict(df[['Target']])))

plt.figure(figsize=(12, 6))
plt.plot(n_clusters, aic_values, label='AIC')
plt.plot(n_clusters, bic_values, label='BIC')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('AIC and BIC for different number of clusters')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(n_clusters[1:], silhouette_scores, label='Silhouette Score')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('Silhouette Score for different number of clusters')
plt.legend()
plt.show()

# Choose the optimal number of clusters based on BIC (you can also choose based on AIC or silhouette score)
optimal_clusters = np.argmin(bic_values) + 1
print(f'Optimal number of clusters: {optimal_clusters}')

# Apply k-means clustering to segment the target variable
kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
df['Cluster'] = kmeans.fit_predict(df[['Target']])

# Separate data into clusters based on k-means clustering
clusters = [df[df['Cluster'] == i] for i in range(optimal_clusters)]

# Calculate and display intervals for each cluster
intervals = []
for cluster_id, cluster in enumerate(clusters):
    min_value = cluster['Target'].min()
    max_value = cluster['Target'].max()
    intervals.append((min_value, max_value))

# Sort intervals to ensure they are in order
intervals.sort()

# Adjust intervals to ensure no gaps
adjusted_intervals = []
for i, (min_value, max_value) in enumerate(intervals):
    if i == 0:
        adjusted_intervals.append((min_value, max_value))
    else:
        previous_max = adjusted_intervals[-1][1]
        adjusted_intervals.append((previous_max, max_value))

# Print adjusted intervals
for i, (min_value, max_value) in enumerate(adjusted_intervals):
    print(f'Cluster {i}: {min_value:.2f} to {max_value:.2f}')

# Define models to train
models = {
    'RandomForest': RandomForestRegressor(random_state=0),
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Huber': HuberRegressor()
}

param_grid = {
    'RandomForest': {'n_estimators': [50, 100], 'max_depth': [3, 5]},
    'Ridge': {'alpha': [0.01, 0.1, 1]},
    'Lasso': {'alpha': [0.01, 0.1, 1]},
    'Huber': {'epsilon': [1.35, 1.5]}
}

# Function to train and evaluate models
def train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid):
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    return r2

# Train and evaluate models for each cluster
results = {}
for cluster_id, cluster in enumerate(clusters):
    X = cluster.drop(columns=['Target', 'Cluster'])
    y = cluster['Target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    cluster_results = {}
    for model_name, model in models.items():
        r2 = train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid.get(model_name, {}))
        cluster_results[model_name] = r2
    results[f'Cluster {cluster_id}'] = cluster_results

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

# Plot R² scores for each model across different clusters
results_df.plot(kind='bar', figsize=(12, 6))
plt.xlabel('Cluster')
plt.ylabel('R² Score')
plt.title('R² Scores for Different Models across Clusters')
plt.legend()
plt.show()











import xgboost as xgb
from sklearn.metrics import r2_score
import itertools

# Define the parameter grid
etas = [0.01, 0.1, 0.3]
max_depths = [2, 4, 6]
subsamples = [0.8, 1.0]
colsample_bytree = [1.0, 0.8]

# List to store results
results = []

# Iterate over all parameter combinations
for eta, max_depth, subsample, colsample in itertools.product(etas, max_depths, subsamples, colsample_bytree):
    xgb_params = {
        'eta': eta,
        'max_depth': max_depth,
        'subsample': subsample,
        'colsample_bytree': colsample,
        'objective': 'reg:squarederror',
        'seed': 1989
    }
    
    xgb_cv = xgb.cv(
        params=xgb_params,
        dtrain=train_xgb,
        num_boost_round=10000,
        nfold=5,
        early_stopping_rounds=10,
        verbose_eval=0
    )
    
    # Extract the best number of boosting rounds
    best_num_boost_rounds = len(xgb_cv['train-rmse-mean'])
    
    # Train the model with the best number of boosting rounds
    model = xgb.train(
        params=xgb_params,
        dtrain=train_xgb,
        num_boost_round=best_num_boost_rounds
    )
    
    # Make predictions
    y_pred = model.predict(test_xgb)
    
    # Evaluate using R2 score
    r2 = r2_score(y_test, y_pred)
    
    # Store the results
    results.append({
        'eta': eta,
        'max_depth': max_depth,
        'subsample': subsample,
        'colsample_bytree': colsample,
        'best_num_boost_rounds': best_num_boost_rounds,
        'r2': r2
    })

# Find the best result
best_result = max(results, key=lambda x: x['r2'])

# Print the best parameters and corresponding R2 score
print("Best parameters found: ", best_result)
print("Best R2 score: ", best_result['r2'])











import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Plot AIC and BIC for different number of clusters
n_clusters = range(1, 11)
aic_values = []
bic_values = []
silhouette_scores = []

for n in n_clusters:
    gmm = GaussianMixture(n_components=n, random_state=0)
    gmm.fit(df[['Target']])
    aic_values.append(gmm.aic(df[['Target']]))
    bic_values.append(gmm.bic(df[['Target']]))
    if n > 1:
        silhouette_scores.append(silhouette_score(df[['Target']], gmm.predict(df[['Target']])))

plt.figure(figsize=(12, 6))
plt.plot(n_clusters, aic_values, label='AIC')
plt.plot(n_clusters, bic_values, label='BIC')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('AIC and BIC for different number of clusters')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(n_clusters[1:], silhouette_scores, label='Silhouette Score')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('Silhouette Score for different number of clusters')
plt.legend()
plt.show()

# Choose the optimal number of clusters based on BIC (you can also choose based on AIC or silhouette score)
optimal_clusters = np.argmin(bic_values) + 1
print(f'Optimal number of clusters: {optimal_clusters}')

# Apply k-means clustering to segment the target variable
kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
df['Cluster'] = kmeans.fit_predict(df[['Target']])

# Separate data into clusters based on k-means clustering
clusters = [df[df['Cluster'] == i] for i in range(optimal_clusters)]

# Calculate and display intervals for each cluster
intervals = {}
for cluster_id, cluster in enumerate(clusters):
    min_value = cluster['Target'].min()
    max_value = cluster['Target'].max()
    intervals[f'Cluster {cluster_id}'] = (min_value, max_value)
    print(f'Cluster {cluster_id}: {min_value:.2f} to {max_value:.2f}')

# Define models to train
models = {
    'RandomForest': RandomForestRegressor(random_state=0),
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Huber': HuberRegressor()
}

param_grid = {
    'RandomForest': {'n_estimators': [50, 100], 'max_depth': [3, 5]},
    'Ridge': {'alpha': [0.01, 0.1, 1]},
    'Lasso': {'alpha': [0.01, 0.1, 1]},
    'Huber': {'epsilon': [1.35, 1.5]}
}

# Function to train and evaluate models
def train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid):
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    return r2

# Train and evaluate models for each cluster
results = {}
for cluster_id, cluster in enumerate(clusters):
    X = cluster.drop(columns=['Target', 'Cluster'])
    y = cluster['Target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    cluster_results = {}
    for model_name, model in models.items():
        r2 = train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid.get(model_name, {}))
        cluster_results[model_name] = r2
    results[f'Cluster {cluster_id}'] = cluster_results

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

# Plot R² scores for each model across different clusters
results_df.plot(kind='bar', figsize=(12, 6))
plt.xlabel('Cluster')
plt.ylabel('R² Score')
plt.title('R² Scores for Different Models across Clusters')
plt.legend()
plt.show()














import pandas as pd
import numpy as np

# Load your dataset
# Replace 'your_dataset.csv' with the path to your dataset and 'target' with the target variable name
df = pd.read_csv('your_dataset.csv')

# Define the target variable
target = 'your_target_variable'

# Calculate the IQR (Interquartile Range) to identify outliers
Q1 = df[target].quantile(0.25)
Q3 = df[target].quantile(0.75)
IQR = Q3 - Q1

# Define the bounds for identifying outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Create the first dataset with outliers and negative target values
outliers_negative = df[(df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0)]

# Create the second dataset with the rest
non_outliers = df[~((df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0))]

# Save the datasets to CSV files (optional)
outliers_negative.to_csv('outliers_negative.csv', index=False)
non_outliers.to_csv('non_outliers.csv', index=False)

# Display the datasets
print("Outliers and Negative Target Values Dataset")
print(outliers_negative.head())

print("\nNon-Outliers Dataset")
print(non_outliers.head())



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Feature': np.random.randn(100) * 20,
    'Target': np.random.uniform(0, 100, size=100)
})

# Split the data into features and target
X = df[['Feature']]
y = df['Target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, return_train_score=True)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()

# Display results in a dataframe
results = {
    'MSE Train': mse_train,
    'MSE Test': mse_test,
    'R^2 Train': r2_train,
    'R^2 Test': r2_test
}
results_df = pd.DataFrame([results])
print(results_df)














import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Feature': np.random.randn(100) * 20,
    'Target': np.random.uniform(0, 100, size=100)
})

# Split the data into features and target
X = df[['Feature']]
y = df['Target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, return_train_score=True)
grid_search.fit(X_train, y_train)

# Get the results in a DataFrame
results = pd.DataFrame(grid_search.cv_results_)

# Calculate RMSE for train and test sets
results['mean_train_rmse'] = np.sqrt(-results['mean_train_score'])
results['mean_test_rmse'] = np.sqrt(-results['mean_test_score'])

# Plot R² scores
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
for learning_rate in param_grid['learning_rate']:
    subset = results[results.param_learning_rate == learning_rate]
    plt.plot(subset['param_n_estimators'], subset['mean_train_score'], label=f'Train - lr={learning_rate}')
    plt.plot(subset['param_n_estimators'], subset['mean_test_score'], linestyle='--', label=f'Test - lr={learning_rate}')
plt.xlabel('Number of Estimators')
plt.ylabel('Neg MSE')
plt.title('Neg MSE vs Number of Estimators')
plt.legend()

# Plot RMSE scores
plt.subplot(1, 2, 2)
for learning_rate in param_grid['learning_rate']:
    subset = results[results.param_learning_rate == learning_rate]
    plt.plot(subset['param_n_estimators'], subset['mean_train_rmse'], label=f'Train - lr={learning_rate}')
    plt.plot(subset['param_n_estimators'], subset['mean_test_rmse'], linestyle='--', label=f'Test - lr={learning_rate}')
plt.xlabel('Number of Estimators')
plt.ylabel('RMSE')
plt.title('RMSE vs Number of Estimators')
plt.legend()

plt.tight_layout()
plt.show()

# Print the best parameters
print("Best Parameters:", grid_search.best_params_)
print("Best R² Score:", grid_search.best_score_)








import xgboost as xgb
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, make_scorer
import pandas as pd
import matplotlib.pyplot as plt

# Sample data preparation (assuming you have your dataset as X and y)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define XGBoost regressor
xgb_reg = xgb.XGBRegressor()

# Define parameter grid
param_grid = {
    'eta': [0.01, 0.02, 0.03],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300]
}

# Define R^2 scorer
r2_scorer = make_scorer(r2_score)

# Perform GridSearchCV
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, scoring=r2_scorer, cv=5, return_train_score=True)
grid_search.fit(X_train, y_train)

# Extract results
results = pd.DataFrame(grid_search.cv_results_)

# Plotting the evolution of R^2
plt.figure(figsize=(14, 7))

plt.plot(results['param_eta'], results['mean_train_score'], label='Train R^2')
plt.plot(results['param_eta'], results['mean_test_score'], label='Test R^2')

plt.xlabel('Learning Rate (eta)')
plt.ylabel('R^2 Score')
plt.title('Evolution of R^2 for different Learning Rates')
plt.legend()
plt.grid(True)
plt.show()






import xgboost as xgb
import pandas as pd
from sklearn.metrics import r2_score

# Define XGBoost parameters
xgb_para = {
    'eta': 0.02,
    'booster': 'gbtree',
    'max_depth': 10
}

# Perform cross-validation
xgb_cv = xgb.cv(
    params=xgb_para,
    dtrain=train_xgb,
    num_boost_round=500,
    nfold=5,
    metrics={'rmse'},  # RMSE is used for early stopping
    as_pandas=True,
    seed=42
)

# Extract the train and test predictions from the cross-validation
train_predictions = xgb_cv['train-rmse-mean']
test_predictions = xgb_cv['test-rmse-mean']

# Calculate R^2 for train and test
train_r2 = [r2_score(train_xgb.get_label(), train_predictions)]
test_r2 = [r2_score(test_xgb.get_label(), test_predictions)]

# Convert R^2 scores to a DataFrame for plotting
r2_df = pd.DataFrame({
    'train-r2': train_r2,
    'test-r2': test_r2
})

# Plot R^2 scores
r2_df.plot()










import xgboost as xgb
from sklearn.model_selection import GridSearchCV
import pandas as pd

# Define the parameter grid
param_grid = {
    'eta': [0.01, 0.02, 0.1],
    'booster': ['gbtree', 'gblinear'],
    'max_depth': [6, 10, 15]
}

# Convert your training data to DMatrix format
dtrain = xgb.DMatrix(data=train_data, label=train_labels)

# Define the model
xgb_model = xgb.XGBRegressor()

# Perform the grid search
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search
grid_search.fit(train_data, train_labels)

# Get the results
cv_results = pd.DataFrame(grid_search.cv_results_)

# Print the best parameters
print("Best parameters:", grid_search.best_params_)

# Plot the results
cv_results[['mean_test_score', 'std_test_score']].plot()
















Rapport sur la Valeur de R² et son Interprétation Statistique
Introduction
La valeur de R², ou coefficient de détermination, est souvent mal comprise. En particulier, beaucoup de gens pensent que R² ne peut être qu'entre 0 et 1. Cependant, il peut être négatif dans certains cas. Ce rapport explique pourquoi et comment cela peut se produire, avec une explication détaillée des calculs de R² et une illustration pratique.

Calcul de R²
Pour comprendre pourquoi R² peut être négatif, nous devons examiner comment il est calculé. Nous utilisons trois variables clés dans ce calcul : RSS (Residual Sum of Squares), TSS (Total Sum of Squares), et ESS (Explained Sum of Squares).

Calcul de RSS :
Pour chaque variable indépendante 
𝑥
x, nous avons une variable dépendante 
𝑦
y. Nous traçons une ligne de régression linéaire qui prédit les valeurs de 
𝑦
y pour chaque valeur de 
𝑥
x. Appelons les valeurs prédites 
𝑦
^
y
^
​
 . L'erreur entre ce que la ligne prédit et les valeurs réelles de 
𝑦
y est calculée par soustraction. Toutes ces différences sont mises au carré et additionnées, ce qui donne la somme des carrés des résidus, RSS.

𝑅
𝑆
𝑆
=
∑
(
𝑦
−
𝑦
^
)
2
RSS=∑(y− 
y
^
​
 ) 
2
 
Calcul de TSS :
Nous pouvons calculer la valeur moyenne de 
𝑦
y, appelée 
𝑦
ˉ
y
ˉ
​
 . Si nous traçons 
𝑦
ˉ
y
ˉ
​
 , c'est simplement une ligne horizontale à travers les données. En soustrayant 
𝑦
ˉ
y
ˉ
​
  de chaque valeur réelle de 
𝑦
y, nous obtenons la somme totale des carrés, TSS.

𝑇
𝑆
𝑆
=
∑
(
𝑦
−
𝑦
ˉ
)
2
TSS=∑(y− 
y
ˉ
​
 ) 
2
 
Calcul de ESS :
Les différences entre les valeurs prédites 
𝑦
^
y
^
​
  et la valeur moyenne 
𝑦
ˉ
y
ˉ
​
  sont mises au carré et additionnées. Ceci est la somme expliquée des carrés, ESS.

𝐸
𝑆
𝑆
=
∑
(
𝑦
^
−
𝑦
ˉ
)
2
ESS=∑( 
y
^
​
 − 
y
ˉ
​
 ) 
2
 
Relation entre TSS, RSS et ESS
Lorsque nous avons une ligne de régression avec une interception, la relation suivante est toujours vraie :

𝑇
𝑆
𝑆
=
𝑅
𝑆
𝑆
+
𝐸
𝑆
𝑆
TSS=RSS+ESS
En divisant tous les termes par TSS et en réarrangeant, nous obtenons :

𝑅
2
=
1
−
𝑅
𝑆
𝑆
𝑇
𝑆
𝑆
R 
2
 =1− 
TSS
RSS
​
 
Dans ce cas, R² est toujours positif.

Cas où R² peut être négatif
Cependant, sans interception, la relation ci-dessus change. La formule devient :

𝑇
𝑆
𝑆
=
𝑅
𝑆
𝑆
+
𝐸
𝑆
𝑆
+
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
TSS=RSS+ESS+2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 )
En divisant tous les termes par TSS, nous obtenons :

𝑅
2
=
𝐸
𝑆
𝑆
+
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
𝑇
𝑆
𝑆
R 
2
 = 
TSS
ESS+2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 )
​
 
Le terme supplémentaire peut rendre le numérateur négatif, ce qui fait que R² peut être négatif. Cela se produit lorsque la ligne horizontale 
𝑦
ˉ
y
ˉ
​
  explique mieux les données que la ligne de régression.

Quand le terme 
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 ) est nul ou non nul
Nul : Ce terme est nul lorsque la ligne de régression passe par le point moyen des données 
(
𝑥
ˉ
,
𝑦
ˉ
)
( 
x
ˉ
 , 
y
ˉ
​
 ), ce qui se produit lorsque le modèle inclut une interception. Dans ce cas, les erreurs 
(
𝑦
−
𝑦
^
)
(y− 
y
^
​
 ) et les différences 
(
𝑦
^
−
𝑦
ˉ
)
( 
y
^
​
 − 
y
ˉ
​
 ) sont orthogonales, ce qui signifie que leur produit est en moyenne nul.
Non nul : Ce terme n'est pas nécessairement nul lorsque la régression ne comprend pas d'interception. Dans ce cas, la ligne de régression peut ne pas passer par le point moyen des données, et les erreurs et les différences peuvent avoir une covariance non nulle, conduisant ainsi à un terme supplémentaire qui peut être positif ou négatif.
Exemple Simple
Considérons un exemple simple pour illustrer ce concept. Supposons que nous ayons les données suivantes :

x	y
1	1
2	2
3	1.3
4	3.75
5	2.25
Calculons la régression linéaire avec et sans interception :

Régression avec interception :
La ligne de régression a une interception et la formule obtenue est :

𝑦
^
=
0.425
+
0.475
𝑥
y
^
​
 =0.425+0.475x
Régression sans interception :
La ligne de régression passe par l'origine (sans interception) et la formule obtenue est :

𝑦
^
=
0.63
𝑥
y
^
​
 =0.63x
La moyenne des 
𝑦
y est 
𝑦
ˉ
=
2.06
y
ˉ
​
 =2.06.

En traçant les lignes de régression et la moyenne de 
𝑦
y sur un graphique, nous observons que la ligne de régression sans interception ne passe pas par la moyenne des données, ce qui peut rendre le terme 
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 ) non nul et potentiellement négatif, rendant ainsi 
𝑅
2
R 
2
  négatif.

Conclusion
La valeur de R² peut être négative si le modèle de régression n'a pas d'interception et s'adapte très mal aux données. Dans ce cas, une ligne horizontale pourrait fournir une meilleure explication des variations dans les données que le modèle lui-même. Comprendre cette nuance est crucial pour une interprétation correcte des résultats de régression linéaire.

Visualisation avec Matplotlib
Voici un code Python qui illustre cet exemple :

python
Copy code
import matplotlib.pyplot as plt
import numpy as np

# Données simples
x = np.array([1, 2, 3, 4, 5])
y = np.array([1, 2, 1.3, 3.75, 2.25])

# Calcul de la régression linéaire avec interception
coeffs_with_intercept = np.polyfit(x, y, 1)
y_hat_with_intercept = np.polyval(coeffs_with_intercept, x)

# Calcul de la régression linéaire sans interception
coeffs_without_intercept = np.polyfit(x, y, 1, full=True)[0]
y_hat_without_intercept = np.polyval([coeffs_without_intercept[0]], x)

# Calcul de la moyenne de y
y_bar = np.mean(y)

# Visualisation des données et des lignes de régression
plt.scatter(x, y, label='Données')
plt.plot(x, y_hat_with_intercept, label='Régression avec interception', color='green')
plt.plot(x, y_hat_without_intercept, label='Régression sans interception', color='red')
plt.axhline(y=y_bar, color='blue', linestyle='--', label='Moyenne de y')

plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Régression Linéaire avec et sans Interception')
plt.show()
Ce graphique montre les données avec deux lignes de régression : une avec interception (en vert) et une sans interception (en rouge), ainsi qu'une ligne horizontale représentant la moyenne de 
𝑦
y (en bleu). La ligne rouge ne passe pas par le point moyen des données, ce qui explique pourquoi 
𝑅
2
R 
2
  peut être négatif dans ce cas.





import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate data that follows the line y = x
np.random.seed(0)
X = np.linspace(-10, 10, 100).reshape(-1, 1)
y = X.flatten()  # y = x

# Fit a linear regression model with the equation y = -x
model = LinearRegression()
model.fit(X, -X)  # The model is trained with y = -x

# Predict using the fitted model
y_pred = model.predict(X)

# Compute R^2 value
r2 = r2_score(y, y_pred)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='black', label='Data: y = x')
plt.plot(X, y_pred, color='red', linewidth=2, label='Model: y = -x')
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f'Linear Regression with y = -x\nR² = {r2:.2f}')
plt.legend()
plt.grid(True)
plt.show()

print(f"R²: {r2:.2f}")












import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate non-linear data
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(80) * 0.5

# Fit linear model
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

# Calculate R^2
r2 = r2_score(y, y_pred)
print(f"R^2: {r2}")

# Plot
plt.scatter(X, y, color='black')
plt.plot(X, y_pred, color='blue', linewidth=3)
plt.title(f'Linear Regression on Non-linear Data\nR^2 = {r2:.2f}')
plt.show()


from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit model on training data
model.fit(X_train, y_train)
y_test_pred = model.predict(X_test)

# Calculate R^2 on test data
r2_test = r2_score(y_test, y_test_pred)
print(f"R^2 on test data: {r2_test}")
















import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
quantiles = df[df['Outlier'] == 'Inlier']['Value'].quantile([0.05, 0.1, 0.6, 0.7, 0.8, 0.9])
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()

# Replace outliers based on specified rules
def replace_outliers(row, quantiles, mean_value):
    noise = np.random.normal(0, 1)
    if row['Outlier'] == 'Outlier':
        if row['Value'] > mean_value:
            if row['Value'] > quantiles[0.9]:
                return quantiles[0.7] + noise
            elif row['Value'] > quantiles[0.8]:
                return quantiles[0.6] + noise
        else:
            return quantiles[0.1] + noise if row['Value'] < quantiles[0.05] else quantiles[0.05] + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, quantiles, mean_value), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model with enable_categorical set to True
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0, enable_categorical=False)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())












import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor, plot_tree
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree and plot the tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group

        # Plot the Decision Tree
        plt.figure(figsize=(12, 8))
        plot_tree(dt_reg, feature_names=[cat_var], filled=True, rounded=True, fontsize=10)
        plt.title(f'Decision Tree for {cat_var}')
        plt.show()
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()









import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()
