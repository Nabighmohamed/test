import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()
















# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.xticks(rotation=45)
plt.show()
















import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_vif(df):
    # Add a constant to the dataframe
    df_with_const = add_constant(df)
    
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = df_with_const.columns
    vif_data["VIF"] = [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])]
    
    return vif_data

# Calculate VIF
vif_data = calculate_vif(df)

# Display the VIF result
print(vif_data)


import matplotlib.pyplot as plt

# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.show()













import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j:
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values')
plt.show()
































import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Store the results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Store the metrics
    results['Model'].append(model_name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1-Score'].append(f1)
    
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")

# Plot the results
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Comparison')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
axes = axes.flatten()

for i, metric in enumerate(metrics):
    axes[i].bar(results['Model'], results[metric], color=['blue', 'green', 'red'])
    axes[i].set_title(metric)
    axes[i].set_ylim([0, 1])
    axes[i].set_xlabel('Model')
    axes[i].set_ylabel(metric)
    for j, val in enumerate(results[metric]):
        axes[i].text(j, val + 0.02, f'{val:.2f}', ha='center')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()




import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Display the matrices
print("Correlation Matrix:")
print(corr_matrix)
print("\nP-Value Matrix:")
print(p_value_matrix)

# Combine the correlation matrix and p-value matrix into one DataFrame for display
combined_matrix = corr_matrix.astype(str) + " (p=" + p_value_matrix.round(4).astype(str) + ")"

print("\nCombined Matrix:")
print(combined_matrix)
















import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")
