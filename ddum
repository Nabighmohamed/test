import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import spearmanr, kendalltau
from sklearn.feature_selection import mutual_info_regression

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Discrete1': np.random.choice([1, 2, 3, 4, 5], size=100),
    'Discrete2': np.random.choice([1, 2, 3], size=100),
    'Target': np.random.uniform(0, 10, size=100)  # Continuous target variable
})

# Encode nominal variables
df_encoded = df.copy()

# Function to compute metrics for discrete variables
def compute_discrete_metrics(df, target):
    results = []
    
    for col in df.columns.difference([target]):
        if df[col].dtype in [np.int64, np.float64]:  # Discrete numeric variables
            spearman_corr, spearman_p = spearmanr(df[col], df[target])
            kendall_tau, kendall_p = kendalltau(df[col], df[target])
            mi = mutual_info_regression(df[[col]], df[target])[0]
            
            results.extend([
                {'Variable': col, 'Metric': 'Spearman Correlation', 'Value': spearman_corr},
                {'Variable': col, 'Metric': 'Kendall Tau', 'Value': kendall_tau},
                {'Variable': col, 'Metric': 'Mutual Information', 'Value': mi}
            ])
    
    return pd.DataFrame(results)

# Compute metrics for discrete variables
results_df = compute_discrete_metrics(df, 'Target')

# Pivot the results DataFrame for heatmap plotting
heatmap_data = results_df.pivot(index='Metric', columns='Variable', values='Value')

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", cbar_kws={'label': 'Value'})
plt.title('Dependency and Explanatory Power of Discrete Variables on Target')
plt.show()




















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import f_oneway

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Target': np.random.uniform(0, 10, size=100)  # Continuous target variable
})

# Encode nominal variables
df_encoded = df.copy()
for col in df.select_dtypes(include=['object']).columns:
    df_encoded[col] = df_encoded[col].astype('category').cat.codes

# Function to compute ANOVA and Eta Squared
def compute_anova_eta_squared(df, target):
    results = []
    for col in df.columns.difference([target]):
        if df[col].dtype == 'object':  # Nominal variables
            groups = [df[target][df[col] == category] for category in df[col].unique()]
            f_stat, p_value = f_oneway(*groups)
            eta_squared = f_stat / (f_stat + (len(df) - len(groups)))
            results.append({
                'Variable': col,
                'Eta Squared': eta_squared
            })
    return pd.DataFrame(results)

# Compute ANOVA and Eta Squared
results_df = compute_anova_eta_squared(df, 'Target')

# Ensure the results DataFrame has unique index for heatmap
results_df.set_index('Variable', inplace=True)

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(results_df.T, annot=True, cmap="coolwarm", cbar_kws={'label': 'Eta Squared'})
plt.title('Explanatory Power of Nominal Variables on Target')
plt.show()














import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import f_oneway

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Target': np.random.uniform(0, 10, size=100)  # Continuous target variable
})

# Encode nominal variables
df_encoded = df.copy()
for col in df.select_dtypes(include=['object']).columns:
    df_encoded[col] = df_encoded[col].astype('category').cat.codes

# Function to compute ANOVA and Eta Squared
def compute_anova_eta_squared(df, target):
    results = []
    for col in df.columns.difference([target]):
        if df[col].dtype == 'object':  # Nominal variables
            groups = [df[target][df[col] == category] for category in df[col].unique()]
            f_stat, p_value = f_oneway(*groups)
            eta_squared = f_stat / (f_stat + (len(df) - len(groups)))
            results.append({
                'Variable': col,
                'F-statistic': f_stat,
                'p-value': p_value,
                'Eta Squared': eta_squared
            })
    return pd.DataFrame(results)

# Compute ANOVA and Eta Squared
results_df = compute_anova_eta_squared(df, 'Target')

# Pivot the results DataFrame for heatmap plotting
heatmap_data = results_df.pivot(index='Variable', columns='Variable', values='Eta Squared')

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", cbar_kws={'label': 'Eta Squared'})
plt.title('Explanatory Power of Nominal Variables on Target')
plt.show()























import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import spearmanr, kendalltau, f_oneway
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import LinearRegression

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Discrete1': np.random.choice([1, 2, 3, 4, 5], size=100),
    'Discrete2': np.random.choice([1, 2, 3], size=100),
    'Target': np.random.uniform(0, 10, size=100)  # Continuous target variable
})

# Encode nominal variables
df_encoded = df.copy()
for col in df.select_dtypes(include=['object']).columns:
    df_encoded[col] = df_encoded[col].astype('category').cat.codes

# Function to compute dependency and explanatory power
def compute_dependency_and_explanatory_power(df, df_encoded, target):
    results = []
    
    for col in df.columns.difference([target]):
        if df[col].dtype == 'object':  # Nominal variables
            groups = [df[target][df[col] == category] for category in df[col].unique()]
            f_stat, p_value = f_oneway(*groups)
            eta_squared = f_stat / (f_stat + (len(df) - len(groups)))
            results.append({
                'Variable': col,
                'Metric': 'Eta Squared',
                'Value': eta_squared
            })
        else:  # Discrete numeric variables
            spearman_corr, spearman_p = spearmanr(df[col], df[target])
            kendall_tau, kendall_p = kendalltau(df[col], df[target])
            mi = mutual_info_regression(df_encoded[[col]], df[target])[0]
            results.extend([
                {'Variable': col, 'Metric': 'Spearman Correlation', 'Value': spearman_corr},
                {'Variable': col, 'Metric': 'Kendall Tau', 'Value': kendall_tau},
                {'Variable': col, 'Metric': 'Mutual Information', 'Value': mi}
            ])
    
    return pd.DataFrame(results)

# Compute dependency and explanatory power
results_df = compute_dependency_and_explanatory_power(df, df_encoded, 'Target')

# Pivot the results DataFrame for heatmap plotting
heatmap_data = results_df.pivot(index='Metric', columns='Variable', values='Value')

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", cbar_kws={'label': 'Explanatory Power'})
plt.title('Explanatory Power of Variables on Target')
plt.show()

# Fit a regression model to get R-squared values
X = df_encoded.drop(columns=['Target'])
y = df_encoded['Target']
reg = LinearRegression().fit(X, y)
r_squared = reg.score(X, y)
print(f"\nOverall R-squared of the model: {r_squared}")





















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest, yeojohnson
from scipy.stats.mstats import winsorize

# Sample DataFrame creation (simulating your scenario)
np.random.seed(0)
df = pd.DataFrame({
    'X1': np.concatenate((np.random.uniform(1, 10, size=950), np.random.uniform(-100, -10, size=50))),
    'X2': np.concatenate((np.random.uniform(1, 10, size=950), np.random.uniform(-100, -10, size=50))),
    'Y': np.concatenate((np.random.uniform(1, 10, size=950), np.random.uniform(-100, -10, size=50)))
})

# Function to apply log transformation with handling negative values
def apply_log_transformation(series):
    min_value = series.min()
    if min_value <= 0:
        shift = abs(min_value) + 1
    else:
        shift = 0
    shifted_series = series + shift
    return np.log(shifted_series), shift

# Function to apply winsorizing
def apply_winsorizing(series, limits=(0.05, 0.05)):
    return winsorize(series, limits=limits)

# Function to plot distributions and box plots
def plot_distributions_and_boxplots(df, transformed_df, winsorized_df):
    for col in df.columns:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # Original distribution
        sns.histplot(df[col], kde=True, ax=axes[0, 0])
        axes[0, 0].set_title(f'Original Distribution of {col}')
        
        # Transformed distribution
        sns.histplot(transformed_df[col], kde=True, ax=axes[0, 1])
        axes[0, 1].set_title(f'Log-Transformed Distribution of {col}')
        
        # Winsorized distribution
        sns.histplot(winsorized_df[col], kde=True, ax=axes[0, 2])
        axes[0, 2].set_title(f'Winsorized Distribution of {col}')
        
        # Original box plot
        sns.boxplot(x=df[col], ax=axes[1, 0])
        axes[1, 0].set_title(f'Original Box plot of {col}')
        
        # Transformed box plot
        sns.boxplot(x=transformed_df[col], ax=axes[1, 1])
        axes[1, 1].set_title(f'Log-Transformed Box plot of {col}')
        
        # Winsorized box plot
        sns.boxplot(x=winsorized_df[col], ax=axes[1, 2])
        axes[1, 2].set_title(f'Winsorized Box plot of {col}')
        
        plt.tight_layout()
        plt.show()

# Apply log transformation and winsorizing to the DataFrame
transformed_df = pd.DataFrame()
winsorized_df = pd.DataFrame()
shifts = {}

for col in df.columns:
    transformed_series, shift = apply_log_transformation(df[col])
    transformed_df[col] = transformed_series
    winsorized_df[col] = apply_winsorizing(transformed_series)
    shifts[col] = shift

# Plot distributions and box plots before and after transformations
plot_distributions_and_boxplots(df, transformed_df, winsorized_df)

# Normality tests
def normality_tests(df, transformed_df, winsorized_df):
    results = []
    for col in df.columns:
        # Original data
        shapiro_test_before = shapiro(df[col])
        kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
        
        # Transformed data
        shapiro_test_after = shapiro(transformed_df[col])
        kstest_after = kstest(transformed_df[col], 'norm', args=(transformed_df[col].mean(), transformed_df[col].std()))
        
        # Winsorized data
        shapiro_test_winsor = shapiro(winsorized_df[col])
        kstest_winsor = kstest(winsorized_df[col], 'norm', args=(winsorized_df[col].mean(), winsorized_df[col].std()))

        results.append({
            'Variable': col,
            'Shapiro Statistic Before': shapiro_test_before.statistic,
            'Shapiro p-value Before': shapiro_test_before.pvalue,
            'K-S Statistic Before': kstest_before.statistic,
            'K-S p-value Before': kstest_before.pvalue,
            'Shapiro Statistic After': shapiro_test_after.statistic,
            'Shapiro p-value After': shapiro_test_after.pvalue,
            'K-S Statistic After': kstest_after.statistic,
            'K-S p-value After': kstest_after.pvalue,
            'Shapiro Statistic Winsor': shapiro_test_winsor.statistic,
            'Shapiro p-value Winsor': shapiro_test_winsor.pvalue,
            'K-S Statistic Winsor': kstest_winsor.statistic,
            'K-S p-value Winsor': kstest_winsor.pvalue
        })
    
    return pd.DataFrame(results)

# Perform normality tests and display results
results_df = normality_tests(df, transformed_df, winsorized_df)
print("\nNormality test results before and after transformations:")
print(results_df)




















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest

# Sample DataFrame creation (simulating your scenario)
np.random.seed(0)
df = pd.DataFrame({
    'Variable': np.concatenate((np.random.uniform(1, 10, size=950), np.random.uniform(-100, -10, size=50)))
})

# Function to apply log transformations with handling negative values
def apply_log_transformations(series):
    transformations = {}
    
    # Shift to handle negative values
    min_value = series.min()
    if min_value <= 0:
        shift = abs(min_value) + 1
    else:
        shift = 0
    
    shifted_series = series + shift
    
    # Different log transformations
    transformations['Log_e'] = (np.log(shifted_series), shift)
    transformations['Log_10'] = (np.log10(shifted_series), shift)
    transformations['Log_2'] = (np.log2(shifted_series), shift)
    
    return transformations

# Function to plot distributions and perform normality tests
def plot_and_test_normality(df, transformations):
    results = []
    for col in df.columns:
        for trans_name, (trans_series, shift) in transformations[col].items():
            # Original data
            shapiro_test_before = shapiro(df[col])
            kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
            
            # Transformed data
            shapiro_test_after = shapiro(trans_series)
            kstest_after = kstest(trans_series, 'norm', args=(trans_series.mean(), trans_series.std()))

            # Plot distributions before and after transformation
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))
            sns.histplot(df[col], kde=True, ax=axes[0])
            axes[0].set_title(f'Original Distribution of {col}')
            sns.histplot(trans_series, kde=True, ax=axes[1])
            axes[1].set_title(f'{trans_name} Transformed Distribution of {col} (Shift: {shift})')
            plt.tight_layout()
            plt.show()

            results.append({
                'Variable': col,
                'Transformation': trans_name,
                'Shapiro Statistic Before': shapiro_test_before.statistic,
                'Shapiro p-value Before': shapiro_test_before.pvalue,
                'K-S Statistic Before': kstest_before.statistic,
                'K-S p-value Before': kstest_before.pvalue,
                'Shapiro Statistic After': shapiro_test_after.statistic,
                'Shapiro p-value After': shapiro_test_after.pvalue,
                'K-S Statistic After': kstest_after.statistic,
                'K-S p-value After': kstest_after.pvalue
            })
    
    return pd.DataFrame(results)

# Apply log transformations to the DataFrame
transformations = {}
for col in df.columns:
    transformations[col] = apply_log_transformations(df[col])

# Perform normality tests and plot distributions
results_df = plot_and_test_normality(df, transformations)

# Display the results
print("\nNormality test results before and after transformations:")
print(results_df)












import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest, yeojohnson, boxcox

# Sample DataFrame creation (simulating your scenario)
np.random.seed(0)
df = pd.DataFrame({
    'Variable': np.concatenate((np.random.uniform(1, 10, size=950), np.random.uniform(-100, -10, size=50)))
})

# Function to apply various transformations
def apply_transformations(series):
    transformations = {}
    
    # Log transformation (shift data to positive values)
    min_value = series.min()
    if min_value <= 0:
        shift = abs(min_value) + 1
    else:
        shift = 0
    shifted_series = series + shift
    transformations['Log'] = (np.log(shifted_series), shift)
    
    # Square Root transformation (shift data to positive values)
    transformations['Square Root'] = (np.sqrt(shifted_series), shift)
    
    # Box-Cox transformation (shift data to positive values)
    boxcox_trans, _ = boxcox(shifted_series)
    transformations['Box-Cox'] = (boxcox_trans, shift)
    
    # Yeo-Johnson transformation (no shift needed)
    yeojohnson_trans, _ = yeojohnson(series)
    transformations['Yeo-Johnson'] = (yeojohnson_trans, 0)
    
    return transformations

# Function to plot distributions and perform normality tests
def plot_and_test_normality(df, transformations):
    results = []
    for col in df.columns:
        for trans_name, (trans_series, shift) in transformations[col].items():
            # Original data
            shapiro_test_before = shapiro(df[col])
            kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
            
            # Transformed data
            shapiro_test_after = shapiro(trans_series)
            kstest_after = kstest(trans_series, 'norm', args=(trans_series.mean(), trans_series.std()))

            # Plot distributions before and after transformation
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))
            sns.histplot(df[col], kde=True, ax=axes[0])
            axes[0].set_title(f'Original Distribution of {col}')
            sns.histplot(trans_series, kde=True, ax=axes[1])
            axes[1].set_title(f'{trans_name} Transformed Distribution of {col} (Shift: {shift})')
            plt.tight_layout()
            plt.show()

            results.append({
                'Variable': col,
                'Transformation': trans_name,
                'Shapiro Statistic Before': shapiro_test_before.statistic,
                'Shapiro p-value Before': shapiro_test_before.pvalue,
                'K-S Statistic Before': kstest_before.statistic,
                'K-S p-value Before': kstest_before.pvalue,
                'Shapiro Statistic After': shapiro_test_after.statistic,
                'Shapiro p-value After': shapiro_test_after.pvalue,
                'K-S Statistic After': kstest_after.statistic,
                'K-S p-value After': kstest_after.pvalue
            })
    
    return pd.DataFrame(results)

# Apply transformations to the DataFrame
transformations = {}
for col in df.columns:
    transformations[col] = apply_transformations(df[col])

# Perform normality tests and plot distributions
results_df = plot_and_test_normality(df, transformations)

# Display the results
print("\nNormality test results before and after transformations:")
print(results_df)





















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest, yeojohnson, boxcox

# Sample DataFrame creation (simulating your scenario)
np.random.seed(0)
df = pd.DataFrame({
    'Variable': np.concatenate((np.random.uniform(1, 10, size=950), np.random.uniform(-100, -10, size=50)))
})

# Function to apply various transformations
def apply_transformations(series):
    transformations = {}
    
    # Log transformation (shift data to positive values)
    min_value = series.min()
    if min_value <= 0:
        shift = abs(min_value) + 1
    else:
        shift = 0
    shifted_series = series + shift
    transformations['Log'] = (np.log(shifted_series), shift)
    
    # Square Root transformation (shift data to positive values)
    transformations['Square Root'] = (np.sqrt(shifted_series), shift)
    
    # Box-Cox transformation (shift data to positive values)
    boxcox_trans, _ = boxcox(shifted_series)
    transformations['Box-Cox'] = (boxcox_trans, shift)
    
    # Yeo-Johnson transformation (no shift needed)
    yeojohnson_trans, _ = yeojohnson(series)
    transformations['Yeo-Johnson'] = (yeojohnson_trans, 0)
    
    return transformations

# Function to plot distributions and perform normality tests
def plot_and_test_normality(df, transformations):
    results = []
    for col in df.columns:
        for trans_name, (trans_series, shift) in transformations[col].items():
            # Original data
            shapiro_test_before = shapiro(df[col])
            kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
            
            # Transformed data
            shapiro_test_after = shapiro(trans_series)
            kstest_after = kstest(trans_series, 'norm', args=(trans_series.mean(), trans_series.std()))

            # Plot distributions before and after transformation
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))
            sns.histplot(df[col], kde=True, ax=axes[0])
            axes[0].set_title(f'Original Distribution of {col}')
            sns.histplot(trans_series, kde=True, ax=axes[1])
            axes[1].set_title(f'{trans_name} Transformed Distribution of {col} (Shift: {shift})')
            plt.tight_layout()
            plt.show()

            results.append({
                'Variable': col,
                'Transformation': trans_name,
                'Shapiro Before': shapiro_test_before.pvalue,
                'K-S Before': kstest_before.pvalue,
                'Shapiro After': shapiro_test_after.pvalue,
                'K-S After': kstest_after.pvalue
            })
    
    return pd.DataFrame(results)

# Apply transformations to the DataFrame
transformations = {}
for col in df.columns:
    transformations[col] = apply_transformations(df[col])

# Perform normality tests and plot distributions
results_df = plot_and_test_normality(df, transformations)

# Display the results
print("\nNormality test results before and after transformations:")
print(results_df)
purchase propensity score
buying propensity score


















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest, yeojohnson, boxcox

# Sample DataFrame creation (including variables with only negative values)
np.random.seed(0)
df = pd.DataFrame({
    'X1': np.random.uniform(-20, -5, size=1000),
    'X2': np.random.uniform(-20, -5, size=1000),
    'Y': np.random.uniform(-20, -5, size=1000)  # Continuous target variable
})

# Function to apply various transformations
def apply_transformations(series):
    transformations = {}
    min_value = series.min()
    
    # Shift to handle negative values
    shift = abs(min_value) + 1
    
    shifted_series = series + shift
    
    # Log transformation
    transformations['Log'] = np.log(shifted_series)
    
    # Square Root transformation
    transformations['Square Root'] = np.sqrt(shifted_series)
    
    # Box-Cox transformation
    boxcox_trans, _ = boxcox(shifted_series)
    transformations['Box-Cox'] = boxcox_trans
    
    # Yeo-Johnson transformation
    yeojohnson_trans, _ = yeojohnson(series)
    transformations['Yeo-Johnson'] = yeojohnson_trans
    
    return transformations, shift

# Function to plot distributions and perform normality tests
def plot_and_test_normality(df, transformations):
    results = []
    for col in df.columns:
        for trans_name, (trans_series, shift) in transformations[col].items():
            # Original data
            shapiro_test_before = shapiro(df[col])
            kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
            
            # Transformed data
            shapiro_test_after = shapiro(trans_series)
            kstest_after = kstest(trans_series, 'norm', args=(trans_series.mean(), trans_series.std()))

            # Plot distributions before and after transformation
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))
            sns.histplot(df[col], kde=True, ax=axes[0])
            axes[0].set_title(f'Original Distribution of {col}')
            sns.histplot(trans_series, kde=True, ax=axes[1])
            axes[1].set_title(f'{trans_name} Transformed Distribution of {col} (Shift: {shift})')
            plt.tight_layout()
            plt.show()

            results.append({
                'Variable': col,
                'Transformation': trans_name,
                'Shapiro Before': shapiro_test_before.pvalue,
                'K-S Before': kstest_before.pvalue,
                'Shapiro After': shapiro_test_after.pvalue,
                'K-S After': kstest_after.pvalue
            })
    
    return pd.DataFrame(results)

# Randomly sample the DataFrame
sample_size = 1000  # Adjust the sample size as needed
df_sample = df.sample(n=sample_size, random_state=0)

# Apply transformations to the sampled DataFrame
transformations = {}
for col in df_sample.columns:
    transformations[col] = {}
    trans, shift = apply_transformations(df_sample[col])
    for key, value in trans.items():
        transformations[col][key] = (value, shift)

# Perform normality tests and plot distributions
results_df = plot_and_test_normality(df_sample, transformations)

# Display the results
print("\nNormality test results before and after transformations:")
print(results_df)















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest, norm, boxcox, yeojohnson

# Sample DataFrame creation (simulating a large dataset)
np.random.seed(0)
df = pd.DataFrame({
    'X1': np.random.uniform(-10, 10, size=10000),
    'X2': np.random.uniform(-10, 10, size=10000),
    'Y': np.random.uniform(-10, 10, size=10000)  # Continuous target variable
})

# Function to apply various transformations
def apply_transformations(series):
    transformations = {}
    min_value = series.min()
    
    # Shift to handle negative values
    if min_value <= 0:
        shift = abs(min_value) + 1
    else:
        shift = 0
    
    shifted_series = series + shift
    
    # Log transformation
    transformations['Log'] = np.log(shifted_series)
    
    # Square Root transformation
    transformations['Square Root'] = np.sqrt(shifted_series)
    
    # Box-Cox transformation
    boxcox_trans, _ = boxcox(shifted_series + 1)  # Adding 1 to handle zero values
    transformations['Box-Cox'] = boxcox_trans
    
    # Yeo-Johnson transformation
    yeojohnson_trans, _ = yeojohnson(series)
    transformations['Yeo-Johnson'] = yeojohnson_trans
    
    return transformations

# Function to plot distributions and perform normality tests
def plot_and_test_normality(df, transformations):
    results = []
    for col in df.columns:
        for trans_name, trans_series in transformations[col].items():
            # Original data
            shapiro_test_before = shapiro(df[col])
            kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
            
            # Transformed data
            shapiro_test_after = shapiro(trans_series)
            kstest_after = kstest(trans_series, 'norm', args=(trans_series.mean(), trans_series.std()))

            # Plot distributions before and after transformation
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))
            sns.histplot(df[col], kde=True, ax=axes[0])
            axes[0].set_title(f'Original Distribution of {col}')
            sns.histplot(trans_series, kde=True, ax=axes[1])
            axes[1].set_title(f'{trans_name} Transformed Distribution of {col}')
            plt.tight_layout()
            plt.show()

            results.append({
                'Variable': col,
                'Transformation': trans_name,
                'Shapiro Before': shapiro_test_before.pvalue,
                'K-S Before': kstest_before.pvalue,
                'Shapiro After': shapiro_test_after.pvalue,
                'K-S After': kstest_after.pvalue
            })
    
    return pd.DataFrame(results)

# Randomly sample the DataFrame
sample_size = 1000  # Adjust the sample size as needed
df_sample = df.sample(n=sample_size, random_state=0)

# Apply transformations to the sampled DataFrame
transformations = {}
for col in df_sample.columns:
    transformations[col] = apply_transformations(df_sample[col])

# Perform normality tests and plot distributions
results_df = plot_and_test_normality(df_sample, transformations)

# Display the results
print("\nNormality test results before and after transformations:")
print(results_df)




















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest, norm

# Sample DataFrame creation (simulating a large dataset)
np.random.seed(0)
df = pd.DataFrame({
    'X1': np.random.uniform(-10, 10, size=10000),
    'X2': np.random.uniform(-10, 10, size=10000),
    'Y': np.random.uniform(-10, 10, size=10000)  # Continuous target variable
})

# Function to apply log transformation with handling negative values
def log_transform_with_shift(series):
    min_value = series.min()
    if min_value <= 0:
        shift = abs(min_value) + 1
    else:
        shift = 0
    return np.log(series + shift), shift

# Function to plot distributions and perform normality tests
def plot_and_test_normality(df, transformed_df):
    results = []
    for col in df.columns:
        # Original data
        shapiro_test_before = shapiro(df[col])
        kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
        
        # Transformed data
        shapiro_test_after = shapiro(transformed_df[col])
        kstest_after = kstest(transformed_df[col], 'norm', args=(transformed_df[col].mean(), transformed_df[col].std()))

        # Plot distributions before and after transformation
        fig, axes = plt.subplots(1, 2, figsize=(12, 6))
        sns.histplot(df[col], kde=True, ax=axes[0])
        axes[0].set_title(f'Original Distribution of {col}')
        sns.histplot(transformed_df[col], kde=True, ax=axes[1])
        axes[1].set_title(f'Log-Transformed Distribution of {col}')
        plt.tight_layout()
        plt.show()

        results.append({
            'Variable': col,
            'Shapiro Before': shapiro_test_before.pvalue,
            'K-S Before': kstest_before.pvalue,
            'Shapiro After': shapiro_test_after.pvalue,
            'K-S After': kstest_after.pvalue
        })
    
    return pd.DataFrame(results)

# Randomly sample the DataFrame
sample_size = 1000  # Adjust the sample size as needed
df_sample = df.sample(n=sample_size, random_state=0)

# Apply log transformation to the sampled DataFrame
transformed_df = pd.DataFrame()
shifts = {}
for col in df_sample.columns:
    transformed_df[col], shift = log_transform_with_shift(df_sample[col])
    shifts[col] = shift

# Perform normality tests and plot distributions
results_df = plot_and_test_normality(df_sample, transformed_df)

# Display the results
print("Shifts applied to each variable to make all values positive:")
print(shifts)
print("\nNormality test results before and after transformation:")
print(results_df)










import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest, norm

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'X1': np.random.uniform(-10, 10, size=100),
    'X2': np.random.uniform(-10, 10, size=100),
    'Y': np.random.uniform(-10, 10, size=100)  # Continuous target variable
})

# Function to apply log transformation with handling negative values
def log_transform_with_shift(series):
    min_value = series.min()
    if min_value <= 0:
        shift = abs(min_value) + 1
    else:
        shift = 0
    return np.log(series + shift), shift

# Function to plot distributions and perform normality tests
def plot_and_test_normality(df, transformed_df):
    results = []
    for col in df.columns:
        # Original data
        shapiro_test_before = shapiro(df[col])
        kstest_before = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
        
        # Transformed data
        shapiro_test_after = shapiro(transformed_df[col])
        kstest_after = kstest(transformed_df[col], 'norm', args=(transformed_df[col].mean(), transformed_df[col].std()))

        # Plot distributions before and after transformation
        fig, axes = plt.subplots(1, 2, figsize=(12, 6))
        sns.histplot(df[col], kde=True, ax=axes[0])
        axes[0].set_title(f'Original Distribution of {col}')
        sns.histplot(transformed_df[col], kde=True, ax=axes[1])
        axes[1].set_title(f'Log-Transformed Distribution of {col}')
        plt.tight_layout()
        plt.show()

        results.append({
            'Variable': col,
            'Shapiro Before': shapiro_test_before.pvalue,
            'K-S Before': kstest_before.pvalue,
            'Shapiro After': shapiro_test_after.pvalue,
            'K-S After': kstest_after.pvalue
        })
    
    return pd.DataFrame(results)

# Apply log transformation to the DataFrame
transformed_df = pd.DataFrame()
shifts = {}
for col in df.columns:
    transformed_df[col], shift = log_transform_with_shift(df[col])
    shifts[col] = shift

# Perform normality tests and plot distributions
results_df = plot_and_test_normality(df, transformed_df)

# Display the results
print("Shifts applied to each variable to make all values positive:")
print(shifts)
print("\nNormality test results before and after transformation:")
print(results_df)












import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import spearmanr, kendalltau
from sklearn.metrics import mutual_info_score
from sklearn.feature_selection import mutual_info_regression
from itertools import combinations

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Discrete1': np.random.choice([1, 2, 3], size=100),
    'Discrete2': np.random.choice([1, 2, 3, 4], size=100),
    'Target': np.random.uniform(0, 1, size=100)  # Continuous target variable
})

# Function to encode categorical variables
def encode_categories(df):
    df_encoded = df.copy()
    for col in df_encoded.select_dtypes(include=['object', 'category']).columns:
        df_encoded[col] = df_encoded[col].astype('category').cat.codes
    return df_encoded

# Encode the DataFrame
df_encoded = encode_categories(df)

# Function to calculate metrics
def calculate_metrics(df, target):
    cols = df.columns.difference([target])
    mutual_info = pd.DataFrame(index=cols, columns=cols)
    spearman_corr = pd.DataFrame(index=cols, columns=cols)
    kendall_tau = pd.DataFrame(index=cols, columns=cols)
    
    for var1, var2 in combinations(cols, 2):
        if df[var1].dtype.name == 'category' or df[var2].dtype.name == 'category':
            # Categorical variables
            mi1 = mutual_info_score(df[target], df[var1])
            mi2 = mutual_info_score(df[target], df[var2])
        else:
            # Discrete numeric variables
            mi1 = mutual_info_regression(df[[var1]], df[target]).item()
            mi2 = mutual_info_regression(df[[var2]], df[target]).item()
        
        mutual_info.loc[var1, var2] = (mi1 + mi2) / 2
        mutual_info.loc[var2, var1] = (mi1 + mi2) / 2
        
        # Calculate Spearman and Kendall correlation for numeric variables
        if df[var1].dtype in [np.int64, np.float64] and df[var2].dtype in [np.int64, np.float64]:
            spearman_corr.loc[var1, var2], _ = spearmanr(df[var1], df[target])
            spearman_corr.loc[var2, var1] = spearman_corr.loc[var1, var2]
            kendall_tau.loc[var1, var2], _ = kendalltau(df[var1], df[target])
            kendall_tau.loc[var2, var1] = kendall_tau.loc[var1, var2]
    
    return mutual_info, spearman_corr, kendall_tau

# Calculate metrics
mutual_info, spearman_corr, kendall_tau = calculate_metrics(df_encoded, 'Target')

# Fill diagonal with 1s for visualization purposes
np.fill_diagonal(mutual_info.values, 1)
np.fill_diagonal(spearman_corr.values, 1)
np.fill_diagonal(kendall_tau.values, 1)

# Plot the heatmaps
plt.figure(figsize=(20, 6))

plt.subplot(1, 3, 1)
sns.heatmap(mutual_info.astype(float), annot=True, cmap='coolwarm')
plt.title('Mutual Information')

plt.subplot(1, 3, 2)
sns.heatmap(spearman_corr.astype(float), annot=True, cmap='coolwarm')
plt.title("Spearman's Rank Correlation")

plt.subplot(1, 3, 3)
sns.heatmap(kendall_tau.astype(float), annot=True, cmap='coolwarm')
plt.title("Kendall's Tau")

plt.tight_layout()
plt.show()

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Discrete1': np.random.choice([1, 2, 3], size=100),
    'Discrete2': np.random.choice([1, 2, 3, 4], size=100),
    'Target': np.random.uniform(0, 1, size=100)  # Continuous target variable
})

# Function to encode categorical variables
def encode_categories(df):
    df_encoded = df.copy()
    for col in df_encoded.select_dtypes(include=['object', 'category']).columns:
        df_encoded[col] = df_encoded[col].astype('category').cat.codes
    return df_encoded

# Encode the DataFrame
df_encoded = encode_categories(df)

# Function to train a decision tree regressor and group categories
def group_categories_with_decision_tree(df, df_encoded, col, target):
    X = df_encoded[[col]]
    y = df_encoded[target]
    tree = DecisionTreeRegressor(max_leaf_nodes=4, random_state=0)  # Limit leaf nodes for simplicity
    tree.fit(X, y)
    
    # Plot the decision tree
    plt.figure(figsize=(12, 8))
    plot_tree(tree, feature_names=[col], filled=True)
    plt.title(f'Decision Tree for {col}')
    plt.show()
    
    # Get the decision path for each category
    categories = df[col].unique()
    grouped_categories = {}
    for category in categories:
        encoded_category = df_encoded[df[col] == category][col].iloc[0]
        node_indicator = tree.decision_path([[encoded_category]])
        leave_id = tree.apply([[encoded_category]])
        group = leave_id[0]
        if group not in grouped_categories:
            grouped_categories[group] = []
        grouped_categories[group].append(category)
    
    return grouped_categories

# Apply the function for all nominal and discrete variables
nominal_vars = df.select_dtypes(include=['object']).columns
discrete_vars = df.select_dtypes(include=['int64']).columns.difference(['Target'])

all_groupings = {}

for col in nominal_vars:
    groupings = group_categories_with_decision_tree(df, df_encoded, col, 'Target')
    all_groupings[col] = groupings
    print(f"Groupings for {col}: {groupings}")

for col in discrete_vars:
    groupings = group_categories_with_decision_tree(df, df_encoded, col, 'Target')
    all_groupings[col] = groupings
    print(f"Groupings for {col}: {groupings}")

# Display the final groupings
print("\nFinal Groupings:")
for col, groupings in all_groupings.items():
    print(f"\nColumn: {col}")
    for group, categories in groupings.items():
        print(f"  Group {group}: {categories}")
























import pandas as pd
import matplotlib.pyplot as plt

# Assuming the dataframes are named df1, df2, ..., df12 and are already loaded into the environment.
dataframes = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12]

# Initialize a list to store mean differences
mean_differences = []

# Loop through each dataframe, starting from the second one
for i in range(1, len(dataframes)):
    current_df = dataframes[i]
    previous_df = dataframes[i - 1]
    
    # Merge the two dataframes on cclikpi to align the clients
    merged_df = current_df.merge(previous_df, on='cclikpi', suffixes=('_current', '_previous'))
    
    # Calculate the difference in pnb_annuel
    merged_df['pnb_diff'] = merged_df['pnb_annuel_current'] - merged_df['pnb_annuel_previous']
    
    # Compute the mean of the differences
    mean_difference = merged_df['pnb_diff'].mean()
    
    # Append the mean difference to the list
    mean_differences.append(mean_difference)

# Create a DataFrame to store the results for plotting
mean_differences_df = pd.DataFrame({
    'Year': range(2, len(dataframes) + 1),
    'Mean Difference': mean_differences
})

# Plot the mean differences
plt.figure(figsize=(10, 6))
plt.plot(mean_differences_df['Year'], mean_differences_df['Mean Difference'], marker='o', linestyle='-')
plt.xlabel('Year')
plt.ylabel('Mean Difference in PNB Annuel')
plt.title('Mean Difference in PNB Annuel Across Years')
plt.grid(True)
plt.show()




















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Discrete1': np.random.choice([1, 2, 3], size=100),
    'Discrete2': np.random.choice([1, 2, 3, 4], size=100),
    'Target': np.random.choice([0, 1], size=100)  # Binary target variable
})

# Function to encode categorical variables
def encode_categories(df):
    df_encoded = df.copy()
    for col in df_encoded.select_dtypes(include=['object', 'category']).columns:
        df_encoded[col] = df_encoded[col].astype('category').cat.codes
    return df_encoded

# Encode the DataFrame
df_encoded = encode_categories(df)

# Function to train a decision tree and group categories
def group_categories_with_decision_tree(df, df_encoded, col, target):
    X = df_encoded[[col]]
    y = df_encoded[target]
    tree = DecisionTreeClassifier(max_leaf_nodes=4, random_state=0)  # Limit leaf nodes for simplicity
    tree.fit(X, y)
    
    # Plot the decision tree
    plt.figure(figsize=(12, 8))
    plot_tree(tree, feature_names=[col], class_names=['0', '1'], filled=True)
    plt.title(f'Decision Tree for {col}')
    plt.show()
    
    # Get the decision path for each category
    categories = df[col].unique()
    grouped_categories = {}
    for category in categories:
        encoded_category = df_encoded[df[col] == category][col].iloc[0]
        node_indicator = tree.decision_path([[encoded_category]])
        leave_id = tree.apply([[encoded_category]])
        group = leave_id[0]
        if group not in grouped_categories:
            grouped_categories[group] = []
        grouped_categories[group].append(category)
    
    return grouped_categories

# Apply the function for all nominal and discrete variables
nominal_vars = df.select_dtypes(include=['object']).columns
discrete_vars = df.select_dtypes(include=['int64']).columns.difference(['Target'])

all_groupings = {}

for col in nominal_vars:
    groupings = group_categories_with_decision_tree(df, df_encoded, col, 'Target')
    all_groupings[col] = groupings
    print(f"Groupings for {col}: {groupings}")

for col in discrete_vars:
    groupings = group_categories_with_decision_tree(df, df_encoded, col, 'Target')
    all_groupings[col] = groupings
    print(f"Groupings for {col}: {groupings}")

# Display the final groupings
print("\nFinal Groupings:")
for col, groupings in all_groupings.items():
    print(f"\nColumn: {col}")
    for group, categories in groupings.items():
        print(f"  Group {group}: {categories}")























import pandas as pd

# Assuming the dataframes are named df1, df2, ..., df12 and are already loaded into the environment.
dataframes = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12]

# Specific cclikpi to track
specific_cclikpi = 'your_specific_cclikpi'  # Replace with the actual cclikpi value

# Initialize a list to store results
pnb_annuel_across_months = []

# Loop through each dataframe and extract pnb_annuel for the specific cclikpi
for i, df in enumerate(dataframes, 1):
    filtered_df = df[df['cclikpi'] == specific_cclikpi]
    for _, row in filtered_df.iterrows():
        pnb_annuel_across_months.append({
            'Month': i,
            'pnb_annuel': row['pnb_annuel']
        })

# Create a DataFrame to display results
pnb_annuel_df = pd.DataFrame(pnb_annuel_across_months)

import ace_tools as tools; tools.display_dataframe_to_user(name="PNB Annuel for Specific cclikpi Across Months", dataframe=pnb_annuel_df)

pnb_annuel_df




















import pandas as pd

# Assuming the dataframes are named df1, df2, ..., df12 and are already loaded into the environment.
dataframes = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12]

# Function to calculate proportions
def calculate_proportions(df):
    unique_clients = df['cclikpi'].unique()
    total_clients = len(unique_clients)
    
    positive_clients = df[df['pnb_annuel'] > 0]['cclikpi'].nunique()
    negative_clients = df[df['pnb_annuel'] < 0]['cclikpi'].nunique()
    
    positive_proportion = positive_clients / total_clients if total_clients else 0
    negative_proportion = negative_clients / total_clients if total_clients else 0
    
    return positive_proportion, negative_proportion

# Initialize a list to store results
results = []

# Loop through each dataframe and calculate proportions
for i, df in enumerate(dataframes, 1):
    pos_prop, neg_prop = calculate_proportions(df)
    results.append({
        'Month': i,
        'Positive Proportion': pos_prop,
        'Negative Proportion': neg_prop
    })

# Create a DataFrame to display results
results_df = pd.DataFrame(results)

import ace_tools as tools; tools.display_dataframe_to_user(name="Proportions of Positive and Negative PNB Annuel", dataframe=results_df)

results_df




















import pandas as pd

# Assuming you have 12 dataframes, named df1, df2, ..., df12
# Replace these with your actual dataframes
dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12]

# Combine all dataframes into a single dataframe
combined_df = pd.concat(dfs)

# Group by 'cclikpi' to get unique clients
unique_clients = combined_df.drop_duplicates(subset=['cclikpi'])

# Calculate the proportion of unique clients for each value in 'col'
unique_clients_prop = unique_clients['col'].value_counts(normalize=True).reset_index()
unique_clients_prop.columns = ['col', 'proportion']

# Display the results
import ace_tools as tools; tools.display_dataframe_to_user(name="Proportion of Unique Clients", dataframe=unique_clients_prop)






















import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, kruskal
from itertools import combinations

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Discrete1': np.random.choice([1, 2, 3], size=100),
    'Discrete2': np.random.choice([1, 2, 3, 4], size=100),
    'Target': np.random.choice([0, 1], size=100)  # Binary target variable
})

# Function to perform Chi-Squared test and group similar categories for nominal variables
def group_nominal(df, col, target):
    categories = df[col].unique()
    cluster_info = []
    while True:
        min_p_value = 1
        to_merge = None
        for cat1, cat2 in combinations(categories, 2):
            contingency_table = pd.crosstab(df[col].replace({cat1: 'group', cat2: 'group'}), df[target])
            chi2, p, _, _ = chi2_contingency(contingency_table)
            if p < min_p_value:
                min_p_value = p
                to_merge = (cat1, cat2)
        if min_p_value > 0.05:  # No significant difference found
            break
        else:
            df[col] = df[col].replace({to_merge[0]: to_merge[1]})
            cluster_info.append((to_merge[0], to_merge[1], min_p_value))
            categories = df[col].unique()
    return df, cluster_info

# Function to perform Kruskal-Wallis test and group similar categories for discrete numeric variables
def group_discrete(df, col, target):
    categories = df[col].unique()
    cluster_info = []
    while True:
        min_p_value = 1
        to_merge = None
        for cat1, cat2 in combinations(categories, 2):
            stat, p = kruskal(df[df[col] == cat1][target], df[df[col] == cat2][target])
            if p < min_p_value:
                min_p_value = p
                to_merge = (cat1, cat2)
        if min_p_value > 0.05:  # No significant difference found
            break
        else:
            df[col] = df[col].replace({to_merge[0]: to_merge[1]})
            cluster_info.append((to_merge[0], to_merge[1], min_p_value))
            categories = df[col].unique()
    return df, cluster_info

# Apply grouping for all nominal variables and store cluster information
nominal_vars = df.select_dtypes(include=['object']).columns
nominal_clusters = {}
for col in nominal_vars:
    df, clusters = group_nominal(df, col, 'Target')
    nominal_clusters[col] = clusters

# Apply grouping for all discrete numeric variables and store cluster information
discrete_vars = df.select_dtypes(include=['int64']).columns.difference(['Target'])
discrete_clusters = {}
for col in discrete_vars:
    df, clusters = group_discrete(df, col, 'Target')
    discrete_clusters[col] = clusters

# Display the cluster information for nominal variables
print("Nominal Variable Clusters:")
for col, clusters in nominal_clusters.items():
    print(f"\nColumn: {col}")
    for cluster in clusters:
        print(f"  Merged {cluster[0]} and {cluster[1]} with p-value: {cluster[2]:.4f}")

# Display the cluster information for discrete numeric variables
print("\nDiscrete Numeric Variable Clusters:")
for col, clusters in discrete_clusters.items():
    print(f"\nColumn: {col}")
    for cluster in clusters:
        print(f"  Merged {cluster[0]} and {cluster[1]} with p-value: {cluster[2]:.4f}")

# Display the modified DataFrame
print("\nModified DataFrame:")
print(df.head())












import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, kruskal
from itertools import combinations

# Sample DataFrame creation
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=100),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Discrete1': np.random.choice([1, 2, 3], size=100),
    'Discrete2': np.random.choice([1, 2, 3, 4], size=100),
    'Target': np.random.choice([0, 1], size=100)  # Binary target variable
})

# Function to perform Chi-Squared test and group similar categories for nominal variables
def group_nominal(df, col, target):
    categories = df[col].unique()
    grouped = False
    while not grouped:
        min_p_value = 1
        to_merge = None
        for cat1, cat2 in combinations(categories, 2):
            contingency_table = pd.crosstab(df[col].replace({cat1: 'group', cat2: 'group'}), df[target])
            chi2, p, _, _ = chi2_contingency(contingency_table)
            if p < min_p_value:
                min_p_value = p
                to_merge = (cat1, cat2)
        if min_p_value > 0.05:  # No significant difference found
            grouped = True
        else:
            df[col] = df[col].replace({to_merge[0]: to_merge[1]})
            categories = df[col].unique()
    return df

# Function to perform Kruskal-Wallis test and group similar categories for discrete numeric variables
def group_discrete(df, col, target):
    categories = df[col].unique()
    grouped = False
    while not grouped:
        min_p_value = 1
        to_merge = None
        for cat1, cat2 in combinations(categories, 2):
            stat, p = kruskal(df[df[col] == cat1][target], df[df[col] == cat2][target])
            if p < min_p_value:
                min_p_value = p
                to_merge = (cat1, cat2)
        if min_p_value > 0.05:  # No significant difference found
            grouped = True
        else:
            df[col] = df[col].replace({to_merge[0]: to_merge[1]})
            categories = df[col].unique()
    return df

# Apply grouping for all nominal variables
nominal_vars = df.select_dtypes(include=['object']).columns
for col in nominal_vars:
    df = group_nominal(df, col, 'Target')

# Apply grouping for all discrete numeric variables
discrete_vars = df.select_dtypes(include=['int64']).columns.difference(['Target'])
for col in discrete_vars:
    df = group_discrete(df, col, 'Target')

# Display the modified DataFrame
print(df.head())




















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats.mstats import winsorize

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 100) for i in range(10)  # Using 10 variables for demonstration
})

# Function to apply winsorizing to a DataFrame
def winsorize_df(df, limits):
    df_winsorized = df.copy()
    for col in df.columns:
        df_winsorized[col] = winsorize(df[col], limits=limits)
    return df_winsorized

# Set the limits for winsorizing (e.g., 5th and 95th percentiles)
limits = (0.05, 0.05)

# Apply winsorizing to the DataFrame
df_winsorized = winsorize_df(df, limits)

# Function to plot box plots before and after winsorizing
def plot_box_plots_before_after(df, df_winsorized):
    num_cols = df.columns
    for col in num_cols:
        fig, axes = plt.subplots(1, 2, figsize=(12, 6))
        
        # Original box plot
        sns.boxplot(x=df[col], ax=axes[0])
        axes[0].set_title(f'Original Box plot of {col}')
        
        # Winsorized box plot
        sns.boxplot(x=df_winsorized[col], ax=axes[1])
        axes[1].set_title(f'Winsorized Box plot of {col}')
        
        plt.tight_layout()
        plt.show()

# Plot the box plots before and after winsorizing
plot_box_plots_before_after(df, df_winsorized)















import numpy as np
import pandas as pd
from itertools import combinations
from scipy.stats import spearmanr, kendalltau, chi2_contingency

# Create a sample DataFrame with discrete numeric values
np.random.seed(0)
df = pd.DataFrame({
    'Discrete1': np.random.randint(1, 6, size=100),
    'Discrete2': np.random.randint(1, 4, size=100),
    'Discrete3': np.random.randint(1, 6, size=100),
    'Discrete4': np.random.randint(1, 5, size=100)
})

# Identify discrete numeric columns
discrete_vars = df.select_dtypes(include=['int64']).columns.tolist()

# Function to compute all correlations and tests
def compute_correlations(df, discrete_vars):
    results = []
    for var1, var2 in combinations(discrete_vars, 2):
        spearman_corr, spearman_p_value = spearmanr(df[var1], df[var2])
        kendall_corr, kendall_p_value = kendalltau(df[var1], df[var2])
        contingency_table = pd.crosstab(df[var1], df[var2])
        chi2, chi2_p_value, _, _ = chi2_contingency(contingency_table)
        
        results.append({
            'Variable 1': var1,
            'Variable 2': var2,
            "Spearman's Correlation": spearman_corr,
            "Spearman's p-value": spearman_p_value,
            "Kendall's Tau": kendall_corr,
            "Kendall's p-value": kendall_p_value,
            'Chi-Squared': chi2,
            'Chi-Squared p-value': chi2_p_value
        })
    return pd.DataFrame(results)

# Compute correlations and tests
correlation_results = compute_correlations(df, discrete_vars)

# Define a threshold for filtering highly correlated variables
correlation_threshold = 0.5  # Adjust as needed
p_value_threshold = 0.05

# Filter results for highly correlated pairs
highly_correlated = correlation_results[
    ((correlation_results["Spearman's Correlation"].abs() >= correlation_threshold) & (correlation_results["Spearman's p-value"] <= p_value_threshold)) |
    ((correlation_results["Kendall's Tau"].abs() >= correlation_threshold) & (correlation_results["Kendall's p-value"] <= p_value_threshold)) |
    ((correlation_results['Chi-Squared'] > chi2_contingency(pd.crosstab(df[correlation_results['Variable 1']], df[correlation_results['Variable 2']]))[0]) & (correlation_results['Chi-Squared p-value'] <= p_value_threshold))
]

# Display the filtered results
print("Highly Correlated Variables:")
print(highly_correlated)



















import numpy as np
import pandas as pd
from itertools import combinations
from scipy.stats import spearmanr, kendalltau, chi2_contingency

# Create a sample DataFrame with discrete numeric values
np.random.seed(0)
df = pd.DataFrame({
    'Discrete1': np.random.randint(1, 6, size=100),
    'Discrete2': np.random.randint(1, 4, size=100),
    'Discrete3': np.random.randint(1, 6, size=100),
    'Discrete4': np.random.randint(1, 5, size=100)
})

# Identify discrete numeric columns
discrete_vars = df.select_dtypes(include=['int64']).columns.tolist()

# Function to compute all correlations and tests
def compute_correlations(df, discrete_vars):
    results = []
    for var1, var2 in combinations(discrete_vars, 2):
        spearman_corr, spearman_p_value = spearmanr(df[var1], df[var2])
        kendall_corr, kendall_p_value = kendalltau(df[var1], df[var2])
        contingency_table = pd.crosstab(df[var1], df[var2])
        chi2, chi2_p_value, _, _ = chi2_contingency(contingency_table)
        
        results.append({
            'Variable 1': var1,
            'Variable 2': var2,
            "Spearman's Correlation": spearman_corr,
            "Spearman's p-value": spearman_p_value,
            "Kendall's Tau": kendall_corr,
            "Kendall's p-value": kendall_p_value,
            'Chi-Squared': chi2,
            'Chi-Squared p-value': chi2_p_value
        })
    return pd.DataFrame(results)

# Compute correlations and tests
correlation_results = compute_correlations(df, discrete_vars)

# Display the results
print(correlation_results)


















import pandas as pd

def filter_columns_by_cardinality(df, threshold):
    """
    Filters columns in the DataFrame based on the cardinality of unique values.

    Parameters:
    df (pd.DataFrame): The input DataFrame.
    threshold (int): The threshold for the maximum number of unique values.

    Returns:
    pd.DataFrame: A DataFrame containing only the columns with unique values less than the threshold.
    """
    low_cardinality_cols = [col for col in df.columns if df[col].nunique() < threshold]
    return df[low_cardinality_cols]

# Example usage
# Create a sample DataFrame
data = {
    'A': [1, 2, 3, 4, 5],
    'B': ['apple', 'banana', 'apple', 'banana', 'apple'],
    'C': ['cat', 'dog', 'cat', 'dog', 'cat'],
    'D': [100, 200, 300, 400, 500],
    'E': ['red', 'blue', 'green', 'blue', 'red'],
    'F': [1, 2, 3, 4, 1]
}

df = pd.DataFrame(data)

# Set the threshold for cardinality
threshold = 4

# Get the sub DataFrame with columns having unique values less than the threshold
sub_df = filter_columns_by_cardinality(df, threshold)

print("Original DataFrame:")
print(df)
print("\nSub DataFrame with columns having unique values less than the threshold:")
print(sub_df)












import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency

# Create a sample DataFrame with nominal data
np.random.seed(0)
df = pd.DataFrame({
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=1000),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=1000),
    'Nominal3': np.random.choice(['D', 'E', 'F'], size=1000)
})

# Function to calculate Cramr's V for nominal data
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    k = min(confusion_matrix.shape)
    return np.sqrt(chi2 / (n * (k - 1)))

# Function to extract nominal variables and calculate their correlations
def calculate_nominal_correlations(df):
    nominal_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()
    corr_matrix = pd.DataFrame(index=nominal_vars, columns=nominal_vars)
    
    for var1 in nominal_vars:
        for var2 in nominal_vars:
            if var1 == var2:
                corr_matrix.loc[var1, var2] = 1.0
            else:
                corr_matrix.loc[var1, var2] = cramers_v(df[var1], df[var2])
    
    return corr_matrix.astype(float)

# Calculate correlation matrix for nominal variables
nominal_correlation_matrix = calculate_nominal_correlations(df)

# Plot the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(nominal_correlation_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5)
plt.title('Cramr\'s V Correlation Matrix for Nominal Variables')
plt.show()

# Show the correlation matrix
print(nominal_correlation_matrix)















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import spearmanr, chi2_contingency
from scipy.special import kl_div

# Create a sample DataFrame with continuous, discrete, and nominal data
np.random.seed(0)
df = pd.DataFrame({
    'Continuous1': np.random.normal(0, 1, 1000),
    'Continuous2': np.random.normal(5, 2, 1000),
    'Discrete1': np.random.randint(1, 6, size=1000),
    'Discrete2': np.random.randint(1, 4, size=1000),
    'Nominal1': np.random.choice(['A', 'B', 'C'], size=1000),
    'Nominal2': np.random.choice(['X', 'Y', 'Z'], size=1000)
})

# Function to calculate Cramr's V for nominal data
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = sum(confusion_matrix.sum())
    return np.sqrt(chi2 / (n * (min(confusion_matrix.shape) - 1)))

# Function to extract variable types
def extract_variable_types(df):
    continuous_vars = df.select_dtypes(include=['float64']).columns.tolist()
    discrete_vars = df.select_dtypes(include=['int64']).columns.tolist()
    nominal_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()
    return continuous_vars, discrete_vars, nominal_vars

# Function to calculate correlations
def calculate_correlations(df):
    continuous_vars, discrete_vars, nominal_vars = extract_variable_types(df)
    all_vars = continuous_vars + discrete_vars + nominal_vars
    corr_matrix = pd.DataFrame(index=all_vars, columns=all_vars)
    
    for var1 in all_vars:
        for var2 in all_vars:
            if var1 == var2:
                corr_matrix.loc[var1, var2] = 1.0
            elif var1 in continuous_vars and var2 in continuous_vars:
                corr_matrix.loc[var1, var2] = df[[var1, var2]].corr().iloc[0, 1]
            elif var1 in discrete_vars and var2 in discrete_vars:
                corr_matrix.loc[var1, var2] = spearmanr(df[var1], df[var2])[0]
            elif var1 in nominal_vars and var2 in nominal_vars:
                corr_matrix.loc[var1, var2] = cramers_v(df[var1], df[var2])
            else:
                # For mixed type pairs, use a general correlation approach or set to NaN
                corr_matrix.loc[var1, var2] = np.nan
    
    return corr_matrix.astype(float)

# Calculate correlation matrix
correlation_matrix = calculate_correlations(df)

# Plot the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Show the correlation matrix
print(correlation_matrix)









import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.exponential(scale=2, size=40000) for i in range(60)  # Using 60 variables for demonstration
})

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Apply log transformation
df_log_transformed = df.copy()
for col in num_cols:
    df_log_transformed[col] = np.log1p(df[col])  # log(1 + data) to handle zero values

# Function to plot box plots in chunks before and after log transformation
def plot_box_plots_before_after(df, df_log_transformed, num_cols, chunk_size=10):
    num_chunks = (len(num_cols) + chunk_size - 1) // chunk_size
    for chunk in range(num_chunks):
        chunk_cols = num_cols[chunk * chunk_size:(chunk + 1) * chunk_size]
        fig, axes = plt.subplots(len(chunk_cols), 2, figsize=(15, 5 * len(chunk_cols)))
        if len(chunk_cols) == 1:
            axes = [axes]
        for i, col in enumerate(chunk_cols):
            sns.boxplot(x=df[col], ax=axes[i, 0])
            axes[i, 0].set_title(f'Original Box plot of {col}')
            sns.boxplot(x=df_log_transformed[col], ax=axes[i, 1])
            axes[i, 1].set_title(f'Log-Transformed Box plot of {col}')
        plt.tight_layout()
        plt.show()

# Plot the box plots before and after log transformation in chunks
plot_box_plots_before_after(df, df_log_transformed, num_cols, chunk_size=10)



























import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)  # Using 60 variables for demonstration
})

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Function to plot box plots in chunks
def plot_box_plots_in_chunks(df, num_cols, chunk_size=10):
    num_chunks = (len(num_cols) + chunk_size - 1) // chunk_size
    for chunk in range(num_chunks):
        chunk_cols = num_cols[chunk * chunk_size:(chunk + 1) * chunk_size]
        fig, axes = plt.subplots(len(chunk_cols), 1, figsize=(15, 5 * len(chunk_cols)))
        if len(chunk_cols) == 1:
            axes = [axes]
        for ax, col in zip(axes, chunk_cols):
            sns.boxplot(x=df[col], ax=ax)
            ax.set_title(f'Box plot of {col}')
        plt.tight_layout()
        plt.show()

# Plot the box plots in chunks
plot_box_plots_in_chunks(df, num_cols, chunk_size=10)











import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, boxcox

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)  # Using 60 variables for demonstration
})

# Define transformations
def apply_transformations(data):
    transformations = {
        'Original': data,
        'Log': np.log1p(data),  # log(1 + data)
        'Square Root': np.sqrt(data),
        'Box-Cox': boxcox(data + 1)[0] if np.all(data + 1 > 0) else np.nan * data,  # Ensure positivity
    }
    return transformations

# Function to plot distributions and transformations for a single variable
def plot_and_test_transformations_single_var(df, col, sample_size=5000):
    data = df[col].dropna().sample(n=sample_size, random_state=0)
    
    # Apply transformations
    transformations = apply_transformations(data)
    
    # Plot original and transformed distributions
    plt.figure(figsize=(15, 10))
    for i, (name, transformed_data) in enumerate(transformations.items()):
        plt.subplot(2, 2, i+1)
        sns.histplot(transformed_data, kde=True)
        plt.title(f'{name} Transformation of {col}')
    
    plt.tight_layout()
    plt.show()
    
    # Perform normality tests
    print(f'Normality tests for {col}:')
    for name, transformed_data in transformations.items():
        transformed_data = transformed_data[~np.isnan(transformed_data)]  # Remove NaNs
        if len(transformed_data) >= 3:
            stat, p_value = shapiro(transformed_data)
            print(f'  {name} Transformation: W={stat:.4f}, p-value={p_value:.4f}')
        else:
            print(f'  {name} Transformation: Not enough data for normality test')
    print()

# Specify the variable to analyze
variable_to_analyze = 'Var0'  # Change this to the desired variable

# Plot and test transformations for the specified variable
plot_and_test_transformations_single_var(df, variable_to_analyze, sample_size=5000)












import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, boxcox, normaltest

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)  # Using 60 variables for demonstration
})

# Define transformations
def apply_transformations(data):
    transformations = {
        'Original': data,
        'Log': np.log1p(data),  # log(1 + data)
        'Square Root': np.sqrt(data),
        'Box-Cox': boxcox(data + 1)[0] if np.all(data + 1 > 0) else np.nan * data,  # Ensure positivity
    }
    return transformations

# Function to plot distributions and transformations in batches
def plot_and_test_transformations(df, batch_size=10, sample_size=5000):
    num_cols = df.select_dtypes(include=['float64', 'int64']).columns
    num_batches = (len(num_cols) + batch_size - 1) // batch_size
    
    for batch in range(num_batches):
        batch_cols = num_cols[batch * batch_size:(batch + 1) * batch_size]
        for col in batch_cols:
            data = df[col].dropna().sample(n=sample_size, random_state=0)
            
            # Apply transformations
            transformations = apply_transformations(data)
            
            # Plot original and transformed distributions
            plt.figure(figsize=(15, 10))
            for i, (name, transformed_data) in enumerate(transformations.items()):
                plt.subplot(2, 2, i+1)
                sns.histplot(transformed_data, kde=True)
                plt.title(f'{name} Transformation of {col}')
            
            plt.tight_layout()
            plt.show()
            
            # Perform normality tests
            print(f'Normality tests for {col}:')
            for name, transformed_data in transformations.items():
                transformed_data = transformed_data[~np.isnan(transformed_data)]  # Remove NaNs
                if len(transformed_data) >= 3:
                    stat, p_value = shapiro(transformed_data)
                    print(f'  {name} Transformation: W={stat:.4f}, p-value={p_value:.4f}')
                else:
                    print(f'  {name} Transformation: Not enough data for normality test')
            print()

# Plot and test transformations in batches
plot_and_test_transformations(df, batch_size=10, sample_size=5000)
















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, boxcox, normaltest

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)  # Using 60 variables for demonstration
})

# Define transformations
def apply_transformations(data):
    transformations = {
        'Original': data,
        'Log': np.log1p(data),  # log(1 + data)
        'Square Root': np.sqrt(data),
        'Box-Cox': boxcox(data + 1)[0] if np.all(data + 1 > 0) else np.nan * data,  # Ensure positivity
    }
    return transformations

# Function to plot distributions and transformations in batches
def plot_and_test_transformations(df, batch_size=10, sample_size=5000):
    num_cols = df.select_dtypes(include=['float64', 'int64']).columns
    num_batches = (len(num_cols) + batch_size - 1) // batch_size
    
    for batch in range(num_batches):
        batch_cols = num_cols[batch * batch_size:(batch + 1) * batch_size]
        for col in batch_cols:
            data = df[col].dropna().sample(n=sample_size, random_state=0)
            
            # Apply transformations
            transformations = apply_transformations(data)
            
            # Plot original and transformed distributions
            plt.figure(figsize=(15, 10))
            for i, (name, transformed_data) in enumerate(transformations.items()):
                plt.subplot(2, 2, i+1)
                sns.histplot(transformed_data, kde=True)
                plt.title(f'{name} Transformation of {col}')
            
            plt.tight_layout()
            plt.show()
            
            # Perform normality tests
            print(f'Normality tests for {col}:')
            for name, transformed_data in transformations.items():
                transformed_data = transformed_data[~np.isnan(transformed_data)]  # Remove NaNs
                stat, p_value = shapiro(transformed_data)
                print(f'  {name} Transformation: W={stat:.4f}, p-value={p_value:.4f}')
            print()

# Plot and test transformations in batches
plot_and_test_transformations(df, batch_size=10, sample_size=5000)
















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, boxcox, normaltest

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)  # Using 60 variables for demonstration
})

# Define transformations
def apply_transformations(data):
    transformations = {
        'Original': data,
        'Log': np.log1p(data),  # log(1 + data)
        'Square Root': np.sqrt(data),
        'Box-Cox': boxcox(data + 1)[0] if np.all(data + 1 > 0) else np.nan * data,  # Ensure positivity
    }
    return transformations

# Function to plot distributions and transformations
def plot_and_test_transformations(df, batch_size=10):
    num_cols = df.select_dtypes(include=['float64', 'int64']).columns
    num_batches = (len(num_cols) + batch_size - 1) // batch_size  # Calculate the number of batches

    for batch in range(num_batches):
        batch_cols = num_cols[batch * batch_size:(batch + 1) * batch_size]
        for col in batch_cols:
            data = df[col].dropna()
            
            # Apply transformations
            transformations = apply_transformations(data)
            
            # Plot original and transformed distributions
            plt.figure(figsize=(15, 10))
            for i, (name, transformed_data) in enumerate(transformations.items()):
                plt.subplot(2, 2, i+1)
                sns.histplot(transformed_data, kde=True)
                plt.title(f'{name} Transformation of {col}')
            
            plt.tight_layout()
            plt.show()
            
            # Perform normality tests
            print(f'Normality tests for {col}:')
            for name, transformed_data in transformations.items():
                transformed_data = transformed_data[~np.isnan(transformed_data)]  # Remove NaNs
                stat, p_value = shapiro(transformed_data)
                print(f'  {name} Transformation: W={stat:.4f}, p-value={p_value:.4f}')
            print()

# Plot and test transformations in batches
plot_and_test_transformations(df, batch_size=10)












import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, boxcox, normaltest

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(10)  # Using 10 variables for demonstration
})

# Define transformations
def apply_transformations(data):
    transformations = {
        'Original': data,
        'Log': np.log1p(data),  # log(1 + data)
        'Square Root': np.sqrt(data),
        'Box-Cox': boxcox(data + 1)[0] if np.all(data + 1 > 0) else np.nan * data,  # Ensure positivity
    }
    return transformations

# Function to plot distributions and transformations
def plot_and_test_transformations(df):
    num_cols = df.select_dtypes(include=['float64', 'int64']).columns
    
    for col in num_cols:
        data = df[col].dropna()
        
        # Apply transformations
        transformations = apply_transformations(data)
        
        # Plot original and transformed distributions
        plt.figure(figsize=(15, 10))
        for i, (name, transformed_data) in enumerate(transformations.items()):
            plt.subplot(2, 2, i+1)
            sns.histplot(transformed_data, kde=True)
            plt.title(f'{name} Transformation of {col}')
        
        plt.tight_layout()
        plt.show()
        
        # Perform normality tests
        print(f'Normality tests for {col}:')
        for name, transformed_data in transformations.items():
            transformed_data = transformed_data[~np.isnan(transformed_data)]  # Remove NaNs
            stat, p_value = shapiro(transformed_data)
            print(f'  {name} Transformation: W={stat:.4f}, p-value={p_value:.4f}')
        print()

# Plot and test transformations
plot_and_test_transformations(df)

























import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)  # Using 60 variables for demonstration
})

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Function to plot box plots in chunks
def plot_box_plots_in_chunks(df, num_cols, chunk_size=10):
    num_chunks = (len(num_cols) + chunk_size - 1) // chunk_size
    for chunk in range(num_chunks):
        chunk_cols = num_cols[chunk * chunk_size:(chunk + 1) * chunk_size]
        plt.figure(figsize=(15, 5 * len(chunk_cols)))
        for i, col in enumerate(chunk_cols, 1):
            plt.subplot(len(chunk_cols), 1, i)
            sns.boxplot(x=df[col])
            plt.title(f'Box plot of {col}')
        plt.tight_layout()
        plt.show()

# Plot the box plots in chunks
plot_box_plots_in_chunks(df, num_cols, chunk_size=10)


















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(10)  # Using 10 variables for demonstration
})

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Determine the number of rows and columns for the grid
num_vars = len(num_cols)
num_cols_per_row = 3  # Number of plots per row
num_rows = (num_vars + num_cols_per_row - 1) // num_cols_per_row  # Compute number of rows needed

# Plot the box plots for each numerical column
plt.figure(figsize=(num_cols_per_row * 5, num_rows * 5))  # Adjust the figure size

for i, col in enumerate(num_cols, 1):
    plt.subplot(num_rows, num_cols_per_row, i)
    sns.boxplot(x=df[col])
    plt.title(f'Box plot of {col}')
    plt.tight_layout()

plt.show()





















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(10)  # Using 10 variables for demonstration
})

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Plot the box plots for each numerical column
plt.figure(figsize=(15, 20))
for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols), 1, i)
    sns.boxplot(x=df[col])
    plt.title(f'Box plot of {col}')

plt.tight_layout()
plt.show()

















import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Function to calculate correlation matrix and filter by threshold
def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Create a sample DataFrame for demonstration
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)
})

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(20, 15))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5, linecolor='black')

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()









import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    f'Var{i}': np.random.normal(0, 1, 40000) for i in range(60)
})

# Function to plot distributions in batches
def plot_distributions(data, columns, batch_size=10, sample_size=5000, kde=True, bins=30):
    num_batches = (len(columns) + batch_size - 1) // batch_size  # Calculate the number of batches
    for batch in range(num_batches):
        batch_cols = columns[batch * batch_size:(batch + 1) * batch_size]
        plt.figure(figsize=(15, 5 * len(batch_cols)))
        for i, col in enumerate(batch_cols, 1):
            plt.subplot(len(batch_cols), 1, i)
            sns.histplot(data[col].sample(sample_size, random_state=0), kde=kde, bins=bins)
            plt.title(f'Distribution of {col}')
            plt.grid(True)  # Add grid to each plot
        plt.tight_layout()
        plt.show()

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Plot distributions in batches
plot_distributions(df, num_cols, batch_size=10, sample_size=5000, kde=True, bins=30)

# Function to calculate the correlation matrix with p-values
def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(20, 16))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()
















Bonjour [Nom du destinataire],

J'espre que vous allez bien.

Je vous cris pour vous informer que j'ai reu un email destin  quelqu'un d'autre. Il semble qu'il ait t envoy  mon adresse par erreur. Je vous prie de bien vouloir vrifier l'adresse email et de le renvoyer au bon destinataire.

Merci de votre comprhension.

Cordialement,










import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    'Normal': np.random.normal(0, 1, 1000),
    'Exponential': np.random.exponential(1, 1000),
    'Uniform': np.random.uniform(-3, 3, 1000),
    'Binomial': np.random.binomial(10, 0.5, 1000)
})

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Plot the distributions using sns.histplot
plt.figure(figsize=(15, 10))
for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols), 1, i)
    sns.histplot(df[col], kde=True)  # kde=True adds a kernel density estimate
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()










import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame with different types of distributions
np.random.seed(0)
df = pd.DataFrame({
    'Normal': np.random.normal(0, 1, 1000),
    'Exponential': np.random.exponential(1, 1000),
    'Uniform': np.random.uniform(-3, 3, 1000),
    'Binomial': np.random.binomial(10, 0.5, 1000)
})

# Select numerical columns
num_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Plot the distributions
plt.figure(figsize=(20, 15))
for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols), 1, i)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()










((y + 1)^{\lambda} - 1) / \lambda & \text{if } y \geq 0 \\
- ((-y + 1)^{2 - \lambda} - 1) / (2 - \lambda) & \text{if } y < 0
\end{array} \right. \]
Let's visualize how these transformations work on right-skewed data using Python.
```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Set the style
sns.set(style="whitegrid")
# Generate right-skewed data
np.random.seed(0)
right_skewed_data = np.random.exponential(scale=2, size=1000)
# Apply transformations
log_transformed = np.log1p(right_skewed_data)
sqrt_transformed = np.sqrt(right_skewed_data)
cbrt_transformed = np.cbrt(right_skewed_data)
boxcox_transformed, _ = stats.boxcox(right_skewed_data + 1) # adding 1 to ensure positivity
reciprocal_transformed = 1 / (right_skewed_data + 1) # adding 1 to avoid division by zero
yeojohnson_transformed, _ = stats.yeojohnson(right_skewed_data)
# Plotting
fig, axs = plt.subplots(3, 2, figsize=(15, 18))
fig.suptitle('Transformations to Correct Right-Skewed Data', fontsize=20)
# Original Right-Skewed Data
sns.histplot(right_skewed_data, kde=True, ax=axs[0, 0])
axs[0, 0].set_title('Original Right-Skewed Data')
# Log Transformation
sns.histplot(log_transformed, kde=True, ax=axs[0, 1])
axs[0, 1].set_title('Log Transformation')
# Square Root Transformation
sns.histplot(sqrt_transformed, kde=True, ax=axs[1, 0])
axs[1, 0].set_title('Square Root Transformation')
# Cube Root Transformation
sns.histplot(cbrt_transformed, kde=True, ax=axs[1, 1])
axs[1, 1].set_title('Cube Root Transformation')
# Box-Cox Transformation
sns.histplot(boxcox_transformed, kde=True, ax=axs[2, 0])
axs[2, 0].set_title('Box-Cox Transformation')
# Reciprocal Transformation
sns.histplot(reciprocal_transformed, kde=True, ax=axs[2, 1])
axs[2, 1].set_title('Reciprocal Transformation')
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()





















import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set the style
sns.set(style="whitegrid")

# Generate data
np.random.seed(0)
normal_data = np.random.normal(0, 1, 1000)
right_skewed_data = np.random.exponential(1, 1000)
left_skewed_data = -np.random.exponential(1, 1000)
bimodal_data = np.concatenate([np.random.normal(-3, 1, 500), np.random.normal(3, 1, 500)])
uniform_data = np.random.uniform(-3, 3, 1000)

# Transformations
log_transformed = np.log1p(right_skewed_data)
sqrt_transformed = np.sqrt(right_skewed_data)
square_transformed = np.square(left_skewed_data)
inverse_log_transformed = -np.log(-left_skewed_data)

# Plotting
fig, axs = plt.subplots(5, 3, figsize=(18, 20))
fig.suptitle('Distributions and Their Transformations', fontsize=20)

# Normal Distribution
sns.histplot(normal_data, kde=True, ax=axs[0, 0])
axs[0, 0].set_title('Normal Distribution')

# Right-Skewed Distribution
sns.histplot(right_skewed_data, kde=True, ax=axs[1, 0])
axs[1, 0].set_title('Right-Skewed Distribution')

# Left-Skewed Distribution
sns.histplot(left_skewed_data, kde=True, ax=axs[2, 0])
axs[2, 0].set_title('Left-Skewed Distribution')

# Bimodal Distribution
sns.histplot(bimodal_data, kde=True, ax=axs[3, 0])
axs[3, 0].set_title('Bimodal Distribution')

# Uniform Distribution
sns.histplot(uniform_data, kde=True, ax=axs[4, 0])
axs[4, 0].set_title('Uniform Distribution')

# Log Transformation
sns.histplot(log_transformed, kde=True, ax=axs[1, 1])
axs[1, 1].set_title('Log Transformation of Right-Skewed')

# Square Root Transformation
sns.histplot(sqrt_transformed, kde=True, ax=axs[1, 2])
axs[1, 2].set_title('Square Root Transformation of Right-Skewed')

# Square Transformation
sns.histplot(square_transformed, kde=True, ax=axs[2, 1])
axs[2, 1].set_title('Square Transformation of Left-Skewed')

# Inverse Log Transformation
sns.histplot(inverse_log_transformed, kde=True, ax=axs[2, 2])
axs[2, 2].set_title('Inverse Log Transformation of Left-Skewed')

# Bimodal and Uniform do not require transformation
axs[3, 1].axis('off')
axs[3, 2].axis('off')
axs[4, 1].axis('off')
axs[4, 2].axis('off')

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()




















import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
import shap
from sklearn.cross_decomposition import PLSRegression
from sklearn.manifold import Isomap
from sklearn.decomposition import PCA

# Sample data for testing
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.choice(['A', 'B', 'C'], 100),
    'feature3': np.random.choice(['X', 'Y', 'Z'], 100),
    'target': np.random.rand(100) * 100
})

# Prepare the data
X = df.drop('target', axis=1)
y = df['target']

# Identify categorical and numerical columns
categorical_cols = ['feature2', 'feature3']
numerical_cols = ['feature1']

# Preprocessing for numerical data
numerical_transformer = StandardScaler()

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# PCA
pca = PCA(n_components=0.95)  # Retain 95% of variance

# Create pipeline for PCA
pipeline_pca = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('pca', pca),
    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
])

# Fit the model with PCA
pipeline_pca.fit(X_train, y_train)

# Predict on the test set with PCA
y_test_pred_pca = pipeline_pca.predict(X_test)

# Calculate evaluation metrics for the test set with PCA
test_rmse_pca = np.sqrt(mean_squared_error(y_test, y_test_pred_pca))
test_r2_pca = r2_score(y_test, y_test_pred_pca)
test_r2_adj_pca = 1 - (1 - test_r2_pca) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

# Print the results for PCA
print(f"\nTest Set Metrics with PCA:\nRMSE: {test_rmse_pca}\nR: {test_r2_pca}\nAdjusted R: {test_r2_adj_pca}")

# PLS
pls = PLSRegression(n_components=2)  # Using 2 components for demonstration

# Create pipeline for PLS
pipeline_pls = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('pls', pls),
    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
])

# Fit the model with PLS
pipeline_pls.fit(X_train, y_train)

# Predict on the test set with PLS
y_test_pred_pls = pipeline_pls.predict(X_test)

# Calculate evaluation metrics for the test set with PLS
test_rmse_pls = np.sqrt(mean_squared_error(y_test, y_test_pred_pls))
test_r2_pls = r2_score(y_test, y_test_pred_pls)
test_r2_adj_pls = 1 - (1 - test_r2_pls) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

# Print the results for PLS
print(f"\nTest Set Metrics with PLS:\nRMSE: {test_rmse_pls}\nR: {test_r2_pls}\nAdjusted R: {test_r2_adj_pls}")

# Isomap
isomap = Isomap(n_components=2)  # Using 2 components for demonstration

# Create pipeline for Isomap
pipeline_isomap = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('isomap', isomap),
    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
])

# Fit the model with Isomap
pipeline_isomap.fit(X_train, y_train)

# Predict on the test set with Isomap
y_test_pred_isomap = pipeline_isomap.predict(X_test)

# Calculate evaluation metrics for the test set with Isomap
test_rmse_isomap = np.sqrt(mean_squared_error(y_test, y_test_pred_isomap))
test_r2_isomap = r2_score(y_test, y_test_pred_isomap)
test_r2_adj_isomap = 1 - (1 - test_r2_isomap) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

# Print the results for Isomap
print(f"\nTest Set Metrics with Isomap:\nRMSE: {test_rmse_isomap}\nR: {test_r2_isomap}\nAdjusted R: {test_r2_adj_isomap}")

















from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline

# Prepare the data
X = df.drop('target', axis=1)
y = df['target']

# Identify categorical and numerical columns
categorical_cols = ['feature2', 'feature3']
numerical_cols = ['feature1']

# Preprocessing for numerical data
numerical_transformer = StandardScaler()

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# PCA
pca = PCA(n_components=0.95)  # Retain 95% of variance

# Create pipeline
pipeline_pca = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('pca', pca),
    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model with PCA
pipeline_pca.fit(X_train, y_train)

# Predict on the test set
y_test_pred_pca = pipeline_pca.predict(X_test)

# Calculate evaluation metrics for the test set
test_rmse_pca = np.sqrt(mean_squared_error(y_test, y_test_pred_pca))
test_r2_pca = r2_score(y_test, y_test_pred_pca)
test_r2_adj_pca = 1 - (1 - test_r2_pca) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

# Print the results for PCA
print(f"\nTest Set Metrics with PCA:\nRMSE: {test_rmse_pca}\nR: {test_r2_pca}\nAdjusted R: {test_r2_adj_pca}")


from sklearn.cross_decomposition import PLSRegression

# PLS
pls = PLSRegression(n_components=2)  # Using 2 components for demonstration

# Create pipeline
pipeline_pls = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('pls', pls),
    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
])

# Fit the model with PLS
pipeline_pls.fit(X_train, y_train)

# Predict on the test set
y_test_pred_pls = pipeline_pls.predict(X_test)

# Calculate evaluation metrics for the test set
test_rmse_pls = np.sqrt(mean_squared_error(y_test, y_test_pred_pls))
test_r2_pls = r2_score(y_test, y_test_pred_pls)
test_r2_adj_pls = 1 - (1 - test_r2_pls) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

# Print the results for PLS
print(f"\nTest Set Metrics with PLS:\nRMSE: {test_rmse_pls}\nR: {test_r2_pls}\nAdjusted R: {test_r2_adj_pls}")


from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Note: LDA is typically used for classification. For the sake of demonstration, we'll use it as if it were a regression task.
lda = LinearDiscriminantAnalysis(n_components=1)

# Create pipeline
pipeline_lda = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('lda', lda),
    ('model', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
])

# Fit the model with LDA
pipeline_lda.fit(X_train, y_train)

# Predict on the test set
y_test_pred_lda = pipeline_lda.predict(X_test)

# Calculate evaluation metrics for the test set
test_rmse_lda = np.sqrt(mean_squared_error(y_test, y_test_pred_lda))
test_r2_lda = r2_score(y_test, y_test_pred_lda)
test_r2_adj_lda = 1 - (1 - test_r2_lda) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

# Print the results for LDA
print(f"\nTest Set Metrics with LDA:\nRMSE: {test_rmse_lda}\nR: {test_r2_lda}\nAdjusted R: {test_r2_adj_lda}")


















import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
import shap

# Sample data for testing
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.choice(['A', 'B', 'C'], 100),
    'feature3': np.random.choice(['X', 'Y', 'Z'], 100),
    'target': np.random.rand(100) * 100
})

# Prepare features and target
X = df.drop('target', axis=1)
y = df['target']

# Identify categorical and numerical columns
categorical_cols = ['feature2', 'feature3']
numerical_cols = ['feature1']

# Preprocessing for numerical data
numerical_transformer = StandardScaler()

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Define the model with regularization parameters
xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
    'alpha': [0.1, 1, 10],
    'lambda': [0.1, 1, 10]
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)

# Fit the model using grid search
grid_search.fit(X_train_preprocessed, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print(f"Best parameters found: {best_params}")

# Train the model using the best parameters
best_xg_reg = grid_search.best_estimator_

# Predict on the training set
y_train_pred = best_xg_reg.predict(X_train_preprocessed)

# Predict on the test set
y_test_pred = best_xg_reg.predict(X_test_preprocessed)

# Calculate evaluation metrics for the training set
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)
train_r2_adj = 1 - (1 - train_r2) * (len(y_train) - 1) / (len(y_train) - X_train_preprocessed.shape[1] - 1)

# Calculate evaluation metrics for the test set
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_r2_adj = 1 - (1 - test_r2) * (len(y_test) - 1) / (len(y_test) - X_test_preprocessed.shape[1] - 1)

# Print the results
print(f"Training Set Metrics:\nRMSE: {train_rmse}\nR: {train_r2}\nAdjusted R: {train_r2_adj}")
print(f"\nTest Set Metrics:\nRMSE: {test_rmse}\nR: {test_r2}\nAdjusted R: {test_r2_adj}")

# SHAP for model interpretation
explainer = shap.Explainer(best_xg_reg, X_train_preprocessed)
shap_values = explainer(X_test_preprocessed)

# Plot summary plot
shap.summary_plot(shap_values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())

# Plot dependence plot for a specific feature
shap.dependence_plot("num__feature1", shap_values.values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())













import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
import shap

# Sample data for testing
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.choice(['A', 'B', 'C'], 100),
    'feature3': np.random.choice(['X', 'Y', 'Z'], 100),
    'target': np.random.rand(100) * 100
})

# Prepare features and target
X = df.drop('target', axis=1)
y = df['target']

# Identify categorical and numerical columns
categorical_cols = ['feature2', 'feature3']
numerical_cols = ['feature1']

# Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False))
])

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Define the model with regularization parameters
xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, 
                          max_depth=5, learning_rate=0.1, n_estimators=1000, 
                          subsample=0.8, colsample_bytree=0.8, alpha=0.01, lambda=0.01)

# Perform cross-validation with early stopping
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_results = xgb.cv(dtrain=xgb.DMatrix(X_train_preprocessed, label=y_train), 
                    params=xg_reg.get_params(), 
                    nfold=kf.get_n_splits(), 
                    num_boost_round=1000, 
                    early_stopping_rounds=50, 
                    metrics="rmse", 
                    as_pandas=True, 
                    seed=42)

best_num_boost_rounds = cv_results.shape[0]
print(f"Best number of boosting rounds: {best_num_boost_rounds}")

# Train the model using the best number of boosting rounds
xg_reg.set_params(n_estimators=best_num_boost_rounds)
xg_reg.fit(X_train_preprocessed, y_train, eval_set=[(X_test_preprocessed, y_test)], early_stopping_rounds=50, verbose=False)

# Predict on the training set
y_train_pred = xg_reg.predict(X_train_preprocessed)

# Predict on the test set
y_test_pred = xg_reg.predict(X_test_preprocessed)

# Calculate evaluation metrics for the training set
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)
train_r2_adj = 1 - (1 - train_r2) * (len(y_train) - 1) / (len(y_train) - X_train_preprocessed.shape[1] - 1)

# Calculate evaluation metrics for the test set
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_r2_adj = 1 - (1 - test_r2) * (len(y_test) - 1) / (len(y_test) - X_test_preprocessed.shape[1] - 1)

# Print the results
print(f"Training Set Metrics:\nRMSE: {train_rmse}\nR: {train_r2}\nAdjusted R: {train_r2_adj}")
print(f"\nTest Set Metrics:\nRMSE: {test_rmse}\nR: {test_r2}\nAdjusted R: {test_r2_adj}")

# SHAP for model interpretation
explainer = shap.Explainer(xg_reg, X_train_preprocessed)
shap_values = explainer(X_test_preprocessed)

# Plot summary plot
shap.summary_plot(shap_values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())

# Plot dependence plot for a specific feature
shap.dependence_plot("num__feature1", shap_values.values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())











import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
import shap

# Sample data for testing
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.choice(['A', 'B', 'C'], 100),
    'feature3': np.random.choice(['X', 'Y', 'Z'], 100),
    'target': np.random.rand(100) * 100
})

# Prepare features and target
X = df.drop('target', axis=1)
y = df['target']

# Identify categorical and numerical columns
categorical_cols = ['feature2', 'feature3']
numerical_cols = ['feature1']

# Preprocessing for numerical data
numerical_transformer = 'passthrough'

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Split the training data into training and validation sets for early stopping
X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_train_preprocessed, y_train, test_size=0.2, random_state=42)

# Define the model
xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, 
                          max_depth=5, learning_rate=0.1, n_estimators=1000, 
                          subsample=0.8, colsample_bytree=0.8)

# Train the model with early stopping
xg_reg.fit(X_train_part, y_train_part,
           eval_set=[(X_valid, y_valid)],
           early_stopping_rounds=50,
           verbose=False)

# Predict on the training set
y_train_pred = xg_reg.predict(X_train_preprocessed)

# Predict on the test set
y_test_pred = xg_reg.predict(X_test_preprocessed)

# Calculate evaluation metrics for the training set
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)
train_r2_adj = 1 - (1 - train_r2) * (len(y_train) - 1) / (len(y_train) - X_train_preprocessed.shape[1] - 1)

# Calculate evaluation metrics for the test set
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_r2_adj = 1 - (1 - test_r2) * (len(y_test) - 1) / (len(y_test) - X_test_preprocessed.shape[1] - 1)

# Print the results
print(f"Training Set Metrics:\nRMSE: {train_rmse}\nR: {train_r2}\nAdjusted R: {train_r2_adj}")
print(f"\nTest Set Metrics:\nRMSE: {test_rmse}\nR: {test_r2}\nAdjusted R: {test_r2_adj}")

# SHAP for model interpretation
explainer = shap.Explainer(xg_reg, X_train_preprocessed)
shap_values = explainer(X_test_preprocessed)

# Plot summary plot
shap.summary_plot(shap_values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())

# Plot dependence plot for a specific feature
shap.dependence_plot("num__feature1", shap_values.values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())


















import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
import shap

# Sample data for testing
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.choice(['A', 'B', 'C'], 100),
    'feature3': np.random.choice(['X', 'Y', 'Z'], 100),
    'target': np.random.rand(100) * 100
})

# Prepare features and target
X = df.drop('target', axis=1)
y = df['target']

# Identify categorical and numerical columns
categorical_cols = ['feature2', 'feature3']
numerical_cols = ['feature1']

# Preprocessing for numerical data
numerical_transformer = 'passthrough'

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Define the model
xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, 
                          max_depth=5, learning_rate=0.1, n_estimators=1000, 
                          subsample=0.8, colsample_bytree=0.8, 
                          early_stopping_rounds=50)

# Perform cross-validation with early stopping
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_results = xgb.cv(dtrain=xgb.DMatrix(X_train_preprocessed, label=y_train), 
                    params=xg_reg.get_params(), 
                    nfold=kf.get_n_splits(), 
                    num_boost_round=1000, 
                    early_stopping_rounds=50, 
                    metrics="rmse", 
                    as_pandas=True, 
                    seed=42)

best_num_boost_rounds = cv_results.shape[0]
print(f"Best number of boosting rounds: {best_num_boost_rounds}")

# Train the model using the best number of boosting rounds
xg_reg.set_params(n_estimators=best_num_boost_rounds)
xg_reg.fit(X_train_preprocessed, y_train)

# Predict on the training set
y_train_pred = xg_reg.predict(X_train_preprocessed)

# Predict on the test set
y_test_pred = xg_reg.predict(X_test_preprocessed)

# Calculate evaluation metrics for the training set
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)
train_r2_adj = 1 - (1 - train_r2) * (len(y_train) - 1) / (len(y_train) - X_train_preprocessed.shape[1] - 1)

# Calculate evaluation metrics for the test set
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_r2_adj = 1 - (1 - test_r2) * (len(y_test) - 1) / (len(y_test) - X_test_preprocessed.shape[1] - 1)

# Print the results
print(f"Training Set Metrics:\nRMSE: {train_rmse}\nR: {train_r2}\nAdjusted R: {train_r2_adj}")
print(f"\nTest Set Metrics:\nRMSE: {test_rmse}\nR: {test_r2}\nAdjusted R: {test_r2_adj}")

# SHAP for model interpretation
explainer = shap.Explainer(xg_reg, X_train_preprocessed)
shap_values = explainer(X_test_preprocessed)

# Plot summary plot
shap.summary_plot(shap_values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())

# Plot dependence plot for a specific feature
shap.dependence_plot("num__feature1", shap_values.values, X_test_preprocessed, feature_names=preprocessor.get_feature_names_out())


















import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb

# Sample data for testing
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.choice(['A', 'B', 'C'], 100),
    'feature3': np.random.choice(['X', 'Y', 'Z'], 100),
    'target': np.random.rand(100) * 100
})

# Prepare features and target
X = df.drop('target', axis=1)
y = df['target']

# Identify categorical and numerical columns
categorical_cols = ['feature2', 'feature3']
numerical_cols = ['feature1']

# Preprocessing for numerical data
numerical_transformer = 'passthrough'

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Define the model
xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, missing=np.nan)

# Create a pipeline that includes preprocessing and the model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', xg_reg)])

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'model__max_depth': [3, 5, 7],
    'model__min_child_weight': [1, 3, 5],
    'model__gamma': [0, 0.1, 0.3],
    'model__subsample': [0.8, 0.9, 1.0],
    'model__colsample_bytree': [0.8, 0.9, 1.0],
    'model__n_estimators': [100, 200, 300],
    'model__learning_rate': [0.01, 0.1, 0.2]
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model using grid search
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print(f"Best parameters found: {best_params}")

# Predict on the training set
y_train_pred = grid_search.predict(X_train)

# Predict on the test set
y_test_pred = grid_search.predict(X_test)

# Calculate evaluation metrics for the training set
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)

# Calculate evaluation metrics for the test set
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)

# Print the results
print(f"Training Set Metrics:\nRMSE: {train_rmse}\nR: {train_r2}")
print(f"\nTest Set Metrics:\nRMSE: {test_rmse}\nR: {test_r2}")

# Plotting feature importance (optional)
import matplotlib.pyplot as plt
best_model = grid_search.best_estimator_.named_steps['model']
xgb.plot_importance(best_model)
plt.show()




















import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
import xgboost as xgb

# Sample data for testing
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.rand(100),
    'target': np.random.rand(100) * 100
})

# Prepare features and target
X = df[['feature1', 'feature2']]
y = df['target']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the XGBoost Regressor model
xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Define a custom scorer for the Gini coefficient
def gini(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    all_data = np.c_[y_true, y_pred, np.arange(len(y_true))]
    all_data = all_data[np.lexsort((all_data[:, 2], -1 * all_data[:, 1]))]
    total_losses = all_data[:, 0].sum()
    gini_sum = all_data[:, 0].cumsum().sum() / total_losses
    gini_sum -= (len(y_true) + 1) / 2.
    return gini_sum / len(y_true)

def gini_normalized(y_true, y_pred):
    return gini(y_true, y_pred) / gini(y_true, y_true)

gini_scorer = make_scorer(gini_normalized, greater_is_better=True)

# Define the evaluation metrics
metrics = {
    'rmse': 'neg_root_mean_squared_error',
    'r2': 'r2',
    'gini': gini_scorer
}

# Perform cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
results = {metric: cross_val_score(xg_reg, X, y, cv=kf, scoring=scoring).mean() for metric, scoring in metrics.items()}

# Train the model on the training set
xg_reg.fit(X_train, y_train)

# Predict on the test set
y_train_pred = xg_reg.predict(X_train)
y_test_pred = xg_reg.predict(X_test)

# Calculate evaluation metrics for the training set
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)
train_gini = gini_normalized(y_train, y_train_pred)

# Calculate evaluation metrics for the test set
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_gini = gini_normalized(y_test, y_test_pred)

# Print the results
print(f"Cross-Validation Results:\n{results}")
print(f"\nTraining Set Metrics:\nRMSE: {train_rmse}\nR: {train_r2}\nGini: {train_gini}")
print(f"\nTest Set Metrics:\nRMSE: {test_rmse}\nR: {test_r2}\nGini: {test_gini}")






















# Function to calculate different correlations
def calculate_correlations(df, target_variable):
    correlations = {}
    for method in ['pearson', 'spearman', 'kendall']:
        correlation_matrix = df.corr(method=method)
        target_correlation = correlation_matrix[[target_variable]].drop(target_variable)
        correlations[method] = target_correlation
    return correlations

# Calculate correlations
correlations = calculate_correlations(df, target_variable)

# Plot heatmaps
for method, target_corr in correlations.items():
    plt.figure(figsize=(5, len(target_corr) * 0.5 + 1))
    sns.heatmap(target_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, cbar=True, yticklabels=target_corr.index)
    plt.title(f'{method.capitalize()} Correlation with {target_variable}')
    plt.show()

















# Function to calculate different correlations
def calculate_correlations(df, target_variable):
    correlations = {}
    for method in ['pearson', 'spearman', 'kendall']:
        correlation_matrix = df.corr(method=method)
        correlations[method] = correlation_matrix
    return correlations

# Calculate correlations
correlations = calculate_correlations(df, target_variable)

# Plot heatmaps
for method, corr_matrix in correlations.items():
    plt.figure(figsize=(8, 6))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
    plt.title(f'{method.capitalize()} Correlation Matrix')
    plt.show()









import pandas as pd

# Sample data
data = {
    'A': [1, 2, 3, 4, 5],
    'B': [5, 4, 3, 2, 1],
    'C': [2, 3, 4, 5, 6],
    'Target': [1, 2, 1, 2, 1]
}

# Create DataFrame
df = pd.DataFrame(data)

# Specify the target variable
target_variable = 'Target'

# Function to calculate different correlations
def calculate_correlations(df, target_variable):
    correlations = {}
    for method in ['pearson', 'spearman', 'kendall']:
        correlation_matrix = df.corr(method=method)
        target_correlation = correlation_matrix[target_variable].drop(target_variable)
        correlations[method] = target_correlation
    return correlations

# Calculate correlations
correlations = calculate_correlations(df, target_variable)

# Display the results
for method, corr in correlations.items():
    print(f"\n{method.capitalize()} Correlation:\n{corr}")

















import pandas as pd

# Sample data
data = {
    'A': [1, 2, 3, 4, 5],
    'B': [5, 4, 3, 2, 1],
    'C': [2, 3, 4, 5, 6],
    'Target': [1, 2, 1, 2, 1]
}

# Create DataFrame
df = pd.DataFrame(data)

# Specify the target variable
target_variable = 'Target'

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Extract the correlations with the target variable
target_correlation = correlation_matrix[target_variable].drop(target_variable)

print(target_correlation)

















# Drop rows from df_main where the 'id' is in df_drop
df_filtered = df_main[~df_main['id'].isin(df_drop['id'])]

print("\nFiltered df_main:")
print(df_filtered)




import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import r2_score

# Generate some sample data
np.random.seed(0)
n_samples = 100
n_features = 5

X = np.random.rand(n_samples, n_features)
y = 4 + 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(n_samples)

# Convert to DataFrame for convenience
df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
df['target'] = y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled data back to DataFrame for feature selection
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

# Define models to try
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "Lasso Regression": Lasso(),
    "ElasticNet Regression": ElasticNet(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Polynomial Regression": Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())]),
    "Polynomial Ridge Regression": Pipeline([('poly', PolynomialFeatures(degree=2)), ('ridge', Ridge())])
}

best_model = None
best_r2 = -np.inf
best_selected_features = None
best_model_name = ""

# Evaluate each model
for model_name, model in models.items():
    # Use Sequential Feature Selector (SFS) to select the best features
    sfs = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward', scoring='r2', cv=5)
    sfs.fit(X_train_scaled_df, y_train)
    
    # Get the selected features
    selected_features = X_train_scaled_df.columns[sfs.get_support()]
    
    # Refit the model with the selected features
    X_train_selected = X_train_scaled_df[selected_features]
    X_test_selected = X_test_scaled_df[selected_features]
    
    model.fit(X_train_selected, y_train)
    y_pred = model.predict(X_test_selected)
    
    # Calculate the performance of the model
    r2 = r2_score(y_test, y_pred)
    print(f"{model_name} R-squared: {r2}")
    
    # Update best model if this model is better
    if r2 > best_r2:
        best_r2 = r2
        best_model = model
        best_selected_features = selected_features
        best_model_name = model_name

print(f"\nBest model: {best_model_name}")
print(f"Best R-squared: {best_r2}")
print(f"Selected features: {best_selected_features.tolist()}")











import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import r2_score

# Generate some sample data
np.random.seed(0)
n_samples = 100
n_features = 5

X = np.random.rand(n_samples, n_features)
y = 4 + 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(n_samples)

# Convert to DataFrame for convenience
df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
df['target'] = y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=42)

# Define models to try
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "Lasso Regression": Lasso(),
    "ElasticNet Regression": ElasticNet(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Polynomial Regression": Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())]),
    "Polynomial Ridge Regression": Pipeline([('poly', PolynomialFeatures(degree=2)), ('ridge', Ridge())])
}

best_model = None
best_r2 = -np.inf
best_selected_features = None
best_model_name = ""

# Evaluate each model
for model_name, model in models.items():
    # Use Sequential Feature Selector (SFS) to select the best features
    sfs = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward', scoring='r2', cv=5)
    sfs.fit(X_train, y_train)
    
    # Get the selected features
    selected_features = X_train.columns[sfs.get_support()]
    
    # Refit the model with the selected features
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]
    
    model.fit(X_train_selected, y_train)
    y_pred = model.predict(X_test_selected)
    
    # Calculate the performance of the model
    r2 = r2_score(y_test, y_pred)
    print(f"{model_name} R-squared: {r2}")
    
    # Update best model if this model is better
    if r2 > best_r2:
        best_r2 = r2
        best_model = model
        best_selected_features = selected_features
        best_model_name = model_name

print(f"\nBest model: {best_model_name}")
print(f"Best R-squared: {best_r2}")
print(f"Selected features: {best_selected_features.tolist()}")











import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import r2_score

# Generate some sample data
np.random.seed(0)
n_samples = 100
n_features = 5

X = np.random.rand(n_samples, n_features)
y = 4 + 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(n_samples)

# Convert to DataFrame for convenience
df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
df['target'] = y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=42)

# Define models to try
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Support Vector Regressor": SVR()
}

best_model = None
best_r2 = -np.inf
best_selected_features = None
best_model_name = ""

# Evaluate each model
for model_name, model in models.items():
    # Use Sequential Feature Selector (SFS) to select the best features
    sfs = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward', scoring='r2', cv=5)
    sfs.fit(X_train, y_train)
    
    # Get the selected features
    selected_features = X_train.columns[sfs.get_support()]
    
    # Refit the model with the selected features
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]
    
    model.fit(X_train_selected, y_train)
    y_pred = model.predict(X_test_selected)
    
    # Calculate the performance of the model
    r2 = r2_score(y_test, y_pred)
    print(f"{model_name} R-squared: {r2}")
    
    # Update best model if this model is better
    if r2 > best_r2:
        best_r2 = r2
        best_model = model
        best_selected_features = selected_features
        best_model_name = model_name

print(f"\nBest model: {best_model_name}")
print(f"Best R-squared: {best_r2}")
print(f"Selected features: {best_selected_features.tolist()}")










import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import r2_score

# Generate some sample data
np.random.seed(0)
n_samples = 100
n_features = 5

X = np.random.rand(n_samples, n_features)
y = 4 + 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(n_samples)

# Convert to DataFrame for convenience
df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
df['target'] = y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Use Sequential Feature Selector (SFS) to select the best features
sfs = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward', scoring='r2', cv=5)
sfs.fit(X_train, y_train)

# Get the selected features
selected_features = X_train.columns[sfs.get_support()]

print("Selected features:")
print(selected_features)

# Refit the model with the selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

model.fit(X_train_selected, y_train)
y_pred = model.predict(X_test_selected)

# Calculate the performance of the model
r2 = r2_score(y_test, y_pred)
print(f"R-squared: {r2}")

# Output selected features
print(f"Selected features: {selected_features.tolist()}")










import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn.metrics import r2_score

# Generate some sample data
np.random.seed(0)
n_samples = 100
n_features = 5

X = np.random.rand(n_samples, n_features)
y = 4 + 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(n_samples)

# Convert to DataFrame for convenience
df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
df['target'] = y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Use Recursive Feature Elimination (RFE) to select the best features
rfe = RFE(model, n_features_to_select=1)
rfe.fit(X_train, y_train)

# Get the ranking of features
ranking = rfe.ranking_
features_ranking = sorted(zip(ranking, df.drop(columns='target').columns))

print("Feature ranking:")
for rank, feature in features_ranking:
    print(f"{feature}: {rank}")

# Select the best features based on RFE ranking
selected_features = [feature for rank, feature in features_ranking if rank == 1]

# Refit the model with the best features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

model.fit(X_train_selected, y_train)
y_pred = model.predict(X_test_selected)

# Calculate the performance of the model
r2 = r2_score(y_test, y_pred)
print(f"R-squared: {r2}")

# Output selected features
print(f"Selected features: {selected_features}")








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Generate some sample data
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate metrics
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Print the summary
print(f"Intercept: {model.intercept_[0]}")
print(f"Coefficient: {model.coef_[0][0]}")
print(f"R-squared: {r2}")
print(f"Mean Squared Error: {mse}")

# Plot the results
plt.figure(figsize=(14, 7))

# Plot the regression line
plt.subplot(1, 2, 1)
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')
plt.title('Linear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()

# Plot residuals
plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, color='blue')
plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), color='red', linewidth=2)
plt.title('Residuals Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')

plt.tight_layout()
plt.show()




import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the logistic regression model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Predict probabilities for the test set
y_pred_prob = model.predict_proba(X_test)[:, 1]

# Calculate ROC curve and ROC area
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot the coefficients
plt.figure(figsize=(12, 6))
coefficients = pd.Series(model.coef_[0], index=X.columns)
coefficients.sort_values().plot(kind='bar')
plt.title('Logistic Regression Coefficients')
plt.show()

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()




For a classification machine learning problem, you'll need functions to handle data preprocessing, model training, evaluation, and possibly deployment. Below is a comprehensive list of functions that cover these aspects using popular libraries like `pandas`, `scikit-learn`, `TensorFlow`, and `PyTorch`.

### Data Handling and Preprocessing
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load Data
def load_data(file_path):
    return pd.read_csv(file_path)

# Split Data
def split_data(X, y, test_size=0.2, random_state=42):
    return train_test_split(X, y, test_size=test_size, random_state=random_state)

# Scale Features
def scale_features(X_train, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled, scaler

# Encode Labels
def encode_labels(y):
    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(y)
    return y_encoded, encoder
```

### Model Training and Evaluation with Scikit-Learn
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Train Model
def train_model(X_train, y_train):
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    return model

# Evaluate Model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    matrix = confusion_matrix(y_test, y_pred)
    return accuracy, report, matrix

# Display Evaluation
def display_evaluation(accuracy, report, matrix):
    print(f"Accuracy: {accuracy}")
    print("Classification Report:\n", report)
    print("Confusion Matrix:\n", matrix)
```

### Model Training and Evaluation with TensorFlow/Keras
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# Build Keras Model
def build_keras_model(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_shape,)),
        Dense(64, activation='relu'),
        Dense(10, activation='softmax')  # Adjust output layer for your classification problem
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Train Keras Model
def train_keras_model(model, X_train, y_train, epochs=10, batch_size=32):
    y_train_categorical = to_categorical(y_train)
    history = model.fit(X_train, y_train_categorical, epochs=epochs, batch_size=batch_size, validation_split=0.2)
    return history

# Evaluate Keras Model
def evaluate_keras_model(model, X_test, y_test):
    y_test_categorical = to_categorical(y_test)
    loss, accuracy = model.evaluate(X_test, y_test_categorical)
    return loss, accuracy
```

### Model Training and Evaluation with PyTorch
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# PyTorch Model Definition
class SimpleNN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Train PyTorch Model
def train_pytorch_model(model, train_loader, criterion, optimizer, num_epochs=10):
    for epoch in range(num_epochs):
        for data, labels in train_loader:
            outputs = model(data)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    return model

# Evaluate PyTorch Model
def evaluate_pytorch_model(model, test_loader, criterion):
    model.eval()
    with torch.no_grad():
        total_loss = 0
        correct = 0
        total = 0
        for data, labels in test_loader:
            outputs = model(data)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        accuracy = 100 * correct / total
    return total_loss / len(test_loader), accuracy
```

### Full Workflow Example
Heres a full example that uses the functions defined above to load data, preprocess it, train a model, and evaluate it using scikit-learn.

```python
def main(file_path):
    # Load and preprocess data
    data = load_data(file_path)
    X = data.drop('target', axis=1)  # Adjust 'target' to your label column
    y = data['target']
    
    X_train, X_test, y_train, y_test = split_data(X, y)
    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)
    y_train_encoded, encoder = encode_labels(y_train)
    y_test_encoded, _ = encode_labels(y_test)
    
    # Train and evaluate model
    model = train_model(X_train_scaled, y_train_encoded)
    accuracy, report, matrix = evaluate_model(model, X_test_scaled, y_test_encoded)
    
    # Display evaluation results
    display_evaluation(accuracy, report, matrix)

if __name__ == "__main__":
    file_path = 'your_data.csv'  # Path to your dataset
    main(file_path)
```

### Additional Utility Functions

#### Hyperparameter Tuning (Using GridSearchCV)
```python
from sklearn.model_selection import GridSearchCV

def hyperparameter_tuning(X_train, y_train):
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10]
    }
    model = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)
    return grid_search.best_estimator_, grid_search.best_params_
```

#### Save and Load Model
```python
import joblib

# Save Model
def save_model(model, filename):
    joblib.dump(model, filename)

# Load Model
def load_model(filename):
    return joblib.load(filename)
```

These functions should provide a comprehensive set of tools to handle any situation related to classification and machine learning problems. Adjust the functions as needed to fit the specific requirements of your task.










import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils import resample
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
from scipy import stats
import statsmodels.api as sm

# Load Data
def load_data(file_path):
    data = pd.read_csv(file_path)
    print(f"Data Loaded: {data.shape[0]} rows, {data.shape[1]} columns")
    return data

# Initial Data Exploration
def initial_exploration(data):
    print("Data Types:\n", data.dtypes)
    print("\nMissing Values:\n", data.isnull().sum())
    print("\nDescriptive Statistics:\n", data.describe())
    print("\nClass Distribution:\n", data['target'].value_counts())

### 2. Data Cleaning and Preprocessing
def preprocess_data(data):
    # Handle missing values
    data.fillna(data.median(), inplace=True)
    
    # Encode categorical variables
    for column in data.select_dtypes(include=['object']).columns:
        data[column] = LabelEncoder().fit_transform(data[column])
    
    return data

### 3. Exploratory Data Analysis (EDA)
def eda(data):
    # Univariate Analysis
    for column in data.columns:
        plt.figure(figsize=(10, 5))
        sns.histplot(data[column], kde=True)
        plt.title(f'Univariate Analysis of {column}')
        plt.show()
    
    # Multivariate Analysis
    sns.pairplot(data)
    plt.title('Multivariate Analysis')
    plt.show()

### 4. Handle Imbalanced Data
def handle_imbalanced_data(X, y):
    smote = SMOTE()
    X_res, y_res = smote.fit_resample(X, y)
    return X_res, y_res

### 5. Feature Engineering
def feature_engineering(data):
    # Example: Creating interaction terms
    data['interaction'] = data['feature1'] * data['feature2']
    return data

### 6. Model Building and Evaluation
def build_and_evaluate_models(X, y):
    classifiers = [
        ('Logistic Regression', LogisticRegression()),
        ('Decision Tree', DecisionTreeClassifier()),
        ('Random Forest', RandomForestClassifier()),
        ('SVC', SVC()),
        ('Naive Bayes', GaussianNB()),
        ('KNN', KNeighborsClassifier()),
        ('Gradient Boosting', GradientBoostingClassifier())
    ]
    
    for name, clf in classifiers:
        pipeline = ImbPipeline([
            ('scaler', StandardScaler()),
            ('smote', SMOTE()),
            ('classifier', clf)
        ])
        scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')
        print(f'{name} Accuracy: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})')

### 7. Model Interpretation
def model_interpretation(model, X):
    feature_importances = pd.DataFrame(model.feature_importances_,
                                       index = X.columns,
                                       columns=['importance']).sort_values('importance', ascending=False)
    print("Feature Importances:\n", feature_importances)

### 8. Anomaly Detection
def perform_stat_tests(X, y):
    print("Performing statistical tests:")
    model = sm.OLS(y, sm.add_constant(X)).fit()
    influence = model.get_influence()
    cooks = influence.cooks_distance[0]
    plt.stem(np.arange(len(cooks)), cooks, markerfmt=",")
    plt.title("Cook's Distance")
    plt.show()
    
    dw = sm.stats.stattools.durbin_watson(model.resid)
    print(f'Durbin-Watson statistic: {dw}')
    
    z_scores = stats.zscore(X)
    abs_z_scores = np.abs(z_scores)
    anomaly_indices = np.where(abs_z_scores > 3)[0]
    print(f'Anomalies detected at indices: {anomaly_indices}')

### Full Workflow
def main(file_path):
    # Step 1: Load and Explore Data
    data = load_data(file_path)
    initial_exploration(data)
    
    # Step 2: Preprocess Data
    data = preprocess_data(data)
    
    # Step 3: Exploratory Data Analysis
    eda(data)
    
    # Step 4: Handle Imbalanced Data
    X = data.drop('target', axis=1)  # Adjust 'target' to your label column
    y = data['target']
    X, y = handle_imbalanced_data(X, y)
    
    # Step 5: Feature Engineering
    X = feature_engineering(X)
    
    # Step 6: Model Building and Evaluation
    build_and_evaluate_models(X, y)
    
    # Step 7: Model Interpretation (Example with RandomForest)
    final_model = RandomForestClassifier(random_state=42)
    final_model.fit(X, y)
    model_interpretation(final_model, X)
    
    # Step 8: Anomaly Detection
    perform_stat_tests(X, y)

if __name__ == "__main__":
    file_path = 'your_data.csv'  # Path to your dataset
    main(file_path)



pip install pandas numpy openpyxl requests beautifulsoup4 flask django fastapi jinja2 sqlalchemy scikit-learn tensorflow keras torch matplotlib seaborn scipy nltk spacy textblob selenium pyautogui click argparse plotly bokeh twisted pymongo pytest tqdm pyyaml configparser












import os

packages = [
    # Data Handling and Analysis
    "pandas",
    "numpy",
    "openpyxl",
    "requests",
    "beautifulsoup4",
    
    # Web Development
    "flask",
    "django",
    "fastapi",
    "jinja2",
    "sqlalchemy",
    
    # Machine Learning and Data Science
    "scikit-learn",
    "tensorflow",
    "keras",
    "torch",  # PyTorch
    "matplotlib",
    "seaborn",
    "scipy",
    
    # Natural Language Processing
    "nltk",
    "spacy",
    "textblob",
    
    # Automation
    "selenium",
    "pyautogui",
    
    # Utilities
    "logging",
    "os",
    "datetime",
    "click",
    "argparse",
    
    # Visualization
    "plotly",
    "bokeh",
    
    # Networking
    "socket",
    "twisted",
    
    # Database
    "sqlite3",
    "pymongo",
    
    # Testing
    "unittest",
    "pytest",
    
    # Deployment
    "docker",
    "heroku",
    
    # Others
    "tqdm",
    "pyyaml",
    "configparser"
]

for package in packages:
    os.system(f"pip install {package}")
python install_packages.py












Contexte :
Je rencontre un problme avec l'installation d'un package via pip sur Domino. Lorsque j'excute la commande pip install pour un package spcifique, l'installation semble se drouler correctement et le package est tlcharg avec succs. Cependant, lorsque j'essaie d'importer ce package dans mon code, je rencontre un problme.
J'ai essay de redmarrer le kernel aprs l'installation, mais cela n'a pas rsolu le problme.














# Convert the 'value' column to numeric, setting errors='coerce' to handle non-numeric values
df['value'] = pd.to_numeric(df['value'], errors='coerce')

# Fill NaN values with a specific value, e.g., 0
df['value'] = df['value'].fillna(0).astype(int)

print("\nDataFrame after converting 'value' to int:")
print(df)











import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'id': [2, 4, 5, 6, 7],
    'value2': ['v', 'w', 'x', 'y', 'z']
})

# Inner Join
inner_merged_df = pd.merge(df1, df2, on='id', how='inner')
print("Inner Merge:")
print(inner_merged_df)

# Left Join
left_merged_df = pd.merge(df1, df2, on='id', how='left')
print("\nLeft Merge:")
print(left_merged_df)

# Right Join
right_merged_df = pd.merge(df1, df2, on='id', how='right')
print("\nRight Merge:")
print(right_merged_df)

# Outer Join
outer_merged_df = pd.merge(df1, df2, on='id', how='outer')
print("\nOuter Merge:")
print(outer_merged_df)











import pandas as pd

# Sample data for testing
og = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

df1 = pd.DataFrame({
    'id': [1, 2, 6],
    'pnb': [100, 200, 300]
})

df2 = pd.DataFrame({
    'id': [2, 3, 7],
    'pnb': [150, 250, 350]
})

df3 = pd.DataFrame({
    'id': [3, 4, 8],
    'pnb': [200, 300, 400]
})

df4 = pd.DataFrame({
    'id': [4, 5, 9],
    'pnb': [250, 350, 450]
})

df5 = pd.DataFrame({
    'id': [5, 1, 10],
    'pnb': [300, 400, 500]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Initialize a DataFrame to store the results
result_df = og[['id']].copy()
result_df['total_pnb'] = 0

# Function to filter dataframes and sum the pnb values for each id in the original df
def calculate_pnb_for_each_id(og_df, dataframes, key='id', target_col='pnb'):
    # Get the unique ids from the og dataframe
    og_ids = set(og_df[key].unique())
    
    # Initialize a dictionary to store the total pnb for each id
    total_pnb_dict = {id: 0 for id in og_ids}
    
    # Iterate through each dataframe
    for df in dataframes:
        # Filter the dataframe to only include rows where the id is in the og dataframe
        filtered_df = df[df[key].isin(og_ids)]
        
        # Sum the pnb column for the filtered dataframe by id and add it to the total_pnb_dict
        for id, pnb_sum in filtered_df.groupby(key)[target_col].sum().items():
            total_pnb_dict[id] += pnb_sum
    
    return total_pnb_dict

# Calculate the pnb sums for each id
total_pnb_dict = calculate_pnb_for_each_id(og, dataframes)

# Update the result_df with the calculated pnb sums
result_df['total_pnb'] = result_df['id'].map(total_pnb_dict)

print(result_df)
















import pandas as pd

# Sample data for testing
og = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

df1 = pd.DataFrame({
    'id': [1, 2, 6],
    'pnb': [100, 200, 300]
})

df2 = pd.DataFrame({
    'id': [2, 3, 7],
    'pnb': [150, 250, 350]
})

df3 = pd.DataFrame({
    'id': [3, 4, 8],
    'pnb': [200, 300, 400]
})

df4 = pd.DataFrame({
    'id': [4, 5, 9],
    'pnb': [250, 350, 450]
})

df5 = pd.DataFrame({
    'id': [5, 1, 10],
    'pnb': [300, 400, 500]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Function to filter dataframes and sum the pnb values
def sum_pnb_for_og_ids(og_df, dataframes, key='id', target_col='pnb'):
    # Get the unique ids from the og dataframe
    og_ids = set(og_df[key].unique())
    
    # Initialize the total sum
    total_sum_pnb = 0
    
    # Iterate through each dataframe
    for df in dataframes:
        # Filter the dataframe to only include rows where the id is in the og dataframe
        filtered_df = df[df[key].isin(og_ids)]
        
        # Sum the pnb column for the filtered dataframe
        sum_pnb = filtered_df[target_col].sum()
        
        # Add the sum to the total sum
        total_sum_pnb += sum_pnb
    
    return total_sum_pnb

# Calculate the target variable
target_variable = sum_pnb_for_og_ids(og, dataframes)

print("Target Variable (Total pnb):", target_variable)










Step 2: Robust Feature Selection
Feature Selection Method:
Instead of traditional feature importance metrics, this approach uses error reduction on cross-validated datasets.
Error Reduction Illustration:
The error reduction is observed by removing certain features and evaluating the resulting error on both train and hold-out sets.
This is done for different age groups and contract lengths.
Error Reduction Metrics:
For example, removing a certain age range might reduce the error on the training set by 0.05 and on the hold-out set by 0.02 when using XGBoost.
The feature importance is estimated based on how much error reduction a feature provides.
Comparison Across Models:
Features are compared across models like GBM (Gradient Boosting Machines) and XGBoost to evaluate their importance.
Step 3: Gradient Boosting with Only Tree Stump & Transform to Linear Output
Tree Stump Gradient Boosting:
The model uses decision trees of depth 1 (tree stumps) to perform gradient boosting.
Each tree considers only one feature, one threshold, and one cut at each iteration.
Predictive Relationship:
The goal is to find predictive relationships between features and the target variable.
Example:
Customers are split based on age and revenue, which helps in categorizing risk levels.
Linearization:
The summed trees are then transformed into a linear function format similar to logistic regression.
This is achieved by summing the impacts of each decision tree split to create a corresponding coefficient table.
Step 4: Identify and Add the Most Important Interaction Pairs
Refinement with Bivariate Effects:
The model identifies important interaction pairs to refine predictive power.
Temporary Level 2 Trees:
These trees are used to identify the most important interactions.
Interaction Pairs:
Interaction pairs are determined based on cross-validated errors.
Implementation:
These interaction pairs are then added sequentially to the model using XGBoost, starting from the most important pairs until no further improvement is seen.
Example Interaction Pairs:
Var2 & Var6 (97.83% importance), Var13 & Var14 (0.87% importance), etc.
Summary
Feature Selection: Important features are identified based on their contribution to error reduction.
Model Building: The model uses tree stumps for gradient boosting, transforming the results into a linear output.
Interaction Refinement: Important interaction pairs are added to enhance the models predictive power.
This workflow combines robust feature selection with simplified tree-based gradient boosting and interaction pair refinement to create an accurate predictive model.














Rapport sur la Perte de Clients Professionnels (CCLIKPI)
Introduction:
Ce rapport prsente les statistiques dtailles sur le nombre de clients professionnels perdus par anne, en se concentrant sur deux axes principaux. Le premier axe examine la perte totale de clients professionnels de 2018  2023. Le deuxime axe se concentre sur une tude de cas spcifique, suivant la perte de clients ayant une anciennet de 2 mois  partir de 2018.

1. Axe 1 : Perte Totale de Clients Professionnels de 2018  2023

Perte Globale de Clients
Cette section fournit une vue d'ensemble de la perte totale de clients professionnels au fil des ans. Les chiffres montrent l'volution annuelle des pertes, ce qui permet d'identifier les tendances et les annes avec des fluctuations significatives.

Anne	Nombre total de clients perdus	Pourcentage de perte
2019	43082	10.91%
2020	37769	9.42%
2021	34718	8.52%
2022	38501	9.08%
2023	45615	10.36%
Observations :

En 2023, le nombre total de clients perdus a augment, atteignant 45615, avec un taux de perte de 10.36%.
Les pertes annuelles de clients montrent une lgre fluctuation mais une tendance globale stable avec une lgre hausse en 2023.
Perte de Clients pour les Entreprises (CNOUVSEG=ER)
Cette sous-section se concentre spcifiquement sur les pertes de clients parmi les entreprises (CNOUVSEG=ER). Les donnes permettent de comprendre comment ce segment particulier a volu par rapport aux autres.

Anne	Nombre de clients perdus	Pourcentage de perte
2019	8110	12.27%
2020	5831	8.87%
2021	5196	7.80%
2022	6216	9.04%
2023	7799	11.17%
Observations :

Pour les entreprises, les pertes ont diminu en 2020 et 2021, puis augment en 2022 et 2023, atteignant 11.17% en 2023.
La perte annuelle la plus faible tait en 2021 avec 7.80%.
Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL)
Cette section examine la perte de clients pour les autres types de clients professionnels (CNOUVSEG=AC AG PL), offrant une vue comparative avec le segment des entreprises.

Anne	Nombre de clients perdus	Pourcentage de perte
2019	36982	11.18%
2020	33854	10.05%
2021	31239	9.13%
2022	34118	8.59%
2023	40085	10.94%
Observations :

La perte de clients parmi les autres clients professionnels montre une tendance similaire  celle des entreprises, avec une baisse en 2020 et 2021 et une remonte en 2022 et 2023.
En 2023, la perte a augment  10.94%, indiquant une tendance inquitante.
2. Axe 2 : Perte de Clients Professionnels dans le Cadre de l'tude de Cas

Perte de Clients Globale dans le Cadre de l'tude de Cas
Anne	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	40109	10.78%
2021	29384	3413	40109	8.51%
2022	26275	3109	40109	7.75%
2023	20360	3189	40109	7.95%
Observations :

En 2023, il restait 20360 clients sur les 40109 initiaux, ce qui reprsente une perte de 7.95% des clients initiaux.
Les pertes annuelles ont montr une augmentation progressive, soulignant la ncessit de stratgies de rtention plus efficaces.
Perte de Clients pour les Entreprises (CNOUVSEG=ER) dans le Cadre de l'tude de Cas
Cette section suit un groupe spcifique de clients entreprises ayant une anciennet de 2 mois  partir de 2018, observant leur fidlit au fil des ans.

Anne	Clients restants ER	Clients perdus ER	Total de clients initiaux
2019	4372	440	4812
2020	3970	402	4812
2021	3679	291	4812
2022	3365	314	4812
2023	3049	316	4812
Observations :

En 2023, il restait 3049 clients entreprises sur les 4812 initiaux, ce qui reprsente une perte de 316 clients en 2023.
Les pertes annuelles ont montr une augmentation progressive mais faible, soulignant la ncessit de stratgies de rtention plus efficaces.
Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL) dans le Cadre de l'tude de Cas
Cette section suit un groupe spcifique d'autres clients professionnels ayant une anciennet de 2 mois  partir de 2018, observant leur fidlit au fil des ans.

Anne	Clients restants AC AG PL	Clients perdus AC AG PL	Total de clients initiaux
2019	32749	2550	35299
2020	28862	3887	35299
2021	25792	3070	35299
2022	23083	2709	35299
2023	20293	2790	35299
Observations :

En 2023, il restait 20293 autres clients professionnels sur les 35299 initiaux, ce qui reprsente une perte de 2790 clients en 2023.
Les pertes annuelles ont montr une tendance fluctuante mais globalement leve, soulignant la ncessit de stratgies de rtention spcifiques pour ce segment.
Conclusion :
Les donnes montrent une tendance inquitante de perte de clients dans les deux segments de clients professionnels. Les entreprises et les autres clients professionnels voient tous deux une augmentation progressive des pertes de clients au fil des annes, avec des pourcentages de perte annuels relativement levs. En particulier, l'tude de cas a rvl une perte notable des clients initiaux d'ici 2023. Cette analyse souligne la ncessit de stratgies de rtention client plus efficaces pour inverser cette tendance.

Pour des analyses plus dtailles et interactives, veuillez consulter le tableau de bord Power BI. Vous y trouverez des graphiques intressants tels que la perte de clients par mois et par anne, le pourcentage de clients perdus selon le segment CNOUVSEG, ainsi que des sections dtailles pour chaque catgorie de clients.















Pour des analyses plus dtailles et interactives, veuillez consulter le tableau de bord Power BI. Vous y trouverez des graphiques intressants tels que la perte de clients par mois et par anne, le pourcentage de clients perdus selon le segment CNOUVSEG, ainsi que des sections dtailles pour chaque catgorie de clients.




Anne	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	40109	10.78%
2021	29384	3413	40109	8.51%
2022	26275	3109	40109	7.75%
2023	20360	3189	40109	7.95%




import matplotlib.pyplot as plt
import pandas as pd

# Data for the plots
data_global = {
    'Anne': [2019, 2020, 2021, 2022, 2023],
    'Nombre total de clients perdus': [43082, 37769, 34718, 38501, 45615],
    'Pourcentage de perte': [0.1091, 0.0942, 0.0852, 0.0908, 0.1036]
}

data_er = {
    'Anne': [2019, 2020, 2021, 2022, 2023],
    'Nombre de clients perdus': [8110, 5831, 5196, 6216, 7799],
    'Pourcentage de perte': [0.1227, 0.0887, 0.0779, 0.0904, 0.1117]
}

data_ac_ag_pl = {
    'Anne': [2019, 2020, 2021, 2022, 2023],
    'Nombre de clients perdus': [36982, 33854, 31239, 34118, 40085],
    'Pourcentage de perte': [0.1118, 0.1005, 0.0913, 0.0859, 0.1094]
}

data_case_study = {
    'Anne': [2019, 2020, 2021, 2022, 2023],
    'Clients restants': [37119, 32797, 29384, 26275, 20360],
    'Clients perdus': [2990, 4322, 3413, 3109, 3189],
    'Pourcentage de perte': [0.0745, 0.1164, 0.1041, 0.1058, 0.1213]
}

# Creating DataFrames
df_global = pd.DataFrame(data_global)
df_er = pd.DataFrame(data_er)
df_ac_ag_pl = pd.DataFrame(data_ac_ag_pl)
df_case_study = pd.DataFrame(data_case_study)

# Plotting
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Global losses plot
axes[0, 0].plot(df_global['Anne'], df_global['Nombre total de clients perdus'], marker='o', linestyle='-')
axes[0, 0].set_title('Perte Globale de Clients')
axes[0, 0].set_xlabel('Anne')
axes[0, 0].set_ylabel('Nombre total de clients perdus')

# ER losses plot
axes[0, 1].plot(df_er['Anne'], df_er['Nombre de clients perdus'], marker='o', linestyle='-', color='orange')
axes[0, 1].set_title('Perte de Clients pour les Entreprises (ER)')
axes[0, 1].set_xlabel('Anne')
axes[0, 1].set_ylabel('Nombre de clients perdus')

# AC AG PL losses plot
axes[1, 0].plot(df_ac_ag_pl['Anne'], df_ac_ag_pl['Nombre de clients perdus'], marker='o', linestyle='-', color='green')
axes[1, 0].set_title('Perte de Clients pour les Autres Professionnels (AC AG PL)')
axes[1, 0].set_xlabel('Anne')
axes[1, 0].set_ylabel('Nombre de clients perdus')

# Case study losses plot
axes[1, 1].plot(df_case_study['Anne'], df_case_study['Clients perdus'], marker='o', linestyle='-', color='red')
axes[1, 1].set_title('Perte de Clients dans l\'tude de Cas')
axes[1, 1].set_xlabel('Anne')
axes[1, 1].set_ylabel('Nombre de clients perdus')

plt.tight_layout()
plt.show()








Rapport sur la Perte de Clients Professionnels (CCLIKPI)
Introduction:
Ce rapport prsente les statistiques dtailles sur le nombre de clients professionnels perdus par anne, en se concentrant sur deux axes principaux. Le premier axe examine la perte totale de clients professionnels de 2018  2023. Le deuxime axe se concentre sur une tude de cas spcifique, suivant la perte de clients ayant une anciennet de 2 mois  partir de 2018.

1. Axe 1 : Perte Totale de Clients Professionnels de 2018  2023

Perte Globale de Clients
Cette section fournit une vue d'ensemble de la perte totale de clients professionnels au fil des ans. Les chiffres montrent l'volution annuelle des pertes, ce qui permet d'identifier les tendances et les annes avec des fluctuations significatives.

Anne	Nombre total de clients perdus	Pourcentage de perte
2019	43082	10.91%
2020	37769	9.42%
2021	34718	8.52%
2022	38501	9.08%
2023	45615	10.36%
Observations :

En 2023, le nombre total de clients perdus a augment, atteignant 45615, avec un taux de perte de 10.36%.
Les pertes annuelles de clients montrent une lgre fluctuation mais une tendance globale stable avec une lgre hausse en 2023.
Perte de Clients pour les Entreprises (CNOUVSEG=ER)
Cette sous-section se concentre spcifiquement sur les pertes de clients parmi les entreprises (CNOUVSEG=ER). Les donnes permettent de comprendre comment ce segment particulier a volu par rapport aux autres.

Anne	Nombre de clients perdus	Pourcentage de perte
2019	8110	12.27%
2020	5831	8.87%
2021	5196	7.80%
2022	6216	9.04%
2023	7799	11.17%
Observations :

Pour les entreprises, les pertes ont diminu en 2020 et 2021, puis augment en 2022 et 2023, atteignant 11.17% en 2023.
La perte annuelle la plus faible tait en 2021 avec 7.80%.
Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL)
Cette section examine la perte de clients pour les autres types de clients professionnels (CNOUVSEG=AC AG PL), offrant une vue comparative avec le segment des entreprises.

Anne	Nombre de clients perdus	Pourcentage de perte
2019	36982	11.18%
2020	33854	10.05%
2021	31239	9.13%
2022	34118	8.59%
2023	40085	10.94%
Observations :

La perte de clients parmi les autres clients professionnels montre une tendance similaire  celle des entreprises, avec une baisse en 2020 et 2021 et une remonte en 2022 et 2023.
En 2023, la perte a augment  10.94%, indiquant une tendance inquitante.
2. Axe 2 : Perte de Clients Professionnels dans le Cadre de l'tude de Cas

Cette section suit un groupe spcifique de clients ayant une anciennet de 2 mois  partir de 2018, observant leur fidlit au fil des ans. Cette approche permet de voir comment ces clients spcifiques ont ragi sur une priode prolonge.

Anne	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	37119	11.64%
2021	29384	3413	32797	10.41%
2022	26275	3109	29384	10.58%
2023	20360	3189	26275	12.13%
Observations :

En 2023, il restait 20360 clients sur les 40109 initiaux, ce qui reprsente une perte de 42% des clients initiaux.
Les pertes annuelles ont montr une augmentation progressive, soulignant la ncessit de stratgies de rtention plus efficaces.
Conclusion :
Les donnes montrent une tendance inquitante de perte de clients dans les deux segments de clients professionnels. Les entreprises et les autres clients professionnels voient tous deux une augmentation progressive des pertes de clients au fil des annes, avec des pourcentages de perte annuels relativement levs. En particulier, l'tude de cas a rvl une perte de 42% des clients initiaux d'ici 2023. Cette analyse souligne la ncessit de stratgies de rtention client plus efficaces pour inverser cette tendance.

Pour des analyses plus dtailles et interactives, veuillez consulter le tableau de bord Power BI.

Cordialement,


























Rapport sur la Perte de Clients Professionnels (CCLIKPI)
Introduction:
Ce rapport prsente une analyse dtaille de la perte de clients professionnels (CCLIKPI) de 2018  2023, divise en deux axes principaux pour mieux comprendre les tendances et les causes des pertes.

1. Axe 1 : Perte Totale de Clients Professionnels de 2018  2023

Perte Globale de Clients
Cette section fournit une vue d'ensemble de la perte totale de clients professionnels au fil des ans. Les chiffres montrent l'volution annuelle des pertes, ce qui permet d'identifier les tendances et les annes avec des fluctuations significatives.

Anne	Nombre total de clients perdus	Pourcentage de perte
2019	43082	10.91%
2020	37769	9.42%
2021	34718	8.52%
2022	38501	9.08%
2023	45615	10.36%
Nous observons que le nombre total de clients perdus a vari chaque anne, avec une augmentation notable en 2023, atteignant 10.36%. Ces donnes montrent l'importance de surveiller continuellement les tendances de perte de clients pour identifier les causes sous-jacentes et ajuster les stratgies de rtention.

Perte de Clients pour les Entreprises (CNOUVSEG=ER)
Cette sous-section se concentre spcifiquement sur les pertes de clients parmi les entreprises (CNOUVSEG=ER). Les donnes permettent de comprendre comment ce segment particulier a volu par rapport aux autres.

Anne	Nombre de clients perdus	Pourcentage de perte
2019	8110	12.27%
2020	5831	8.87%
2021	5196	7.80%
2022	6216	9.04%
2023	7799	11.17%
Pour les entreprises, nous notons une baisse des pertes en 2020 et 2021, suivie d'une augmentation en 2022 et 2023. Ces fluctuations peuvent tre dues  divers facteurs conomiques ou  des changements dans les offres de services.

Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL)
Cette section examine la perte de clients pour les autres types de clients professionnels (CNOUVSEG=AC AG PL), offrant une vue comparative avec le segment des entreprises.

Anne	Nombre de clients perdus	Pourcentage de perte
2019	36982	11.18%
2020	33854	10.05%
2021	31239	9.13%
2022	34118	8.59%
2023	40085	10.94%
La perte de clients parmi les autres clients professionnels montre une tendance similaire  celle des entreprises, avec une baisse en 2020 et 2021 et une remonte en 2022 et 2023. Ces donnes peuvent aider  cibler des interventions spcifiques pour chaque segment.

2. Axe 2 : Perte de Clients Professionnels dans le Cadre de l'tude de Cas

Cette section suit un groupe spcifique de clients ayant une anciennet de 2 mois  partir de 2018, observant leur fidlit au fil des ans. Cette approche permet de voir comment ces clients spcifiques ont ragi sur une priode prolonge.

Anne	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	37119	11.64%
2021	29384	3413	32797	10.41%
2022	26275	3109	29384	10.58%
2023	20360	3189	26275	12.13%
Nous observons que, dans cette tude de cas, 42% des clients initiaux ont t perdus d'ici 2023. Ces chiffres montrent une perte progressive et significative de ce groupe de clients, soulignant l'importance de stratgies de rtention cibles pour maintenir la fidlit des clients sur le long terme.

Conclusion :
Environ 42 % des clients initiaux ont t perdus dans le cadre de notre tude de cas d'ici 2023. Ces analyses montrent l'importance de surveiller et de comprendre les tendances de perte de clients pour mieux adapter les stratgies de rtention et amliorer la satisfaction et la fidlit des clients.

Pour des analyses plus dtailles et interactives, veuillez consulter le tableau de bord Power BI.

























import pandas as pd

def summarize_key_existence(original_df, key, stop_dfs):
    summaries = []
    current_df = original_df.copy()

    for i, stop_df in enumerate(stop_dfs):
        unique_keys_original = set(current_df[key].unique())
        unique_keys_stop = set(stop_df[key].unique())

        # Find keys that exist in both dataframes
        keys_exist = unique_keys_original & unique_keys_stop
        
        # Find keys that are in the original dataframe but not in the current stop dataframe
        keys_missing = unique_keys_original - unique_keys_stop

        # Update the current_df to only keep keys that exist in the current stop dataframe
        current_df = current_df[current_df[key].isin(keys_exist)]

        # Summarize the results
        summary = {
            'stop_number': i + 1,
            'keys_exist': len(keys_exist),
            'keys_missing': len(keys_missing),
            'total_keys_left': len(current_df)
        }
        summaries.append(summary)

    return summaries

# Sample dataframes for testing
original_df = pd.DataFrame({
    'key': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

df1 = pd.DataFrame({
    'key': [2, 4, 6, 8, 10],
    'value': ['b', 'd', 'f', 'h', 'j']
})

df2 = pd.DataFrame({
    'key': [2, 3, 6, 7],
    'value': ['b', 'c', 'f', 'g']
})

df3 = pd.DataFrame({
    'key': [1, 3, 5, 7],
    'value': ['a', 'c', 'e', 'g']
})

df4 = pd.DataFrame({
    'key': [1, 2, 3, 9],
    'value': ['a', 'b', 'c', 'i']
})

df5 = pd.DataFrame({
    'key': [1, 5, 9],
    'value': ['a', 'e', 'i']
})

# List of stop dataframes
stop_dfs = [df1, df2, df3, df4, df5]

# Specify the key column
key_column = 'key'

# Get the summaries
summaries = summarize_key_existence(original_df, key_column, stop_dfs)

for summary in summaries:
    print(f"Stop {summary['stop_number']}: Keys exist = {summary['keys_exist']}, Keys missing = {summary['keys_missing']}, Total keys left = {summary['total_keys_left']}")


















import pandas as pd

def check_key_existence(df1, df2, key):
    # Get unique values of the key from the first dataframe
    unique_keys_df1 = set(df1[key].unique())
    
    # Get unique values of the key from the second dataframe
    unique_keys_df2 = set(df2[key].unique())
    
    # Find keys that exist in both dataframes
    keys_exist = unique_keys_df1 & unique_keys_df2
    
    # Find keys that are in the first dataframe but not in the second
    keys_missing = unique_keys_df1 - unique_keys_df2
    
    result = {
        'total_keys_df1': len(unique_keys_df1),
        'keys_exist_in_both': len(keys_exist),
        'keys_missing_in_df2': len(keys_missing)
    }
    
    return result

# Sample dataframes for testing
df1 = pd.DataFrame({
    'key': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'key': [2, 4, 5, 6],
    'value': ['b', 'd', 'e', 'f']
})

# Specify the key column
key_column = 'key'

# Get the result
result = check_key_existence(df1, df2, key_column)

print(result)



















import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'cclikpi': [3, 4, 3, 4, 5],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'cclikpi': [4, 5, 4, 5, 6],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'cclikpi': [5, 6, 5, 6, 7],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of cclikpi to track
cclikpi_list = [1, 2, 3, 4, 5]

# Combine dataframes into one
dataframes = [df1, df2, df3, df4, df5]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Filter combined_df to include only cclikpi from the original list
filtered_df = combined_df[combined_df['cclikpi'].isin(cclikpi_list)]

# Initialize an empty DataFrame to store the results
losses_per_month = pd.DataFrame(columns=['year', 'month', 'losses'])

# Iterate over each year and month to calculate the number of losses
for year in filtered_df['year'].unique():
    for month in range(1, 13):
        if month == 12:
            next_year = year + 1
            next_month = 1
        else:
            next_year = year
            next_month = month + 1
        
        current_month_df = filtered_df[(filtered_df['year'] == year) & (filtered_df['month'] == month)]
        next_month_df = filtered_df[(filtered_df['year'] == next_year) & (filtered_df['month'] == next_month)]
        
        current_clients = set(current_month_df['cclikpi'])
        next_clients = set(next_month_df['cclikpi'])
        
        lost_clients = current_clients - next_clients
        losses_per_month = losses_per_month.append({'year': year, 'month': month, 'losses': len(lost_clients)}, ignore_index=True)

print(losses_per_month)

















import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] <= month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    
    previous_clients.add(cclikpi)

# Mark clients as 'left' if they are not present in the next month or the first month of the next year
for year in combined_df['year'].unique():
    for month in range(1, 13):  # Go through all months in a year
        current_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]
        
        if month < 12:  # Check with the next month in the same year
            next_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month + 1)]
        else:  # Check with the first month of the next year
            next_month_df = combined_df[(combined_df['year'] == year + 1) & (combined_df['month'] == 1)]
        
        current_month_clients = set(current_month_df['cclikpi'])
        next_month_clients = set(next_month_df['cclikpi'])
        
        left_clients = current_month_clients - next_month_clients
        
        for client in left_clients:
            combined_df.loc[(combined_df['cclikpi'] == client) & (combined_df['year'] == year) & (combined_df['month'] == month), 'status'] = 'left'

print(combined_df)











import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] <= month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    
    previous_clients.add(cclikpi)

# Mark clients as 'left' if they are not present in the next month
for year in combined_df['year'].unique():
    for month in range(1, 12):  # Only go up to month 11 to compare with next month
        current_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]
        next_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month + 1)]
        current_month_clients = set(current_month_df['cclikpi'])
        next_month_clients = set(next_month_df['cclikpi'])
        
        left_clients = current_month_clients - next_month_clients
        
        for client in left_clients:
            combined_df.loc[(combined_df['cclikpi'] == client) & (combined_df['year'] == year) & (combined_df['month'] == month), 'status'] = 'left'

print(combined_df)














import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018_jan = pd.DataFrame({'cclikpi': [1, 2, 3], 'revenue': [100, 200, 300], 'category': ['A', 'B', 'C']})
data_2018_feb = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [150, 250, 350], 'category': ['B', 'C', 'A']})
data_2019_jan = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [200, 300, 400], 'category': ['B', 'C', 'A']})
data_2019_feb = pd.DataFrame({'cclikpi': [3, 4, 5], 'revenue': [250, 350, 450], 'category': ['C', 'A', 'B']})

# List of dataframes and corresponding months and years
dataframes_2018 = [(data_2018_jan, '2018-01'), (data_2018_feb, '2018-02')]
dataframes_2019 = [(data_2019_jan, '2019-01'), (data_2019_feb, '2019-02')]

# Combine dataframes into a list with month information
dataframes = dataframes_2018 + dataframes_2019

# Add a 'month' column to each dataframe
for df, month in dataframes:
    df['month'] = month

# Initialize a list to store lost client details each year
lost_clients_details = []

# Combine dataframes for each year
combined_data = pd.concat([df for df, month in dataframes], ignore_index=True)

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Identify lost clients between each month
for i in range(1, len(dataframes)):
    previous_month_df = dataframes[i-1][0]
    current_month_df = dataframes[i][0]
    
    previous_month_clients = set(previous_month_df['cclikpi'])
    current_month_clients = set(current_month_df['cclikpi'])
    
    lost_clients = previous_month_clients - current_month_clients
    lost_clients_df = previous_month_df[previous_month_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_month'] = dataframes[i][1]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each month
loss_df = pd.DataFrame({
    'Month': [month for _, month in dataframes[1:]],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Month'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()








import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create a list to store the final rows including the left clients
final_rows = []

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    # Get the clients present in the current month
    current_month_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_month_clients:
        row['status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_month_clients:
        row['status'] = 'stayed'
    elif cclikpi in previous_clients and cclikpi not in current_month_clients:
        row['status'] = 'left'
        final_rows.append(row.copy())
        continue  # Skip adding this row to previous_clients
    
    previous_clients.add(cclikpi)
    final_rows.append(row.copy())

# Ensure clients who left are marked as 'left' in the subsequent years and months
for client in previous_clients:
    if client not in current_month_clients:
        for year in range(2021, 2023):  # Adjust years according to the data
            for month in range(1, 13):
                left_row = {'cclikpi': client, 'month': month, 'year': year, 'status': 'left'}
                final_rows.append(left_row)

# Create the final dataframe
final_df = pd.DataFrame(final_rows).sort_values(by=['year', 'month', 'cclikpi'])

print(final_df)












import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] <= month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    elif cclikpi in previous_clients and cclikpi not in current_clients:
        combined_df.at[index, 'status'] = 'left'
    
    previous_clients.add(cclikpi)

# Update the status for those who left in the previous year and did not join again
for cclikpi in all_clients:
    if combined_df[(combined_df['cclikpi'] == cclikpi) & (combined_df['status'] == 'left')].empty:
        last_month = combined_df[(combined_df['cclikpi'] == cclikpi)]['month'].max()
        combined_df.loc[(combined_df['cclikpi'] == cclikpi) & (combined_df['month'] > last_month), 'status'] = 'left'

print(combined_df)









import pandas as pd

# Sample data
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
previous_clients = set()
current_clients = set(combined_df['cclikpi'].unique())

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    if cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    elif cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi not in current_clients:
        combined_df.at[index, 'status'] = 'left'
    
    # Update previous_clients for the next iteration
    previous_clients.update(current_clients)
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]['cclikpi'])

# Update the status for those who left in the previous year and did not join again
for cclikpi in previous_clients:
    if cclikpi not in current_clients:
        combined_df.loc[(combined_df['cclikpi'] == cclikpi) & (combined_df['year'] > year), 'status'] = 'left'

print(combined_df)










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Combine all dataframes into one DataFrame
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize a dictionary to track the status of each client
client_status = {}

# Initialize the 'status' column
combined_df['status'] = 'Stayed'

# Update the status and pnb/pnb-rep columns
for index, row in combined_df.iterrows():
    cclikpi = row['CCLIKPI']
    
    if cclikpi in client_status:
        if client_status[cclikpi] == 'Left':
            combined_df.at[index, 'status'] = 'Left'
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
    else:
        # Check if the client is present in the next month's data
        next_month_df = combined_df[(combined_df['year'] == row['year']) & (combined_df['month'] == (row['month'] + 1))]
        
        if cclikpi not in set(next_month_df['CCLIKPI']):
            combined_df.at[index, 'status'] = 'Left'
            client_status[cclikpi] = 'Left'
        else:
            client_status[cclikpi] = 'Stayed'

# Update all remaining occurrences of clients marked as 'Left'
for cclikpi, status in client_status.items():
    if status == 'Left':
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'status'] = 'Left'
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'pnb'] = 0
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'pnb-rep'] = 0

print(combined_df)










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Combine all dataframes into one DataFrame
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Create a column to store the status
combined_df['status'] = 'Stayed'

# Track the status of clients
client_status = {}

# Update status to 'Left' and set pnb and pnb-rep to 0 when a client is lost
for index, row in combined_df.iterrows():
    cclikpi = row['CCLIKPI']
    
    if cclikpi in client_status:
        # If the client was previously marked as 'Left', keep it 'Left'
        if client_status[cclikpi] == 'Left':
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
        else:
            client_status[cclikpi] = 'Stayed'
    else:
        # If the client is not in the current month's data, mark as 'Left'
        current_month_df = combined_df[(combined_df['year'] == row['year']) & (combined_df['month'] == row['month'])]
        current_month_clients = set(current_month_df['CCLIKPI'])
        if cclikpi not in current_month_clients:
            combined_df.at[index, 'status'] = 'Left'
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
            client_status[cclikpi] = 'Left'
        else:
            client_status[cclikpi] = 'Stayed'

# Ensure that the first occurrence of each client has the correct 'Stayed' status
for cclikpi in client_status.keys():
    if client_status[cclikpi] == 'Left':
        first_occurrence_index = combined_df[combined_df['CCLIKPI'] == cclikpi].index[0]
        combined_df.at[first_occurrence_index, 'status'] = 'Stayed'

print(combined_df)






import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Identify lost clients year by year
lost_clients_count_year = []
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['CCLIKPI'])
    current_year_clients = set(current_year_df['CCLIKPI'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_count_year.append(len(lost_clients))

loss_df_year = pd.DataFrame({
    'Year': [df['year'].iloc[0] for df in dataframes[1:]],
    'Lost Clients': lost_clients_count_year
})

print(loss_df_year)

# Combine all dataframes into one with month and year information
combined_df = pd.concat(dataframes, ignore_index=True)

# Identify lost clients month by month
lost_clients_count_month = []
combined_df = combined_df.sort_values(by=['year', 'month'])
for i in range(1, len(combined_df)):
    previous_month_df = combined_df.iloc[:i]
    current_month_df = combined_df.iloc[i:]
    
    previous_month_clients = set(previous_month_df['CCLIKPI'])
    current_month_clients = set(current_month_df['CCLIKPI'])
    
    lost_clients = previous_month_clients - current_month_clients
    
    lost_clients_count_month.append(len(lost_clients))

loss_df_month = pd.DataFrame({
    'Year-Month': combined_df['year'].astype(str) + '-' + combined_df['month'].astype(str),
    'Lost Clients': [0] + lost_clients_count_month  # Adding 0 for the first month
})

print(loss_df_month)

# Plot the number of lost clients each year
plt.figure(figsize=(10, 6))
plt.plot(loss_df_year['Year'], loss_df_year['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Year')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.grid(True)
plt.show()

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df_month['Year-Month'], loss_df_month['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Year-Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()











import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018_jan = pd.DataFrame({'cclikpi': [1, 2, 3], 'revenue': [100, 200, 300], 'category': ['A', 'B', 'C']})
data_2018_feb = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [150, 250, 350], 'category': ['B', 'C', 'A']})
data_2019_jan = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [200, 300, 400], 'category': ['B', 'C', 'A']})
data_2019_feb = pd.DataFrame({'cclikpi': [3, 4, 5], 'revenue': [250, 350, 450], 'category': ['C', 'A', 'B']})

# List of dataframes and corresponding months and years
dataframes_2018 = [(data_2018_jan, '2018-01'), (data_2018_feb, '2018-02')]
dataframes_2019 = [(data_2019_jan, '2019-01'), (data_2019_feb, '2019-02')]

# Combine dataframes into a list with month information
dataframes = dataframes_2018 + dataframes_2019

# Add a 'month' column to each dataframe
for df, month in dataframes:
    df['month'] = month

# Initialize a list to store lost client details each year
lost_clients_details = []

# Combine dataframes for each year
combined_data = pd.concat([df for df, month in dataframes], ignore_index=True)

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Identify lost clients between each month
for i in range(1, len(dataframes)):
    previous_month_df = dataframes[i-1][0]
    current_month_df = dataframes[i][0]
    
    previous_month_clients = set(previous_month_df['cclikpi'])
    current_month_clients = set(current_month_df['cclikpi'])
    
    lost_clients = previous_month_clients - current_month_clients
    lost_clients_df = previous_month_df[previous_month_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_month'] = dataframes[i][1]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each month
loss_df = pd.DataFrame({
    'Month': [month for _, month in dataframes[1:]],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Month'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()





import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each year
loss_df = pd.DataFrame({
    'Year': years[1:],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each year
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Year'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Year')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.grid(True)
plt.show()










Environ 32,2 % des clients ont t perdus tous types confondus entre 2019 et 2023.

Pour des analyses plus dtailles et interactives, veuillez consulter le tableau de bord Power BI.

Cordialement,



import pandas as pd

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

# Combine all years data for identifying clients who stayed
all_years_data = pd.concat(dataframes, ignore_index=True)

# Identify unique clients who stayed
stayed_clients = all_years_data[~all_years_data['cclikpi'].isin(lost_clients_combined_df['cclikpi'])].drop_duplicates('cclikpi')
stayed_clients['status'] = 'Stayed'
stayed_clients['lost_year'] = None

# Combine lost and stayed clients
combined_df = pd.concat([lost_clients_combined_df, stayed_clients], ignore_index=True)

# Add columns for visualization
combined_df['initial_year'] = combined_df.groupby('cclikpi')['lost_year'].transform('min')
combined_df['final_year'] = combined_df.groupby('cclikpi')['lost_year'].transform('max')
combined_df['total_revenue'] = combined_df[['revenue_2018', 'revenue_2019', 'revenue_2020', 'revenue_2021', 'revenue_2022']].sum(axis=1)

print(combined_df)

# Export the combined DataFrame to a CSV file for use in Power BI
combined_df.to_csv('client_status_analysis.csv', index=False)
print("Data exported to 'client_status_analysis.csv' for use in Power BI.")










import pandas as pd

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

# Isolate the 'cclikpi' column and remove duplicates to get unique lost clients
unique_lost_clients = lost_clients_combined_df['cclikpi'].drop_duplicates()

# Calculate the total number of unique clients lost
total_unique_clients_lost = len(unique_lost_clients)

print("Total number of unique clients lost:", total_unique_clients_lost)











Rapport sur la Perte de Clients Professionnels (CCLIKPI)

Introduction:
Ce rapport prsente les statistiques dtailles sur le nombre de clients professionnels perdus par anne, en se concentrant sur deux segments : les entreprises (CNOUVSEG=ER) et les autres clients professionnels (CNOUVSEG=AC PL AG). Les donnes incluent galement des mesures de frquence et des pourcentages de pertes annuelles.

1. Perte de Clients pour les Entreprises (CNOUVSEG=ER) :

Anne	Nombre de clients perdus	% de perte annuelle
2023	5635	8.8%
2022	5596	8.7%
2021	4776	7.9%
2020	4875	8%
Observations :

En 2023, le nombre de clients perdus pour les entreprises a lgrement augment, atteignant 5635, avec un taux de perte de 8.8%.
La perte annuelle de clients a fluctu entre 7.9% et 8.8% de 2020  2023, indiquant une tendance stable mais lgrement croissante.
2. Perte de Clients pour les Autres Professionnels (CNOUVSEG=AC PL AG) :

Anne	Nombre de clients perdus	% de perte annuelle
2023	35819	10.7%
2022	33410	10.4%
2021	29445	9.5%
2020	30937	10.3%
Observations :

Le nombre de clients perdus dans ce segment a considrablement augment en 2023, atteignant 35819, avec un taux de perte de 10.7%.
La perte annuelle de clients dans ce segment montre une tendance  la hausse, passant de 9.5% en 2021  10.7% en 2023.
Analyse des Tendances de Perte de Clients :

Entre 2019 et 2020 :

Entreprises (ER) : 8% de clients perdus.
Autres Professionnels (AC AG PL) : 10.3% de clients perdus.
Entre 2020 et 2021 :

Entreprises (ER) : 7.9% de clients perdus.
Autres Professionnels (AC AG PL) : 9.5% de clients perdus.
Entre 2021 et 2022 :

Entreprises (ER) : 8.7% de clients perdus.
Autres Professionnels (AC AG PL) : 10.4% de clients perdus.
Entre 2022 et 2023 :

Entreprises (ER) : 8.8% de clients perdus.
Autres Professionnels (AC AG PL) : 10.7% de clients perdus.
Conclusion:
Les donnes montrent une tendance inquitante de perte de clients dans les deux segments de clients professionnels. Les entreprises et les autres clients professionnels voient tous deux une augmentation progressive des pertes de clients au fil des annes, avec des pourcentages de perte annuels relativement levs. Cette analyse souligne la ncessit de stratgies de rtention client plus efficaces pour inverser cette tendance.

Cordialement,










import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

print(lost_clients_combined_df)

# Export the combined DataFrame to a CSV file for use in Power BI
lost_clients_combined_df.to_csv('lost_clients_analysis.csv', index=False)
print("Data exported to 'lost_clients_analysis.csv' for use in Power BI.")

# Plot the number of lost clients each year by category
plt.figure(figsize=(10, 6))
sns.countplot(data=lost_clients_combined_df, x='lost_year', hue='category', palette='Set2')
plt.title('Number of Clients Lost Each Year by Category')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.legend(title='Category')
plt.grid(True)
plt.show()

# Plot the distribution of lost client revenues by year and category
plt.figure(figsize=(10, 6))
sns.boxplot(data=lost_clients_combined_df.melt(id_vars=['cclikpi', 'category', 'lost_year'], 
                                               value_vars=['revenue_2018', 'revenue_2019', 'revenue_2020', 'revenue_2021'], 
                                               var_name='year', value_name='revenue'),
            x='year', y='revenue', hue='category', palette='Set2')
plt.title('Distribution of Lost Client Revenues by Year and Category')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt



import csv

def list_to_csv(int_list, filename='output.csv'):
    # Convert each integer to a string and add '0' at the beginning
    str_list = ['0' + str(i) for i in int_list]
    
    # Write the list to a CSV file
    with open(filename, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        # Writing the list as a single row
        csvwriter.writerow(str_list)

# Example usage
int_list = [123, 456, 789]
list_to_csv(int_list)



import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, VotingRegressor, BaggingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B'],  # Example category column
    'target': [1.5, 2.0, 0.5, 2.5, 1.0]  # Example continuous target variable
})

# Discretize the continuous variables as described before
# For simplicity, using already discretized values
data['pnb'] = np.digitize(data['pnb'], bins=[150, 250, 350, 450])
data['pnb2023'] = np.digitize(data['pnb2023'], bins=[200, 300, 400, 500])

# Define features and target
X = data[['pnb', 'pnb2023']]
y = data['target']

# Initialize the models
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),
    'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),
    'Voting Regressor': VotingRegressor(estimators=[
        ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)),
        ('et', ExtraTreesRegressor(n_estimators=50, random_state=42))
    ]),
    'Bagging Regressor': BaggingRegressor(n_estimators=100, random_state=42)
}

# Define cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models
def evaluate_model(model, X, y, cv):
    scoring = {
        'MAE': 'neg_mean_absolute_error',
        'MSE': 'neg_mean_squared_error',
        'R2': 'r2'
    }
    
    results = {
        'MAE': -np.mean(cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')),
        'MSE': -np.mean(cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error')),
        'R2': np.mean(cross_val_score(model, X, y, cv=cv, scoring='r2'))
    }
    return results

# Evaluate all models
results = {}
for model_name, model in models.items():
    results[model_name] = evaluate_model(model, X, y, kf)

# Display results
results_df = pd.DataFrame(results).T
print("Evaluation Results:")
print(results_df)






import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier, BaggingClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B'],  # Example category column
    'target': [1, 0, 1, 0, 1]  # Example binary target variable
})

# Discretize the continuous variables as described before
# For simplicity, using already discretized values
data['pnb'] = np.digitize(data['pnb'], bins=[150, 250, 350, 450])
data['pnb2023'] = np.digitize(data['pnb2023'], bins=[200, 300, 400, 500])

# Define features and target
X = data[['pnb', 'pnb2023']]
y = data['target']

# Initialize the models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),
    'Voting Classifier': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
        ('et', ExtraTreesClassifier(n_estimators=50, random_state=42))
    ], voting='soft'),
    'Bagging Classifier': BaggingClassifier(n_estimators=100, random_state=42)
}

# Define cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models
def evaluate_model(model, X, y, cv):
    scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    results = {
        'Accuracy': np.mean(cross_val_score(model, X, y, cv=cv, scoring='accuracy')),
        'F1 Score': np.mean(cross_val_score(model, X, y, cv=cv, scoring='f1')),
        'Precision': np.mean(cross_val_score(model, X, y, cv=cv, scoring='precision')),
        'Recall': np.mean(cross_val_score(model, X, y, cv=cv, scoring='recall')),
        'ROC-AUC': np.mean(cross_val_score(model, X, y, cv=cv, scoring='roc_auc'))
    }
    return results

# Evaluate all models
results = {}
for model_name, model in models.items():
    results[model_name] = evaluate_model(model, X, y, kf)

# Display results
results_df = pd.DataFrame(results).T
print("Evaluation Results:")
print(results_df)









import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The continuous target variable
target = pd.Series([1.5, 2.0, 0.5, 2.5, 1.0], name='target')

# Function to find split points using a decision tree regressor
def find_splits(data, target):
    tree = DecisionTreeRegressor(criterion='mse', max_leaf_nodes=6)
    tree.fit(data.reshape(-1, 1), target)
    
    thresholds = tree.tree_.threshold
    split_points = sorted(threshold for threshold in thresholds if threshold != -2)
    return split_points

# Apply decision tree regressor to find splits for each continuous column
columns_to_discretize = ['pnb', 'pnb2023']
split_points_dict = {col: find_splits(data[col].values, target.values) for col in columns_to_discretize}

# Discretize the data based on the split points
def discretize(data, split_points):
    return np.digitize(data, split_points, right=True)

df_disc = data.copy()
for col, splits in split_points_dict.items():
    df_disc[col] = discretize(data[col].values, splits)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, df_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(df_disc)














import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Function to find split points using a decision tree
def find_splits(data, target):
    tree = DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=6)
    tree.fit(data.reshape(-1, 1), target)
    
    thresholds = tree.tree_.threshold
    split_points = sorted(threshold for threshold in thresholds if threshold != -2)
    return split_points

# Apply decision tree to find splits for each continuous column
columns_to_discretize = ['pnb', 'pnb2023']
split_points_dict = {col: find_splits(data[col].values, target_shifted.values) for col in columns_to_discretize}

# Discretize the data based on the split points
def discretize(data, split_points):
    return np.digitize(data, split_points, right=True)

df_disc = data.copy()
for col, splits in split_points_dict.items():
    df_disc[col] = discretize(data[col].values, splits)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, df_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(df_disc)









import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mdlp.discretization import MDLP

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target_shifted.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, data_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)












import pandas as pd
from mdlp.discretization import MDLP
import numpy as np

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target_shifted.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)














import pandas as pd
from mdlp.discretization import MDLP

# Sample DataFrame with negative numbers for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [-100, 200, -300, 400, -500],
    'pnb2023': [-150, 250, -350, 450, -550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable
target = pd.Series([0, 1, 0, 1, 0], name='target')

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)










import pandas as pd
from mdlp.discretization import MDLP

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable
target = pd.Series([0, 1, 0, 1, 0], name='target')

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)













import numpy as np
import pandas as pd
from math import log2

def entropy(y):
    """Calculate the entropy of a distribution for the classes in y."""
    if len(y) == 0:
        return 0
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

def mdlp_cut(data, labels):
    """Find the optimal cut point for discretization using MDLPC."""
    best_cut = None
    min_mdlp_value = float('inf')
    
    sorted_idx = np.argsort(data)
    sorted_data = data[sorted_idx]
    sorted_labels = labels[sorted_idx]
    
    for i in range(1, len(data)):
        if sorted_data[i] == sorted_data[i - 1]:
            continue
        
        left_labels = sorted_labels[:i]
        right_labels = sorted_labels[i:]
        
        left_entropy = entropy(left_labels)
        right_entropy = entropy(right_labels)
        
        left_prob = len(left_labels) / len(labels)
        right_prob = len(right_labels) / len(labels)
        
        total_entropy = left_prob * left_entropy + right_prob * right_entropy
        
        mdlp_value = entropy(labels) - total_entropy - (log2(len(data) - 1) / len(data))
        
        if mdlp_value < min_mdlp_value:
            min_mdlp_value = mdlp_value
            best_cut = (sorted_data[i] + sorted_data[i - 1]) / 2
    
    return best_cut

def mdlp_discretize(data, labels):
    """Apply the MDLPC algorithm to discretize the data."""
    data = np.array(data)
    labels = np.array(labels)
    
    cut_points = []
    intervals = [(0, len(data))]
    
    while intervals:
        start, end = intervals.pop()
        best_cut = mdlp_cut(data[start:end], labels[start:end])
        
        if best_cut is not None:
            cut_points.append(best_cut)
            left_end = np.searchsorted(data, best_cut, side='right')
            right_start = np.searchsorted(data, best_cut, side='left')
            
            intervals.append((start, left_end))
            intervals.append((right_start, end))
    
    cut_points = sorted(cut_points)
    
    discretized_data = np.digitize(data, cut_points, right=True)
    
    return discretized_data, cut_points

# Sample usage with Iris dataset
from sklearn.datasets import load_iris

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Apply MDLP discretization to each feature
discretized_features = []
cut_points_dict = {}
for i in range(X.shape[1]):
    discretized_feature, cut_points = mdlp_discretize(X[:, i], y)
    discretized_features.append(discretized_feature)
    cut_points_dict[iris.feature_names[i]] = cut_points

# Create a new DataFrame with the discretized features
df_disc = pd.DataFrame(np.array(discretized_features).T, columns=iris.feature_names)
df_disc['target'] = y

# Print the cut points and the new DataFrame with discretized columns
print("Cut points for each feature:")
for feature, cuts in cut_points_dict.items():
    print(f"{feature}: {cuts}")

print("\nNew DataFrame with discretized columns:")
print(df_disc.head())








import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from mdlp.discretization import MDLP

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Create DataFrame from the Iris dataset
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y

# Apply MDLP discretization
transformer = MDLP()
X_disc = transformer.fit_transform(X, y)

# Create a new DataFrame with the discretized columns
df_disc = pd.DataFrame(X_disc, columns=iris.feature_names)
df_disc['target'] = y

# Function to plot before and after discretization
def plot_discretization(df, df_disc, feature_name):
    # Create a figure with subplots
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot the continuous variable (before discretization)
    axs[0].hist(df[feature_name], bins=30, color='skyblue', edgecolor='black')
    axs[0].set_title(f'Before Discretization: {feature_name}')
    axs[0].set_xlabel(feature_name)
    axs[0].set_ylabel('Frequency')
    
    # Plot the discrete variable (after discretization)
    axs[1].hist(df_disc[feature_name], bins=np.arange(df_disc[feature_name].min(), df_disc[feature_name].max() + 1), color='lightcoral', edgecolor='black')
    axs[1].set_title(f'After Discretization: {feature_name}')
    axs[1].set_xlabel(f'{feature_name} (discretized)')
    axs[1].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot for each feature
for feature_name in iris.feature_names:
    plot_discretization(df, df_disc, feature_name)

# Print the new DataFrame with discretized columns
print("New DataFrame with discretized columns:")
print(df_disc.head())










import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from mdlp.discretization import MDLP

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Apply MDLP discretization
transformer = MDLP()
X_disc = transformer.fit_transform(X, y)

# Function to plot before and after discretization
def plot_discretization(X, X_disc, feature_index):
    feature_name = iris.feature_names[feature_index]
    
    # Create a figure with subplots
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot the continuous variable (before discretization)
    axs[0].hist(X[:, feature_index], bins=30, color='skyblue', edgecolor='black')
    axs[0].set_title(f'Before Discretization: {feature_name}')
    axs[0].set_xlabel(feature_name)
    axs[0].set_ylabel('Frequency')
    
    # Plot the discrete variable (after discretization)
    axs[1].hist(X_disc[:, feature_index], bins=np.arange(X_disc[:, feature_index].min(), X_disc[:, feature_index].max() + 1), color='lightcoral', edgecolor='black')
    axs[1].set_title(f'After Discretization: {feature_name}')
    axs[1].set_xlabel(f'{feature_name} (discretized)')
    axs[1].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot for each feature
for i in range(X.shape[1]):
    plot_discretization(X, X_disc, i)

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def entropy(y):
    """Calculate the entropy of a distribution for the classes in y."""
    if len(y) == 0:
        return 0
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

def mdlp_cut(data, labels):
    """Find the optimal cut point for discretization using MDLPC."""
    best_cut = None
    min_mdlp_value = float('inf')
    
    sorted_idx = np.argsort(data)
    sorted_data = data[sorted_idx]
    sorted_labels = labels[sorted_idx]
    
    for i in range(1, len(data)):
        if sorted_data[i] == sorted_data[i - 1]:
            continue
        
        left_labels = sorted_labels[:i]
        right_labels = sorted_labels[i:]
        
        left_entropy = entropy(left_labels)
        right_entropy = entropy(right_labels)
        
        left_prob = len(left_labels) / len(labels)
        right_prob = len(right_labels) / len(labels)
        
        total_entropy = left_prob * left_entropy + right_prob * right_entropy
        
        mdlp_value = entropy(labels) - total_entropy - (log(len(data) - 1) / len(data))
        
        if mdlp_value < min_mdlp_value:
            min_mdlp_value = mdlp_value
            best_cut = sorted_data[i]
    
    return best_cut

def mdlp_discretize(data, labels):
    """Apply the MDLPC algorithm to discretize the data."""
    data = np.array(data)
    labels = np.array(labels)
    
    cut_points = []
    intervals = [(0, len(data))]
    
    while intervals:
        start, end = intervals.pop()
        best_cut = mdlp_cut(data[start:end], labels[start:end])
        
        if best_cut is not None:
            cut_points.append(best_cut)
            left_end = np.searchsorted(data, best_cut, side='right')
            right_start = np.searchsorted(data, best_cut, side='left')
            
            intervals.append((start, left_end))
            intervals.append((right_start, end))
    
    cut_points = sorted(cut_points)
    
    discretized_data = np.digitize(data, cut_points, right=True)
    
    return discretized_data, cut_points

# Sample usage
data = pd.DataFrame({
    'continuous_variable': np.random.rand(100),
    'label': np.random.choice([0, 1], size=100)
})

discretized_data, cut_points = mdlp_discretize(data['continuous_variable'], data['label'])
data['discretized_variable'] = discretized_data

# Visualization
plt.figure(figsize=(12, 6))

# Histogram of continuous variable
plt.subplot(1, 2, 1)
sns.histplot(data['continuous_variable'], bins=20, kde=True)
plt.title('Continuous Variable Before Discretization')
plt.xlabel('Continuous Variable')
plt.ylabel('Frequency')

# Bar plot of discretized variable
plt.subplot(1, 2, 2)
sns.countplot(x='discretized_variable', data=data)
plt.title('Discretized Variable After MDLPC')
plt.xlabel('Discretized Variable')
plt.ylabel('Count')

plt.tight_layout()
plt.show()














import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi', 'category'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Define a color palette for the categories
palette = sns.color_palette("husl", data_long['category'].nunique())
color_map = {category: palette[i] for i, category in enumerate(data_long['category'].unique())}

# Plot the evolution of PNB for each company with lines colored based on the category
plt.figure(figsize=(10, 6))
for key, grp in data_long.groupby(['cclikpi']):
    category = grp['category'].values[0]
    plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}', color=color_map[category])

# Create a custom legend for the categories
handles = [plt.Line2D([0], [0], color=color_map[cat], lw=4, label=cat) for cat in color_map]
plt.legend(handles=handles, title='Category')

plt.xlabel('Year')
plt.ylabel('PNB Value')
plt.title('Evolution of PNB from 2018 to 2023 for Each Company')
plt.grid(True)
plt.show()














import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi', 'category'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Define a color palette for the categories
palette = sns.color_palette("husl", data_long['category'].nunique())
color_map = {category: palette[i] for i, category in enumerate(data_long['category'].unique())}

# Function to plot batches of companies
def plot_batches(data, batch_size=10):
    unique_ids = data['cclikpi'].unique()
    num_batches = len(unique_ids) // batch_size + (1 if len(unique_ids) % batch_size != 0 else 0)
    
    for i in range(num_batches):
        plt.figure(figsize=(10, 6))
        batch_ids = unique_ids[i * batch_size: (i + 1) * batch_size]
        batch_data = data[data['cclikpi'].isin(batch_ids)]
        
        for key, grp in batch_data.groupby(['cclikpi']):
            category = grp['category'].values[0]
            plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}', color=color_map[category])
        
        plt.xlabel('Year')
        plt.ylabel('PNB Value')
        plt.title(f'Evolution of PNB from 2018 to 2023 for Companies {i * batch_size + 1} to {(i + 1) * batch_size}')
        plt.legend(title='cclikpi')
        plt.grid(True)
        plt.show()

# Plot the data in batches
plot_batches(data_long, batch_size=2)  # Adjust batch_size as needed




















import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550]
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Function to plot batches of companies
def plot_batches(data, batch_size=10):
    unique_ids = data['cclikpi'].unique()
    num_batches = len(unique_ids) // batch_size + (1 if len(unique_ids) % batch_size != 0 else 0)
    
    for i in range(num_batches):
        plt.figure(figsize=(10, 6))
        batch_ids = unique_ids[i * batch_size: (i + 1) * batch_size]
        batch_data = data[data['cclikpi'].isin(batch_ids)]
        
        for key, grp in batch_data.groupby(['cclikpi']):
            plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}')
        
        plt.xlabel('Year')
        plt.ylabel('PNB Value')
        plt.title(f'Evolution of PNB from 2018 to 2023 for Companies {i * batch_size + 1} to {(i + 1) * batch_size}')
        plt.legend(title='cclikpi')
        plt.grid(True)
        plt.show()

# Plot the data in batches
plot_batches(data_long, batch_size=2)  # Adjust batch_size as needed













import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550]
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Plot the evolution of PNB for each company
plt.figure(figsize=(10, 6))
for key, grp in data_long.groupby(['cclikpi']):
    plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}')

plt.xlabel('Year')
plt.ylabel('PNB Value')
plt.title('Evolution of PNB from 2018 to 2023 for Each Company')
plt.legend(title='cclikpi')
plt.grid(True)
plt.show()










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def find_new_rows_added(df1, df2, id_column, merge_column):
    # Perform the merge
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left', indicator=True)
    
    # Find new rows added by checking for rows in df2 but not in df1
    new_rows = merged_df[merged_df['_merge'] == 'right_only']
    
    return new_rows

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Find new rows added during the merge
new_rows_added = find_new_rows_added(df1, df2, id_column, merge_column)

print("New Rows Added:")
print(new_rows_added)














import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def merge_dataframes(df1, df2, id_column, merge_column):
    # Merge the two DataFrames on the id_column
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left')
    
    return merged_df

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Merge the dataframes and add the specified column from df2 to df1
result_df = merge_dataframes(df1, df2, id_column, merge_column)

print("Resulting DataFrame:")
print(result_df)













import pandas as pd

def get_column_types(df):
    # Create a dictionary with column names as keys and their data types as values
    column_types = {col: df[col].dtype for col in df.columns}
    return column_types

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'type_de_client': ['A', 'B', 'C', 'D', 'E'],
    'value1': [10, 20, 30, 40, 50],
    'value2': [100.0, 200.0, 300.0, 400.0, 500.0]
})

# Get the column types
column_types = get_column_types(data)
print("Column Types:")
print(column_types)









import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'category': ['X', 'Y', 'X', 'Y', 'X', 'X', 'Y'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_orders = {
    'type_de_client': ['A', 'B'],  # Priority order for 'type_de_client'
    'category': ['X', 'Y']         # Priority order for 'category'
}

# Custom aggregation function for categorical variables
def agg_priority(series, priority_order):
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Wrapper function to apply the correct priority order
def agg_wrapper(column, priority_orders):
    return lambda series: agg_priority(series, priority_orders[column])

# Perform the groupby operation and apply the custom aggregation functions
agg_dict = {
    'type_de_client': agg_wrapper('type_de_client', priority_orders),
    'category': agg_wrapper('category', priority_orders),
    'value1': 'sum',  # Example aggregation for numerical columns
    'value2': 'mean'  # Example aggregation for numerical columns
}

grouped_data = data.groupby('id').agg(agg_dict).reset_index()

print("Grouped DataFrame:")
print(grouped_data)






import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_order = ['A', 'B']  # You can easily change this list later

# Custom aggregation function for categorical variables
def agg_priority(series):
    # Get the first value in the series that matches the priority order
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Perform the groupby operation and apply the custom aggregation function
grouped_data = data.groupby('id').agg({
    'type_de_client': agg_priority,  # Apply the custom aggregation function
    'value1': 'sum',                 # Example aggregation for numerical columns
    'value2': 'mean'                 # Example aggregation for numerical columns
}).reset_index()

print("Grouped DataFrame:")
print(grouped_data)









import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column, class_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    # Calculate frequency and percentage of class distribution for matched IDs
    matched_class_freq = matched_df[class_column].value_counts()
    matched_class_percentage = matched_df[class_column].value_counts(normalize=True) * 100
    
    # Filter the DataFrame to get rows where the id is not in the id_list
    unmatched_df = df[~df[id_column].isin(id_list)]
    
    # Calculate frequency and percentage of class distribution for unmatched IDs
    unmatched_class_freq = unmatched_df[class_column].value_counts()
    unmatched_class_percentage = unmatched_df[class_column].value_counts(normalize=True) * 100
    
    return (matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
            unmatched_class_freq, unmatched_class_percentage)

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column names for IDs and class
id_column = 'id'
class_column = 'value'

# Get the results
(matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
 unmatched_class_freq, unmatched_class_percentage) = filter_dataframe_and_find_unmatched(large_df, id_list, id_column, class_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)
print("\nMatched Class Frequency:")
print(matched_class_freq)
print("\nMatched Class Percentage:")
print(matched_class_percentage)
print("\nUnmatched Class Frequency:")
print(unmatched_class_freq)
print("\nUnmatched Class Percentage:")
print(unmatched_class_percentage)









import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'ccli': [1, 2, 3, 4, 5],
    'cclikpi': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'avant_ccli': [1, 2, 3, 6],
    'apre_ccli': [10, 20, 30, 60],
    'cclikpi': [100, 200, 300, 400]
})

# Create a dictionary from df2 for quick lookup
cclikpi_map = df2.set_index('avant_ccli')['cclikpi'].to_dict()

# Function to determine the value for cclikpi2
def get_cclikpi2(ccli, cclikpi):
    return cclikpi_map.get(ccli, cclikpi)

# Apply the function to create the new column in df1
df1['cclikpi2'] = df1.apply(lambda row: get_cclikpi2(row['ccli'], row['cclikpi']), axis=1)

print("Resulting DataFrame:")
print(df1)





import pandas as pd

def filter_by_id_pattern(df, id_column, pattern):
    # Convert the id column to string
    df[id_column] = df[id_column].astype(str)
    
    # Filter rows where the id starts with the specific pattern
    filtered_df = df[df[id_column].str.startswith(pattern)]
    
    return filtered_df

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [3001, 3002, 123, 4003, 3004, 567, 3005],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g']
})

# Specify the column name for IDs and the pattern
id_column = 'id'
pattern = '300'

# Get the filtered DataFrame
filtered_df = filter_by_id_pattern(large_df, id_column, pattern)

print("Filtered DataFrame:")
print(filtered_df)




import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    return matched_df, unmatched_ids

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column name for IDs
id_column = 'id'

# Get the results
matched_df, unmatched_ids = filter_dataframe_and_find_unmatched(large_df, id_list, id_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)










Bonjour Solne,

Je suis en train d'agrger les donnes, mais j'ai un problme avec l'agrgation des variables catgorielles comme "type de foyer" ou "CNOUVSEG". Abidjo et Arthur m'ont donn une liste d'ordre de priorit pour certaines catgories, mais je ne suis pas sr si cela est toujours correct pour mon cas, et en plus, elle n'est pas exhaustive. Aurais-tu des ides ou des suggestions pour m'aider  rsoudre ce problme?

Merci d'avance pour ton aide.


Aurais-tu une liste de priorit pour le reste des variables catgorielles?

Je vous remercie sincrement pour votre offre d'aide. Je n'hsiterai pas  faire appel  vous si j'ai besoin. Votre soutien est grandement apprci


Bonjour Solne,

Je suis en train d'agrger les donnes, mais j'ai un problme avec l'agrgation des variables catgorielles comme "type de foyer" ou "CNOUVSEG". Aurais-tu des ides ou des suggestions pour m'aider  rsoudre ce problme?

Merci d'avance pour ton aide.










import pandas as pd

def get_values_for_same_col1(data, col1, col2):
    # Group by the first column and collect the values of the second column
    result = data.groupby(col1)[col2].apply(list).to_dict()
    return result

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'c', 'd', 'e', 'f', 'g'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to use
col1 = 'col1'
col2 = 'col2'

values_for_same_col1 = get_values_for_same_col1(data, col1, col2)
print(values_for_same_col1)











import pandas as pd

def get_duplicates_across_columns(data, col1, col2):
    # Find duplicate rows based on the specified columns
    duplicate_rows = data[data.duplicated(subset=[col1, col2], keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=[col1, col2])
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to check for duplicates
col1 = 'col1'
col2 = 'col2'

duplicates = get_duplicates_across_columns(data, col1, col2)
print(duplicates)











import pandas as pd

def get_duplicates(data):
    # Find duplicate rows based on all columns
    duplicate_rows = data[data.duplicated(keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=list(data.columns))
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 20, 40, 50, 20, 10]
})

duplicates = get_duplicates(data)
print(duplicates)













import pandas as pd
import numpy as np

def get_indices_not_in_data2(data1, data2, col_name):
    # Drop NaN values from the target column in both dataframes
    data1_clean = data1.dropna(subset=[col_name])
    data2_clean = data2.dropna(subset=[col_name])
    
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2_clean[col_name].astype(data1_clean[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1_clean[~data1_clean[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5, np.nan],
    'value': ['a', 'b', 'c', 'd', 'e', 'f']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0, np.nan],
    'value': ['x', 'y', 'z', 'w']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)







import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2[col_name].astype(data1[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)











import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Get the unique values in the specified column of data2
    data2_values = set(data2[col_name].unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3, 4, 5],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)










import pandas as pd
import numpy as np

# Sample dataframe
data = {
    'ccli': [1, 1, 1, 2, 2, 2, 3, 3, 3, 3],
    'cclikpi': [10, 10, 10, 20, 20, 20, 30, 30, 30, 30],
    'col': [12, 12, 12, 11, 11, 11, 12, 12, 12, 12],  # The column with values from 1 to 12
    'value1': [0, 200, np.nan, 400, 0, 600, 700, 0, np.nan, 0],
    'value2': [100, 0, 0, 300, np.nan, 0, 500, 600, 0, 0]
}

df = pd.DataFrame(data)

# Function to calculate the number of zeros and NaNs in each row
def count_zeros_nans(row):
    return (row == 0).sum() + row.isna().sum()

# Add a column that counts the number of zeros and NaNs in each row
df['zeros_nans_count'] = df.apply(count_zeros_nans, axis=1)

# Sort the dataframe by 'ccli', 'cclikpi', 'col' and 'zeros_nans_count'
df = df.sort_values(by=['ccli', 'cclikpi', 'col', 'zeros_nans_count'])

# Drop duplicates keeping the first one based on 'ccli', 'cclikpi', and 'col' (the one with fewest zeros/nans)
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi', 'col'], keep='first')

# Drop the helper column as it's no longer needed
df_unique = df_unique.drop(columns=['zeros_nans_count'])

# Display the resulting dataframe
print(df_unique)



import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'col': [1, 12, 2, 11, 3, 6, 12],  # The column with values from 1 to 12
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Sort the dataframe by 'ccli', 'cclikpi' and 'col'
df = df.sort_values(by=['ccli', 'cclikpi', 'col'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)















import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-01', '2023-01-02', '2023-01-04'],
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Convert date column to datetime
df['date'] = pd.to_datetime(df['date'])

# Sort the dataframe by 'ccli', 'cclikpi' and 'date'
df = df.sort_values(by=['ccli', 'cclikpi', 'date'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)










Certainly! Here is an in-depth explanation of the first ten laws from Robert Greene's "The 48 Laws of Power":

Law 1: Never Outshine the Master
Concept: Always make those above you feel superior. Do not highlight your own talents to the point of making them feel insecure.
Application: Subtly display your skills while emphasizing your deference to your superiors. Praise their wisdom and ability, ensuring they receive credit.
Example: In the court of Louis XIV, his finance minister, Nicolas Fouquet, outshone the king by throwing an extravagant party. This led to Fouquet's downfall as Louis XIV felt threatened and imprisoned him.
Law 2: Never Put Too Much Trust in Friends, Learn How to Use Enemies
Concept: Friends are more likely to betray you as they become envious or complacent. Enemies, motivated by self-interest, are more reliable when properly managed.
Application: Be cautious with friends and keep a degree of separation. Convert enemies into allies by demonstrating that cooperation is mutually beneficial.
Example: Michael III of the Byzantine Empire relied on his friend Basil the Macedonian, who ultimately betrayed and killed him to take power.
Law 3: Conceal Your Intentions
Concept: Keep people off-balance and in the dark about your plans. This makes them unable to counter your moves.
Application: Use red herrings and decoys to mislead others. Keep your real goals hidden and reveal only enough to maintain trust.
Example: Otto von Bismarck, the Prussian statesman, used various deceptions to keep his opponents guessing about his true political intentions, which allowed him to unify Germany.
Law 4: Always Say Less Than Necessary
Concept: The more you say, the more likely you are to say something foolish. Silence and brevity create an aura of power.
Application: Be concise and deliberate with your words. Let others reveal more than you do, gaining insight into their thoughts and intentions.
Example: When interviewed, Andy Warhol often gave brief, enigmatic answers, which increased his mystique and allure in the art world.
Law 5: So Much Depends on Reputation  Guard It with Your Life
Concept: Your reputation is the cornerstone of your power. A strong reputation increases your presence and influence, while a tarnished reputation diminishes your power.
Application: Protect your reputation at all costs and be vigilant against attacks. Use subtle tactics to undermine the reputations of your rivals.
Example: Julius Caesar carefully cultivated his reputation for being generous and merciful, which helped him gain the loyalty and support of the Roman populace.
Law 6: Court Attention at All Costs
Concept: Stand out and make yourself noticed. Being unseen or forgotten is detrimental to your power.
Application: Create memorable and dramatic events or actions that capture attention. Cultivate an air of mystery and unpredictability.
Example: P.T. Barnum, the famous showman, constantly created public spectacles and stunts to keep himself and his shows in the public eye.
Law 7: Get Others to Do the Work for You, but Always Take the Credit
Concept: Use the skills, knowledge, and effort of others to further your own goals. Taking credit for their work enhances your reputation and power.
Application: Delegate tasks while ensuring that you receive recognition for the results. Reward those who contribute to keep them motivated.
Example: Thomas Edison often employed skilled engineers and researchers who helped him develop his inventions, but he ensured that he was recognized as the primary inventor.
Law 8: Make Other People Come to You  Use Bait If Necessary
Concept: By making others come to you, you control the situation and set the terms. Luring people to you ensures they play by your rules.
Application: Use allurements and incentives to attract people. Once they are drawn in, assert your dominance and influence.
Example: During negotiations, Henry Kissinger would often make foreign diplomats come to him, allowing him to dictate the terms and maintain a position of strength.
Law 9: Win Through Your Actions, Never Through Argument
Concept: Demonstrating your ideas through actions is far more persuasive than arguing. Arguments often lead to resentment and resistance.
Application: Let your actions speak for you. Show, dont tell, and allow people to see the benefits or consequences of your ideas firsthand.
Example: Instead of arguing with critics, Leonardo da Vinci would demonstrate his inventions and artworks, which left little room for doubt about his genius.
Law 10: Infection: Avoid the Unhappy and Unlucky
Concept: Emotions and attitudes are contagious. Associating with those who are negative or unlucky can bring you down.
Application: Distance yourself from those who have a detrimental effect on your well-being and success. Seek out positive and successful individuals.
Example: The French adventurer, Napoleon Bonaparte, avoided associating with people who were constantly negative or failed, focusing instead on those who were optimistic and successful.
These laws emphasize strategic thinking, careful manipulation, and maintaining control over social and professional interactions to enhance one's power and influen


















import pandas as pd
import numpy as np
from scipy.stats.mstats import winsorize

# Sample DataFrame with numeric data (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.randn(100) * 100,
    'B': np.random.randn(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to apply Winsorization to all numeric columns in the DataFrame
def winsorize_dataframe(df, limits=(0.05, 0.05)):
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    df_winsorized = df.copy()
    
    for col in numeric_cols:
        df_winsorized[col] = winsorize(df[col], limits=limits)
    
    return df_winsorized

# Apply Winsorization to the DataFrame
winsorized_df = winsorize_dataframe(df, limits=(0.05, 0.05))

# Display the original and Winsorized DataFrame
print("Original DataFrame:")
print(df.describe())

print("\nWinsorized DataFrame:")
print(winsorized_df.describe())













import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy.stats import f_oneway

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to perform ANOVA test for each categorical variable against the continuous target variable
def anova_test(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"ANOVA test for {col} in relation to {target_var}:")
        
        # Perform ANOVA using statsmodels
        formula = f'{target_var} ~ C({col})'
        model = ols(formula, data=df).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)
        
        print(anova_table)
        print("\n")

# Specify the target column
target_column = 'Target'

# Perform ANOVA tests
anova_test(df, target_column)



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to generate statistical summaries and plots for categorical variables in relation to a continuous target variable
def categorical_summary_with_target(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col} in relation to {target_var}:")
        summary = df.groupby(col)[target_var].describe()
        print(summary)
        print("\n")
        
        # Plotting the distribution of the continuous target variable for each category using box plot
        plt.figure(figsize=(8, 6))
        sns.boxplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

        # Plotting the distribution of the continuous target variable for each category using violin plot
        plt.figure(figsize=(8, 6))
        sns.violinplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

# Specify the target column
target_column = 'Target'

# Generate and plot summaries for categorical variables in relation to the continuous target variable
categorical_summary_with_target(df, target_column)













import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100)
})

# Function to generate statistical summaries for categorical variables
def categorical_summary(df):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col}:")
        print(df[col].value_counts())
        print("\n")
        
        # Plotting the distribution of the categorical variable
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x=col, palette='viridis')
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.show()

# Generate and plot summaries for categorical variables
categorical_summary(df)




















import pandas as pd
import numpy as np

# Sample data
data = {'values': [1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]}
df = pd.DataFrame(data)

# Central Tendency
mean = df['values'].mean()
median = df['values'].median()
mode = df['values'].mode()

# Variability
range_ = df['values'].max() - df['values'].min()
variance = df['values'].var()
std_dev = df['values'].std()
iqr = df['values'].quantile(0.75) - df['values'].quantile(0.25)

# Shape
skewness = df['values'].skew()
kurtosis = df['values'].kurt()

# Summary
summary = df['values'].describe()

print("Mean:", mean)
print("Median:", median)
print("Mode:", mode[0])
print("Range:", range_)
print("Variance:", variance)
print("Standard Deviation:", std_dev)
print("Interquartile Range (IQR):", iqr)
print("Skewness:", skewness)
print("Kurtosis:", kurtosis)
print("\nSummary:\n", summary)




















import pandas as pd

# Sample data
data = {
    'Column1': ['A', 'B', 'A', 'C', 'B', 'A', 'B', 'C'],
    'Column2': [1, 2, 1, 3, 2, 1, 2, 3]
}

df = pd.DataFrame(data)

# Check for duplicates based on Column1 and Column2
duplicates = df.duplicated(subset=['Column1', 'Column2'])
print("Boolean Series indicating duplicates:")
print(duplicates)

# Get only the rows that are duplicates
duplicate_rows = df[df.duplicated(subset=['Column1', 'Column2'], keep=False)]
print("\nDuplicate rows:")
print(duplicate_rows)















duplicates = df.duplicated(subset=['Column1', 'Column2'])
print(duplicates)









import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Split the data into training (non-missing target values) and testing (missing target values) sets
train_df = prepared_df[prepared_df[target_column].notna()]
test_df = prepared_df[prepared_df[target_column].isna()]

# Define features and target
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]
X_test = test_df.drop(columns=[target_column])

# Train the XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)

# Predict the missing values
y_pred = xgb_model.predict(X_test)

# Evaluate the model (Since we don't have true values for test set, we can't compute RMSE here)
# In practice, you would compare y_pred with the true values if they were known

# Add predictions back to the original DataFrame
prepared_df.loc[prepared_df[target_column].isna(), target_column] = y_pred

# Display the results
print("Predicted values for the missing target variable:")
print(prepared_df[prepared_df[target_column].isna()][[target_column]])

# Optionally, if we had true values, we would compute the RMSE like this:
# y_true = true_values_array_for_missing_entries
# rmse = np.sqrt(mean_squared_error(y_true, y_pred))
# print(f"RMSE for XGBoost predictions: {rmse}")













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    
    # Extract the rows with missing values in the target column
    missing_df = df[df[target_column].isna()]
    
    # Extract the rows with non-missing values in the target column
    non_missing_df = df[df[target_column].notna()]
    
    # Extract the true values of the target column for evaluation
    y_true = non_missing_df[target_column].copy()
    
    for method_name, imputer in imputation_methods.items():
        df_copy = non_missing_df.dropna(axis=1, how='any')
        df_copy[target_column] = non_missing_df[target_column]
        
        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        
        y_pred = imputed_df[target_column]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")























import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    y_true = df[target_column][df[target_column].isna()]

    for method_name, imputer in imputation_methods.items():
        df_copy = df.copy()
        df_copy = df_copy.dropna(axis=1, how='any')
        df_copy[target_column] = df[target_column]

        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)

        y_pred = imputed_df[target_column][df[target_column].isna()]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    # Create a copy of the DataFrame
    df_copy = df.copy()
    
    # Separate the target column
    target = df_copy[[target_column]].copy()
    
    # Drop columns with missing values
    df_copy = df_copy.dropna(axis=1, how='any')
    
    # Add the target column back
    df_copy = pd.concat([df_copy, target], axis=1)
    
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Display the prepared DataFrame
print(prepared_df.head())













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        # Ensure the training data does not contain NaNs
        X_train_imputed = imputer.fit_transform(X_train)
        imputed_train_df = pd.DataFrame(X_train_imputed, columns=X_train.columns)
        
        # Impute missing values in the test set using the trained imputer
        X_test = test_df.drop(columns=[target_column])
        X_test_imputed = imputer.transform(X_test)
        imputed_test_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)
        imputed_test_df[target_column] = test_df[target_column]
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")



















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Drop columns with missing values except for the target column
        df_copy = df_copy.dropna(axis=1, how='any', subset=[col for col in df.columns if col != target_column])
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")






















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques using cross-validation
def evaluate_imputation_methods(df, imputation_methods):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    results = {method: [] for method in imputation_methods.keys()}

    for train_index, test_index in kf.split(df):
        train_df, test_df = df.iloc[train_index], df.iloc[test_index]

        for method_name, imputer in imputation_methods.items():
            # Fit the imputer on the training set
            imputer.fit(train_df)
            
            # Transform both the training and test sets
            imputed_train_df = pd.DataFrame(imputer.transform(train_df), columns=df.columns)
            imputed_test_df = pd.DataFrame(imputer.transform(test_df), columns=df.columns)
            
            for col in df.columns:
                if train_df[col].isna().sum() > 0 or test_df[col].isna().sum() > 0:
                    # Calculate RMSE for each column with missing values
                    y_true = test_df[col].dropna()
                    y_pred = imputed_test_df.loc[y_true.index, col]
                    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                    results[method_name].append(rmse)
    
    # Calculate the mean RMSE for each imputation method
    mean_results = {method: np.mean(rmses) for method, rmses in results.items()}
    
    return mean_results

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, imputation_methods)

# Display the results
print("Mean RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")

















Handle Missing Values: Impute missing values to avoid complications in subsequent steps.
Handle Outliers: Address outliers to ensure they do not skew the scaling and modeling.
Handle Categorical Data: Encode categorical variables.
Scale Data: Normalize or standardize the data.
Handle Multicollinearity: Check for and address multicollinearity.










import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")
























import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5,
    'Target': np.random.rand(100) * 100
})

# Calculate correlation matrix
corr_matrix = df.corr()

# Display correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# Calculate R-squared values
target_var = 'Target'
r2_results = {}

for col in df.columns:
    if col != target_var:
        X = df[[col]]
        y = df[target_var]
        model = LinearRegression().fit(X, y)
        y_pred = model.predict(X)
        r2 = r2_score(y, y_pred)
        r2_results[col] = r2

# Display R-squared results
r2_df = pd.DataFrame.from_dict(r2_results, orient='index', columns=['R-Squared']).sort_values(by='R-Squared', ascending=False)
print(r2_df)

# Plot R-squared values
plt.figure(figsize=(10, 6))
sns.barplot(x=r2_df.index, y='R-Squared', data=r2_df, palette='viridis')
plt.title('R-Squared Values of Variables Explaining Target')
plt.xlabel('Variables')
plt.ylabel('R-Squared')
plt.xticks(rotation=45)
plt.show()














import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to plot distribution and detect outliers
def plot_distribution_and_outliers(df):
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in num_cols:
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Histogram and KDE Plot
        sns.histplot(df[col], kde=True, ax=axes[0], color='blue')
        axes[0].set_title(f'Distribution of {col}')
        axes[0].set_xlabel(col)
        
        # Box Plot for outliers
        sns.boxplot(y=df[col], ax=axes[1], color='orange')
        axes[1].set_title(f'Box Plot of {col}')
        
        plt.tight_layout()
        plt.show()
        
        # Detect outliers using IQR method
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        
        print(f'{col}:')
        print(f'Number of outliers: {outliers.shape[0]}')
        if not outliers.empty:
            print('Outliers:')
            print(outliers[[col]])
        print('\n')

# Plot distributions and outliers
plot_distribution_and_outliers(df)








# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Set font size for the plot
plt.rcParams.update({'font.size': 14})

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5, annot_kws={"size": 14})

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black', fontsize=14)

plt.title('Correlation Matrix with p-values (Filtered)', fontsize=18)
plt.xlabel('Variables', fontsize=16)
plt.ylabel('Variables', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()














import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()
















# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.xticks(rotation=45)
plt.show()
















import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_vif(df):
    # Add a constant to the dataframe
    df_with_const = add_constant(df)
    
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = df_with_const.columns
    vif_data["VIF"] = [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])]
    
    return vif_data

# Calculate VIF
vif_data = calculate_vif(df)

# Display the VIF result
print(vif_data)


import matplotlib.pyplot as plt

# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.show()













import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j:
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values')
plt.show()
































import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Store the results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Store the metrics
    results['Model'].append(model_name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1-Score'].append(f1)
    
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")

# Plot the results
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Comparison')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
axes = axes.flatten()

for i, metric in enumerate(metrics):
    axes[i].bar(results['Model'], results[metric], color=['blue', 'green', 'red'])
    axes[i].set_title(metric)
    axes[i].set_ylim([0, 1])
    axes[i].set_xlabel('Model')
    axes[i].set_ylabel(metric)
    for j, val in enumerate(results[metric]):
        axes[i].text(j, val + 0.02, f'{val:.2f}', ha='center')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()




import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Display the matrices
print("Correlation Matrix:")
print(corr_matrix)
print("\nP-Value Matrix:")
print(p_value_matrix)

# Combine the correlation matrix and p-value matrix into one DataFrame for display
combined_matrix = corr_matrix.astype(str) + " (p=" + p_value_matrix.round(4).astype(str) + ")"

print("\nCombined Matrix:")
print(combined_matrix)
















import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")
