import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate non-linear data
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(80) * 0.5

# Fit linear model
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

# Calculate R^2
r2 = r2_score(y, y_pred)
print(f"R^2: {r2}")

# Plot
plt.scatter(X, y, color='black')
plt.plot(X, y_pred, color='blue', linewidth=3)
plt.title(f'Linear Regression on Non-linear Data\nR^2 = {r2:.2f}')
plt.show()


from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit model on training data
model.fit(X_train, y_train)
y_test_pred = model.predict(X_test)

# Calculate R^2 on test data
r2_test = r2_score(y_test, y_test_pred)
print(f"R^2 on test data: {r2_test}")
















import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
quantiles = df[df['Outlier'] == 'Inlier']['Value'].quantile([0.05, 0.1, 0.6, 0.7, 0.8, 0.9])
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()

# Replace outliers based on specified rules
def replace_outliers(row, quantiles, mean_value):
    noise = np.random.normal(0, 1)
    if row['Outlier'] == 'Outlier':
        if row['Value'] > mean_value:
            if row['Value'] > quantiles[0.9]:
                return quantiles[0.7] + noise
            elif row['Value'] > quantiles[0.8]:
                return quantiles[0.6] + noise
        else:
            return quantiles[0.1] + noise if row['Value'] < quantiles[0.05] else quantiles[0.05] + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, quantiles, mean_value), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model with enable_categorical set to True
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0, enable_categorical=False)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())












import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor, plot_tree
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree and plot the tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group

        # Plot the Decision Tree
        plt.figure(figsize=(12, 8))
        plot_tree(dt_reg, feature_names=[cat_var], filled=True, rounded=True, fontsize=10)
        plt.title(f'Decision Tree for {cat_var}')
        plt.show()
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()









import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()
