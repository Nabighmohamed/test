import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Sample DataFrame with null values and strings
data = {
    'Feature_1': [1, 2, 3, None, 5, 6, 7, 8, 9, 10],
    'Feature_2': [1.1, 2.2, None, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', None],
    'Target': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
}
df = pd.DataFrame(data)

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Identify numerical and categorical columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Preprocessing for numerical data: impute missing values with mean
numerical_transformer = SimpleImputer(strategy='mean')

# Preprocessing for categorical data: impute missing values with constant and then one-hot encode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Define the model
model = LinearRegression()

# Create and evaluate the pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', model)])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Coefficients as feature importance
importance = pipeline.named_steps['model'].coef_
feature_names = np.concatenate([numerical_cols, pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_cols)])
for i, v in enumerate(importance):
    print(f'Feature: {feature_names[i]}, Score: {v:.5f}')










import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Sample DataFrame with null values and strings
data = {
    'Feature_1': [1, 2, 3, None, 5, 6, 7, 8, 9, 10],
    'Feature_2': [1.1, 2.2, None, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', None],
    'Target': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
}
df = pd.DataFrame(data)

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Identify numerical and categorical columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Preprocessing for numerical data: impute missing values with mean
numerical_transformer = SimpleImputer(strategy='mean')

# Preprocessing for categorical data: impute missing values with constant and then one-hot encode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Define the model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Create and evaluate the pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', model)])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Extract feature importances
importances = pipeline.named_steps['model'].feature_importances_

# Get the feature names after one-hot encoding
onehot_feature_names = pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_cols)
feature_names = np.concatenate([numerical_cols, onehot_feature_names])

# Sort the feature importances
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for i in range(len(importances)):
    print(f'{i + 1}. {feature_names[indices[i]]} ({importances[indices[i]]:.5f})')









import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.inspection import permutation_importance
import numpy as np

# Sample DataFrame with null values and strings
data = {
    'Feature_1': [1, 2, 3, None, 5, 6, 7, 8, 9, 10],
    'Feature_2': [1.1, 2.2, None, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', None],
    'Target': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
}
df = pd.DataFrame(data)

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Identify numerical and categorical columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Preprocessing for numerical data: impute missing values with mean
numerical_transformer = SimpleImputer(strategy='mean')

# Preprocessing for categorical data: impute missing values with constant and then one-hot encode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Define the model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Create and evaluate the pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', model)])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Permutation importance
results = permutation_importance(pipeline, X_test, y_test, scoring='neg_mean_squared_error')
importances = results.importances_mean

# Get the feature names after one-hot encoding
onehot_feature_names = pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_cols)
feature_names = np.concatenate([numerical_cols, onehot_feature_names])

# Sort the feature importances
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for i in range(len(importances)):
    print(f'{i + 1}.
























Bonjour Solène,

J'espère que vous allez bien. Je voulais savoir s'il y avait des préoccupations ou des protocoles de sécurité à prendre en compte si j'utilise mon ordinateur de travail ce week-end pour terminer certaines tâches professionnelles.

Merci pour vos conseils.

Cordialement,










Bonjour Solène,

J'espère que vous allez bien.

Est-ce qu'il y a des soucis de sécurité si j'utilise mon ordinateur de travail ce week-end pour finir des tâches ?

Aussi, pourriez-vous vérifier l'Excel que j'ai rempli ?

Je veux m'assurer que tout est correct, surtout pour "Unité de rattachement" et "UO3".

Merci beaucoup pour votre aide.

Bonne journée,

