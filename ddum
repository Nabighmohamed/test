Objet: Lien vers le rapport sur la valeur de RÂ² et son interprÃ©tation statistique

Bonjour SolÃ¨ne,

Je vous envoie le lien vers le rapport sur la valeur de RÂ² et son interprÃ©tation statistique que nous avons rÃ©cemment discutÃ©.

[InsÃ©rez le lien ici]

N'hÃ©sitez pas Ã  me faire savoir si vous avez des questions ou si vous avez besoin de plus d'informations.

Cordialement,















Rapport sur la Valeur de RÂ² et son InterprÃ©tation Statistique
Introduction
La valeur de RÂ², ou coefficient de dÃ©termination, est souvent mal comprise. En particulier, beaucoup de gens pensent que RÂ² ne peut Ãªtre qu'entre 0 et 1. Cependant, il peut Ãªtre nÃ©gatif dans certains cas. Ce rapport explique pourquoi et comment cela peut se produire, avec une explication dÃ©taillÃ©e des calculs de RÂ² et une illustration pratique.

Calcul de RÂ²
Pour comprendre pourquoi RÂ² peut Ãªtre nÃ©gatif, nous devons examiner comment il est calculÃ©. Nous utilisons trois variables clÃ©s dans ce calcul : RSS (Residual Sum of Squares), TSS (Total Sum of Squares), et ESS (Explained Sum of Squares).

Calcul de RSS :
Pour chaque variable indÃ©pendante 
ð‘¥
x, nous avons une variable dÃ©pendante 
ð‘¦
y. Nous traÃ§ons une ligne de rÃ©gression linÃ©aire qui prÃ©dit les valeurs de 
ð‘¦
y pour chaque valeur de 
ð‘¥
x. Appelons les valeurs prÃ©dites 
ð‘¦
^
y
^
â€‹
 . L'erreur entre ce que la ligne prÃ©dit et les valeurs rÃ©elles de 
ð‘¦
y est calculÃ©e par soustraction. Toutes ces diffÃ©rences sont mises au carrÃ© et additionnÃ©es, ce qui donne la somme des carrÃ©s des rÃ©sidus, RSS.

ð‘…
ð‘†
ð‘†
=
âˆ‘
(
ð‘¦
âˆ’
ð‘¦
^
)
2
RSS=âˆ‘(yâˆ’ 
y
^
â€‹
 ) 
2
 
Calcul de TSS :
Nous pouvons calculer la valeur moyenne de 
ð‘¦
y, appelÃ©e 
ð‘¦
Ë‰
y
Ë‰
â€‹
 . Si nous traÃ§ons 
ð‘¦
Ë‰
y
Ë‰
â€‹
 , c'est simplement une ligne horizontale Ã  travers les donnÃ©es. En soustrayant 
ð‘¦
Ë‰
y
Ë‰
â€‹
  de chaque valeur rÃ©elle de 
ð‘¦
y, nous obtenons la somme totale des carrÃ©s, TSS.

ð‘‡
ð‘†
ð‘†
=
âˆ‘
(
ð‘¦
âˆ’
ð‘¦
Ë‰
)
2
TSS=âˆ‘(yâˆ’ 
y
Ë‰
â€‹
 ) 
2
 
Calcul de ESS :
Les diffÃ©rences entre les valeurs prÃ©dites 
ð‘¦
^
y
^
â€‹
  et la valeur moyenne 
ð‘¦
Ë‰
y
Ë‰
â€‹
  sont mises au carrÃ© et additionnÃ©es. Ceci est la somme expliquÃ©e des carrÃ©s, ESS.

ð¸
ð‘†
ð‘†
=
âˆ‘
(
ð‘¦
^
âˆ’
ð‘¦
Ë‰
)
2
ESS=âˆ‘( 
y
^
â€‹
 âˆ’ 
y
Ë‰
â€‹
 ) 
2
 
Relation entre TSS, RSS et ESS
Lorsque nous avons une ligne de rÃ©gression avec une interception, la relation suivante est toujours vraie :

ð‘‡
ð‘†
ð‘†
=
ð‘…
ð‘†
ð‘†
+
ð¸
ð‘†
ð‘†
TSS=RSS+ESS
En divisant tous les termes par TSS et en rÃ©arrangeant, nous obtenons :

ð‘…
2
=
1
âˆ’
ð‘…
ð‘†
ð‘†
ð‘‡
ð‘†
ð‘†
R 
2
 =1âˆ’ 
TSS
RSS
â€‹
 
Dans ce cas, RÂ² est toujours positif.

Cas oÃ¹ RÂ² peut Ãªtre nÃ©gatif
Cependant, sans interception, la relation ci-dessus change. La formule devient :

ð‘‡
ð‘†
ð‘†
=
ð‘…
ð‘†
ð‘†
+
ð¸
ð‘†
ð‘†
+
2
âˆ‘
(
ð‘¦
âˆ’
ð‘¦
^
)
(
ð‘¦
^
âˆ’
ð‘¦
Ë‰
)
TSS=RSS+ESS+2âˆ‘(yâˆ’ 
y
^
â€‹
 )( 
y
^
â€‹
 âˆ’ 
y
Ë‰
â€‹
 )
En divisant tous les termes par TSS, nous obtenons :

ð‘…
2
=
ð¸
ð‘†
ð‘†
+
2
âˆ‘
(
ð‘¦
âˆ’
ð‘¦
^
)
(
ð‘¦
^
âˆ’
ð‘¦
Ë‰
)
ð‘‡
ð‘†
ð‘†
R 
2
 = 
TSS
ESS+2âˆ‘(yâˆ’ 
y
^
â€‹
 )( 
y
^
â€‹
 âˆ’ 
y
Ë‰
â€‹
 )
â€‹
 
Le terme supplÃ©mentaire peut rendre le numÃ©rateur nÃ©gatif, ce qui fait que RÂ² peut Ãªtre nÃ©gatif. Cela se produit lorsque la ligne horizontale 
ð‘¦
Ë‰
y
Ë‰
â€‹
  explique mieux les donnÃ©es que la ligne de rÃ©gression.

Quand le terme 
2
âˆ‘
(
ð‘¦
âˆ’
ð‘¦
^
)
(
ð‘¦
^
âˆ’
ð‘¦
Ë‰
)
2âˆ‘(yâˆ’ 
y
^
â€‹
 )( 
y
^
â€‹
 âˆ’ 
y
Ë‰
â€‹
 ) est nul ou non nul
Nul : Ce terme est nul lorsque la ligne de rÃ©gression passe par le point moyen des donnÃ©es 
(
ð‘¥
Ë‰
,
ð‘¦
Ë‰
)
( 
x
Ë‰
 , 
y
Ë‰
â€‹
 ), ce qui se produit lorsque le modÃ¨le inclut une interception. Dans ce cas, les erreurs 
(
ð‘¦
âˆ’
ð‘¦
^
)
(yâˆ’ 
y
^
â€‹
 ) et les diffÃ©rences 
(
ð‘¦
^
âˆ’
ð‘¦
Ë‰
)
( 
y
^
â€‹
 âˆ’ 
y
Ë‰
â€‹
 ) sont orthogonales, ce qui signifie que leur produit est en moyenne nul.
Non nul : Ce terme n'est pas nÃ©cessairement nul lorsque la rÃ©gression ne comprend pas d'interception. Dans ce cas, la ligne de rÃ©gression peut ne pas passer par le point moyen des donnÃ©es, et les erreurs et les diffÃ©rences peuvent avoir une covariance non nulle, conduisant ainsi Ã  un terme supplÃ©mentaire qui peut Ãªtre positif ou nÃ©gatif.
Exemple Simple
ConsidÃ©rons un exemple simple pour illustrer ce concept. Supposons que nous ayons les donnÃ©es suivantes :

x	y
1	1
2	2
3	1.3
4	3.75
5	2.25
Calculons la rÃ©gression linÃ©aire avec et sans interception :

RÃ©gression avec interception :
La ligne de rÃ©gression a une interception et la formule obtenue est :

ð‘¦
^
=
0.425
+
0.475
ð‘¥
y
^
â€‹
 =0.425+0.475x
RÃ©gression sans interception :
La ligne de rÃ©gression passe par l'origine (sans interception) et la formule obtenue est :

ð‘¦
^
=
0.63
ð‘¥
y
^
â€‹
 =0.63x
La moyenne des 
ð‘¦
y est 
ð‘¦
Ë‰
=
2.06
y
Ë‰
â€‹
 =2.06.

En traÃ§ant les lignes de rÃ©gression et la moyenne de 
ð‘¦
y sur un graphique, nous observons que la ligne de rÃ©gression sans interception ne passe pas par la moyenne des donnÃ©es, ce qui peut rendre le terme 
2
âˆ‘
(
ð‘¦
âˆ’
ð‘¦
^
)
(
ð‘¦
^
âˆ’
ð‘¦
Ë‰
)
2âˆ‘(yâˆ’ 
y
^
â€‹
 )( 
y
^
â€‹
 âˆ’ 
y
Ë‰
â€‹
 ) non nul et potentiellement nÃ©gatif, rendant ainsi 
ð‘…
2
R 
2
  nÃ©gatif.

Conclusion
La valeur de RÂ² peut Ãªtre nÃ©gative si le modÃ¨le de rÃ©gression n'a pas d'interception et s'adapte trÃ¨s mal aux donnÃ©es. Dans ce cas, une ligne horizontale pourrait fournir une meilleure explication des variations dans les donnÃ©es que le modÃ¨le lui-mÃªme. Comprendre cette nuance est crucial pour une interprÃ©tation correcte des rÃ©sultats de rÃ©gression linÃ©aire.

Visualisation avec Matplotlib
Voici un code Python qui illustre cet exemple :

python
Copy code
import matplotlib.pyplot as plt
import numpy as np

# DonnÃ©es simples
x = np.array([1, 2, 3, 4, 5])
y = np.array([1, 2, 1.3, 3.75, 2.25])

# Calcul de la rÃ©gression linÃ©aire avec interception
coeffs_with_intercept = np.polyfit(x, y, 1)
y_hat_with_intercept = np.polyval(coeffs_with_intercept, x)

# Calcul de la rÃ©gression linÃ©aire sans interception
coeffs_without_intercept = np.polyfit(x, y, 1, full=True)[0]
y_hat_without_intercept = np.polyval([coeffs_without_intercept[0]], x)

# Calcul de la moyenne de y
y_bar = np.mean(y)

# Visualisation des donnÃ©es et des lignes de rÃ©gression
plt.scatter(x, y, label='DonnÃ©es')
plt.plot(x, y_hat_with_intercept, label='RÃ©gression avec interception', color='green')
plt.plot(x, y_hat_without_intercept, label='RÃ©gression sans interception', color='red')
plt.axhline(y=y_bar, color='blue', linestyle='--', label='Moyenne de y')

plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('RÃ©gression LinÃ©aire avec et sans Interception')
plt.show()
Ce graphique montre les donnÃ©es avec deux lignes de rÃ©gression : une avec interception (en vert) et une sans interception (en rouge), ainsi qu'une ligne horizontale reprÃ©sentant la moyenne de 
ð‘¦
y (en bleu). La ligne rouge ne passe pas par le point moyen des donnÃ©es, ce qui explique pourquoi 
ð‘…
2
R 
2
  peut Ãªtre nÃ©gatif dans ce cas.





import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate data that follows the line y = x
np.random.seed(0)
X = np.linspace(-10, 10, 100).reshape(-1, 1)
y = X.flatten()  # y = x

# Fit a linear regression model with the equation y = -x
model = LinearRegression()
model.fit(X, -X)  # The model is trained with y = -x

# Predict using the fitted model
y_pred = model.predict(X)

# Compute R^2 value
r2 = r2_score(y, y_pred)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='black', label='Data: y = x')
plt.plot(X, y_pred, color='red', linewidth=2, label='Model: y = -x')
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f'Linear Regression with y = -x\nRÂ² = {r2:.2f}')
plt.legend()
plt.grid(True)
plt.show()

print(f"RÂ²: {r2:.2f}")












import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate non-linear data
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(80) * 0.5

# Fit linear model
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

# Calculate R^2
r2 = r2_score(y, y_pred)
print(f"R^2: {r2}")

# Plot
plt.scatter(X, y, color='black')
plt.plot(X, y_pred, color='blue', linewidth=3)
plt.title(f'Linear Regression on Non-linear Data\nR^2 = {r2:.2f}')
plt.show()


from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit model on training data
model.fit(X_train, y_train)
y_test_pred = model.predict(X_test)

# Calculate R^2 on test data
r2_test = r2_score(y_test, y_test_pred)
print(f"R^2 on test data: {r2_test}")
















import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
quantiles = df[df['Outlier'] == 'Inlier']['Value'].quantile([0.05, 0.1, 0.6, 0.7, 0.8, 0.9])
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()

# Replace outliers based on specified rules
def replace_outliers(row, quantiles, mean_value):
    noise = np.random.normal(0, 1)
    if row['Outlier'] == 'Outlier':
        if row['Value'] > mean_value:
            if row['Value'] > quantiles[0.9]:
                return quantiles[0.7] + noise
            elif row['Value'] > quantiles[0.8]:
                return quantiles[0.6] + noise
        else:
            return quantiles[0.1] + noise if row['Value'] < quantiles[0.05] else quantiles[0.05] + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, quantiles, mean_value), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model with enable_categorical set to True
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0, enable_categorical=False)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())












import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor, plot_tree
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree and plot the tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group

        # Plot the Decision Tree
        plt.figure(figsize=(12, 8))
        plot_tree(dt_reg, feature_names=[cat_var], filled=True, rounded=True, fontsize=10)
        plt.title(f'Decision Tree for {cat_var}')
        plt.show()
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()









import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()
