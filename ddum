import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the logistic regression model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Predict probabilities for the test set
y_pred_prob = model.predict_proba(X_test)[:, 1]

# Calculate ROC curve and ROC area
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# Plot the coefficients
plt.figure(figsize=(12, 6))
coefficients = pd.Series(model.coef_[0], index=X.columns)
coefficients.sort_values().plot(kind='bar')
plt.title('Logistic Regression Coefficients')
plt.show()

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()




For a classification machine learning problem, you'll need functions to handle data preprocessing, model training, evaluation, and possibly deployment. Below is a comprehensive list of functions that cover these aspects using popular libraries like `pandas`, `scikit-learn`, `TensorFlow`, and `PyTorch`.

### Data Handling and Preprocessing
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load Data
def load_data(file_path):
    return pd.read_csv(file_path)

# Split Data
def split_data(X, y, test_size=0.2, random_state=42):
    return train_test_split(X, y, test_size=test_size, random_state=random_state)

# Scale Features
def scale_features(X_train, X_test):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled, scaler

# Encode Labels
def encode_labels(y):
    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(y)
    return y_encoded, encoder
```

### Model Training and Evaluation with Scikit-Learn
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Train Model
def train_model(X_train, y_train):
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    return model

# Evaluate Model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    matrix = confusion_matrix(y_test, y_pred)
    return accuracy, report, matrix

# Display Evaluation
def display_evaluation(accuracy, report, matrix):
    print(f"Accuracy: {accuracy}")
    print("Classification Report:\n", report)
    print("Confusion Matrix:\n", matrix)
```

### Model Training and Evaluation with TensorFlow/Keras
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# Build Keras Model
def build_keras_model(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_shape,)),
        Dense(64, activation='relu'),
        Dense(10, activation='softmax')  # Adjust output layer for your classification problem
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Train Keras Model
def train_keras_model(model, X_train, y_train, epochs=10, batch_size=32):
    y_train_categorical = to_categorical(y_train)
    history = model.fit(X_train, y_train_categorical, epochs=epochs, batch_size=batch_size, validation_split=0.2)
    return history

# Evaluate Keras Model
def evaluate_keras_model(model, X_test, y_test):
    y_test_categorical = to_categorical(y_test)
    loss, accuracy = model.evaluate(X_test, y_test_categorical)
    return loss, accuracy
```

### Model Training and Evaluation with PyTorch
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# PyTorch Model Definition
class SimpleNN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Train PyTorch Model
def train_pytorch_model(model, train_loader, criterion, optimizer, num_epochs=10):
    for epoch in range(num_epochs):
        for data, labels in train_loader:
            outputs = model(data)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    return model

# Evaluate PyTorch Model
def evaluate_pytorch_model(model, test_loader, criterion):
    model.eval()
    with torch.no_grad():
        total_loss = 0
        correct = 0
        total = 0
        for data, labels in test_loader:
            outputs = model(data)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        accuracy = 100 * correct / total
    return total_loss / len(test_loader), accuracy
```

### Full Workflow Example
Hereâ€™s a full example that uses the functions defined above to load data, preprocess it, train a model, and evaluate it using scikit-learn.

```python
def main(file_path):
    # Load and preprocess data
    data = load_data(file_path)
    X = data.drop('target', axis=1)  # Adjust 'target' to your label column
    y = data['target']
    
    X_train, X_test, y_train, y_test = split_data(X, y)
    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)
    y_train_encoded, encoder = encode_labels(y_train)
    y_test_encoded, _ = encode_labels(y_test)
    
    # Train and evaluate model
    model = train_model(X_train_scaled, y_train_encoded)
    accuracy, report, matrix = evaluate_model(model, X_test_scaled, y_test_encoded)
    
    # Display evaluation results
    display_evaluation(accuracy, report, matrix)

if __name__ == "__main__":
    file_path = 'your_data.csv'  # Path to your dataset
    main(file_path)
```

### Additional Utility Functions

#### Hyperparameter Tuning (Using GridSearchCV)
```python
from sklearn.model_selection import GridSearchCV

def hyperparameter_tuning(X_train, y_train):
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10]
    }
    model = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)
    return grid_search.best_estimator_, grid_search.best_params_
```

#### Save and Load Model
```python
import joblib

# Save Model
def save_model(model, filename):
    joblib.dump(model, filename)

# Load Model
def load_model(filename):
    return joblib.load(filename)
```

These functions should provide a comprehensive set of tools to handle any situation related to classification and machine learning problems. Adjust the functions as needed to fit the specific requirements of your task.










import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils import resample
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
from scipy import stats
import statsmodels.api as sm

# Load Data
def load_data(file_path):
    data = pd.read_csv(file_path)
    print(f"Data Loaded: {data.shape[0]} rows, {data.shape[1]} columns")
    return data

# Initial Data Exploration
def initial_exploration(data):
    print("Data Types:\n", data.dtypes)
    print("\nMissing Values:\n", data.isnull().sum())
    print("\nDescriptive Statistics:\n", data.describe())
    print("\nClass Distribution:\n", data['target'].value_counts())

### 2. Data Cleaning and Preprocessing
def preprocess_data(data):
    # Handle missing values
    data.fillna(data.median(), inplace=True)
    
    # Encode categorical variables
    for column in data.select_dtypes(include=['object']).columns:
        data[column] = LabelEncoder().fit_transform(data[column])
    
    return data

### 3. Exploratory Data Analysis (EDA)
def eda(data):
    # Univariate Analysis
    for column in data.columns:
        plt.figure(figsize=(10, 5))
        sns.histplot(data[column], kde=True)
        plt.title(f'Univariate Analysis of {column}')
        plt.show()
    
    # Multivariate Analysis
    sns.pairplot(data)
    plt.title('Multivariate Analysis')
    plt.show()

### 4. Handle Imbalanced Data
def handle_imbalanced_data(X, y):
    smote = SMOTE()
    X_res, y_res = smote.fit_resample(X, y)
    return X_res, y_res

### 5. Feature Engineering
def feature_engineering(data):
    # Example: Creating interaction terms
    data['interaction'] = data['feature1'] * data['feature2']
    return data

### 6. Model Building and Evaluation
def build_and_evaluate_models(X, y):
    classifiers = [
        ('Logistic Regression', LogisticRegression()),
        ('Decision Tree', DecisionTreeClassifier()),
        ('Random Forest', RandomForestClassifier()),
        ('SVC', SVC()),
        ('Naive Bayes', GaussianNB()),
        ('KNN', KNeighborsClassifier()),
        ('Gradient Boosting', GradientBoostingClassifier())
    ]
    
    for name, clf in classifiers:
        pipeline = ImbPipeline([
            ('scaler', StandardScaler()),
            ('smote', SMOTE()),
            ('classifier', clf)
        ])
        scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')
        print(f'{name} Accuracy: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})')

### 7. Model Interpretation
def model_interpretation(model, X):
    feature_importances = pd.DataFrame(model.feature_importances_,
                                       index = X.columns,
                                       columns=['importance']).sort_values('importance', ascending=False)
    print("Feature Importances:\n", feature_importances)

### 8. Anomaly Detection
def perform_stat_tests(X, y):
    print("Performing statistical tests:")
    model = sm.OLS(y, sm.add_constant(X)).fit()
    influence = model.get_influence()
    cooks = influence.cooks_distance[0]
    plt.stem(np.arange(len(cooks)), cooks, markerfmt=",")
    plt.title("Cook's Distance")
    plt.show()
    
    dw = sm.stats.stattools.durbin_watson(model.resid)
    print(f'Durbin-Watson statistic: {dw}')
    
    z_scores = stats.zscore(X)
    abs_z_scores = np.abs(z_scores)
    anomaly_indices = np.where(abs_z_scores > 3)[0]
    print(f'Anomalies detected at indices: {anomaly_indices}')

### Full Workflow
def main(file_path):
    # Step 1: Load and Explore Data
    data = load_data(file_path)
    initial_exploration(data)
    
    # Step 2: Preprocess Data
    data = preprocess_data(data)
    
    # Step 3: Exploratory Data Analysis
    eda(data)
    
    # Step 4: Handle Imbalanced Data
    X = data.drop('target', axis=1)  # Adjust 'target' to your label column
    y = data['target']
    X, y = handle_imbalanced_data(X, y)
    
    # Step 5: Feature Engineering
    X = feature_engineering(X)
    
    # Step 6: Model Building and Evaluation
    build_and_evaluate_models(X, y)
    
    # Step 7: Model Interpretation (Example with RandomForest)
    final_model = RandomForestClassifier(random_state=42)
    final_model.fit(X, y)
    model_interpretation(final_model, X)
    
    # Step 8: Anomaly Detection
    perform_stat_tests(X, y)

if __name__ == "__main__":
    file_path = 'your_data.csv'  # Path to your dataset
    main(file_path)



pip install pandas numpy openpyxl requests beautifulsoup4 flask django fastapi jinja2 sqlalchemy scikit-learn tensorflow keras torch matplotlib seaborn scipy nltk spacy textblob selenium pyautogui click argparse plotly bokeh twisted pymongo pytest tqdm pyyaml configparser












import os

packages = [
    # Data Handling and Analysis
    "pandas",
    "numpy",
    "openpyxl",
    "requests",
    "beautifulsoup4",
    
    # Web Development
    "flask",
    "django",
    "fastapi",
    "jinja2",
    "sqlalchemy",
    
    # Machine Learning and Data Science
    "scikit-learn",
    "tensorflow",
    "keras",
    "torch",  # PyTorch
    "matplotlib",
    "seaborn",
    "scipy",
    
    # Natural Language Processing
    "nltk",
    "spacy",
    "textblob",
    
    # Automation
    "selenium",
    "pyautogui",
    
    # Utilities
    "logging",
    "os",
    "datetime",
    "click",
    "argparse",
    
    # Visualization
    "plotly",
    "bokeh",
    
    # Networking
    "socket",
    "twisted",
    
    # Database
    "sqlite3",
    "pymongo",
    
    # Testing
    "unittest",
    "pytest",
    
    # Deployment
    "docker",
    "heroku",
    
    # Others
    "tqdm",
    "pyyaml",
    "configparser"
]

for package in packages:
    os.system(f"pip install {package}")
python install_packages.py












Contexte :
Je rencontre un problÃ¨me avec l'installation d'un package via pip sur Domino. Lorsque j'exÃ©cute la commande pip install pour un package spÃ©cifique, l'installation semble se dÃ©rouler correctement et le package est tÃ©lÃ©chargÃ© avec succÃ¨s. Cependant, lorsque j'essaie d'importer ce package dans mon code, je rencontre un problÃ¨me.
J'ai essayÃ© de redÃ©marrer le kernel aprÃ¨s l'installation, mais cela n'a pas rÃ©solu le problÃ¨me.














# Convert the 'value' column to numeric, setting errors='coerce' to handle non-numeric values
df['value'] = pd.to_numeric(df['value'], errors='coerce')

# Fill NaN values with a specific value, e.g., 0
df['value'] = df['value'].fillna(0).astype(int)

print("\nDataFrame after converting 'value' to int:")
print(df)











import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'id': [2, 4, 5, 6, 7],
    'value2': ['v', 'w', 'x', 'y', 'z']
})

# Inner Join
inner_merged_df = pd.merge(df1, df2, on='id', how='inner')
print("Inner Merge:")
print(inner_merged_df)

# Left Join
left_merged_df = pd.merge(df1, df2, on='id', how='left')
print("\nLeft Merge:")
print(left_merged_df)

# Right Join
right_merged_df = pd.merge(df1, df2, on='id', how='right')
print("\nRight Merge:")
print(right_merged_df)

# Outer Join
outer_merged_df = pd.merge(df1, df2, on='id', how='outer')
print("\nOuter Merge:")
print(outer_merged_df)











import pandas as pd

# Sample data for testing
og = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

df1 = pd.DataFrame({
    'id': [1, 2, 6],
    'pnb': [100, 200, 300]
})

df2 = pd.DataFrame({
    'id': [2, 3, 7],
    'pnb': [150, 250, 350]
})

df3 = pd.DataFrame({
    'id': [3, 4, 8],
    'pnb': [200, 300, 400]
})

df4 = pd.DataFrame({
    'id': [4, 5, 9],
    'pnb': [250, 350, 450]
})

df5 = pd.DataFrame({
    'id': [5, 1, 10],
    'pnb': [300, 400, 500]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Initialize a DataFrame to store the results
result_df = og[['id']].copy()
result_df['total_pnb'] = 0

# Function to filter dataframes and sum the pnb values for each id in the original df
def calculate_pnb_for_each_id(og_df, dataframes, key='id', target_col='pnb'):
    # Get the unique ids from the og dataframe
    og_ids = set(og_df[key].unique())
    
    # Initialize a dictionary to store the total pnb for each id
    total_pnb_dict = {id: 0 for id in og_ids}
    
    # Iterate through each dataframe
    for df in dataframes:
        # Filter the dataframe to only include rows where the id is in the og dataframe
        filtered_df = df[df[key].isin(og_ids)]
        
        # Sum the pnb column for the filtered dataframe by id and add it to the total_pnb_dict
        for id, pnb_sum in filtered_df.groupby(key)[target_col].sum().items():
            total_pnb_dict[id] += pnb_sum
    
    return total_pnb_dict

# Calculate the pnb sums for each id
total_pnb_dict = calculate_pnb_for_each_id(og, dataframes)

# Update the result_df with the calculated pnb sums
result_df['total_pnb'] = result_df['id'].map(total_pnb_dict)

print(result_df)
















import pandas as pd

# Sample data for testing
og = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

df1 = pd.DataFrame({
    'id': [1, 2, 6],
    'pnb': [100, 200, 300]
})

df2 = pd.DataFrame({
    'id': [2, 3, 7],
    'pnb': [150, 250, 350]
})

df3 = pd.DataFrame({
    'id': [3, 4, 8],
    'pnb': [200, 300, 400]
})

df4 = pd.DataFrame({
    'id': [4, 5, 9],
    'pnb': [250, 350, 450]
})

df5 = pd.DataFrame({
    'id': [5, 1, 10],
    'pnb': [300, 400, 500]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Function to filter dataframes and sum the pnb values
def sum_pnb_for_og_ids(og_df, dataframes, key='id', target_col='pnb'):
    # Get the unique ids from the og dataframe
    og_ids = set(og_df[key].unique())
    
    # Initialize the total sum
    total_sum_pnb = 0
    
    # Iterate through each dataframe
    for df in dataframes:
        # Filter the dataframe to only include rows where the id is in the og dataframe
        filtered_df = df[df[key].isin(og_ids)]
        
        # Sum the pnb column for the filtered dataframe
        sum_pnb = filtered_df[target_col].sum()
        
        # Add the sum to the total sum
        total_sum_pnb += sum_pnb
    
    return total_sum_pnb

# Calculate the target variable
target_variable = sum_pnb_for_og_ids(og, dataframes)

print("Target Variable (Total pnb):", target_variable)










Step 2: Robust Feature Selection
Feature Selection Method:
Instead of traditional feature importance metrics, this approach uses error reduction on cross-validated datasets.
Error Reduction Illustration:
The error reduction is observed by removing certain features and evaluating the resulting error on both train and hold-out sets.
This is done for different age groups and contract lengths.
Error Reduction Metrics:
For example, removing a certain age range might reduce the error on the training set by 0.05 and on the hold-out set by 0.02 when using XGBoost.
The feature importance is estimated based on how much error reduction a feature provides.
Comparison Across Models:
Features are compared across models like GBM (Gradient Boosting Machines) and XGBoost to evaluate their importance.
Step 3: Gradient Boosting with Only Tree Stump & Transform to Linear Output
Tree Stump Gradient Boosting:
The model uses decision trees of depth 1 (tree stumps) to perform gradient boosting.
Each tree considers only one feature, one threshold, and one cut at each iteration.
Predictive Relationship:
The goal is to find predictive relationships between features and the target variable.
Example:
Customers are split based on age and revenue, which helps in categorizing risk levels.
Linearization:
The summed trees are then transformed into a linear function format similar to logistic regression.
This is achieved by summing the impacts of each decision tree split to create a corresponding coefficient table.
Step 4: Identify and Add the Most Important Interaction Pairs
Refinement with Bivariate Effects:
The model identifies important interaction pairs to refine predictive power.
Temporary Level 2 Trees:
These trees are used to identify the most important interactions.
Interaction Pairs:
Interaction pairs are determined based on cross-validated errors.
Implementation:
These interaction pairs are then added sequentially to the model using XGBoost, starting from the most important pairs until no further improvement is seen.
Example Interaction Pairs:
Var2 & Var6 (97.83% importance), Var13 & Var14 (0.87% importance), etc.
Summary
Feature Selection: Important features are identified based on their contribution to error reduction.
Model Building: The model uses tree stumps for gradient boosting, transforming the results into a linear output.
Interaction Refinement: Important interaction pairs are added to enhance the modelâ€™s predictive power.
This workflow combines robust feature selection with simplified tree-based gradient boosting and interaction pair refinement to create an accurate predictive model.














Rapport sur la Perte de Clients Professionnels (CCLIKPI)
Introduction:
Ce rapport prÃ©sente les statistiques dÃ©taillÃ©es sur le nombre de clients professionnels perdus par annÃ©e, en se concentrant sur deux axes principaux. Le premier axe examine la perte totale de clients professionnels de 2018 Ã  2023. Le deuxiÃ¨me axe se concentre sur une Ã©tude de cas spÃ©cifique, suivant la perte de clients ayant une anciennetÃ© de 2 mois Ã  partir de 2018.

1. Axe 1 : Perte Totale de Clients Professionnels de 2018 Ã  2023

Perte Globale de Clients
Cette section fournit une vue d'ensemble de la perte totale de clients professionnels au fil des ans. Les chiffres montrent l'Ã©volution annuelle des pertes, ce qui permet d'identifier les tendances et les annÃ©es avec des fluctuations significatives.

AnnÃ©e	Nombre total de clients perdus	Pourcentage de perte
2019	43082	10.91%
2020	37769	9.42%
2021	34718	8.52%
2022	38501	9.08%
2023	45615	10.36%
Observations :

En 2023, le nombre total de clients perdus a augmentÃ©, atteignant 45615, avec un taux de perte de 10.36%.
Les pertes annuelles de clients montrent une lÃ©gÃ¨re fluctuation mais une tendance globale stable avec une lÃ©gÃ¨re hausse en 2023.
Perte de Clients pour les Entreprises (CNOUVSEG=ER)
Cette sous-section se concentre spÃ©cifiquement sur les pertes de clients parmi les entreprises (CNOUVSEG=ER). Les donnÃ©es permettent de comprendre comment ce segment particulier a Ã©voluÃ© par rapport aux autres.

AnnÃ©e	Nombre de clients perdus	Pourcentage de perte
2019	8110	12.27%
2020	5831	8.87%
2021	5196	7.80%
2022	6216	9.04%
2023	7799	11.17%
Observations :

Pour les entreprises, les pertes ont diminuÃ© en 2020 et 2021, puis augmentÃ© en 2022 et 2023, atteignant 11.17% en 2023.
La perte annuelle la plus faible Ã©tait en 2021 avec 7.80%.
Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL)
Cette section examine la perte de clients pour les autres types de clients professionnels (CNOUVSEG=AC AG PL), offrant une vue comparative avec le segment des entreprises.

AnnÃ©e	Nombre de clients perdus	Pourcentage de perte
2019	36982	11.18%
2020	33854	10.05%
2021	31239	9.13%
2022	34118	8.59%
2023	40085	10.94%
Observations :

La perte de clients parmi les autres clients professionnels montre une tendance similaire Ã  celle des entreprises, avec une baisse en 2020 et 2021 et une remontÃ©e en 2022 et 2023.
En 2023, la perte a augmentÃ© Ã  10.94%, indiquant une tendance inquiÃ©tante.
2. Axe 2 : Perte de Clients Professionnels dans le Cadre de l'Ã‰tude de Cas

Perte de Clients Globale dans le Cadre de l'Ã‰tude de Cas
AnnÃ©e	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	40109	10.78%
2021	29384	3413	40109	8.51%
2022	26275	3109	40109	7.75%
2023	20360	3189	40109	7.95%
Observations :

En 2023, il restait 20360 clients sur les 40109 initiaux, ce qui reprÃ©sente une perte de 7.95% des clients initiaux.
Les pertes annuelles ont montrÃ© une augmentation progressive, soulignant la nÃ©cessitÃ© de stratÃ©gies de rÃ©tention plus efficaces.
Perte de Clients pour les Entreprises (CNOUVSEG=ER) dans le Cadre de l'Ã‰tude de Cas
Cette section suit un groupe spÃ©cifique de clients entreprises ayant une anciennetÃ© de 2 mois Ã  partir de 2018, observant leur fidÃ©litÃ© au fil des ans.

AnnÃ©e	Clients restants ER	Clients perdus ER	Total de clients initiaux
2019	4372	440	4812
2020	3970	402	4812
2021	3679	291	4812
2022	3365	314	4812
2023	3049	316	4812
Observations :

En 2023, il restait 3049 clients entreprises sur les 4812 initiaux, ce qui reprÃ©sente une perte de 316 clients en 2023.
Les pertes annuelles ont montrÃ© une augmentation progressive mais faible, soulignant la nÃ©cessitÃ© de stratÃ©gies de rÃ©tention plus efficaces.
Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL) dans le Cadre de l'Ã‰tude de Cas
Cette section suit un groupe spÃ©cifique d'autres clients professionnels ayant une anciennetÃ© de 2 mois Ã  partir de 2018, observant leur fidÃ©litÃ© au fil des ans.

AnnÃ©e	Clients restants AC AG PL	Clients perdus AC AG PL	Total de clients initiaux
2019	32749	2550	35299
2020	28862	3887	35299
2021	25792	3070	35299
2022	23083	2709	35299
2023	20293	2790	35299
Observations :

En 2023, il restait 20293 autres clients professionnels sur les 35299 initiaux, ce qui reprÃ©sente une perte de 2790 clients en 2023.
Les pertes annuelles ont montrÃ© une tendance fluctuante mais globalement Ã©levÃ©e, soulignant la nÃ©cessitÃ© de stratÃ©gies de rÃ©tention spÃ©cifiques pour ce segment.
Conclusion :
Les donnÃ©es montrent une tendance inquiÃ©tante de perte de clients dans les deux segments de clients professionnels. Les entreprises et les autres clients professionnels voient tous deux une augmentation progressive des pertes de clients au fil des annÃ©es, avec des pourcentages de perte annuels relativement Ã©levÃ©s. En particulier, l'Ã©tude de cas a rÃ©vÃ©lÃ© une perte notable des clients initiaux d'ici 2023. Cette analyse souligne la nÃ©cessitÃ© de stratÃ©gies de rÃ©tention client plus efficaces pour inverser cette tendance.

Pour des analyses plus dÃ©taillÃ©es et interactives, veuillez consulter le tableau de bord Power BI. Vous y trouverez des graphiques intÃ©ressants tels que la perte de clients par mois et par annÃ©e, le pourcentage de clients perdus selon le segment CNOUVSEG, ainsi que des sections dÃ©taillÃ©es pour chaque catÃ©gorie de clients.















Pour des analyses plus dÃ©taillÃ©es et interactives, veuillez consulter le tableau de bord Power BI. Vous y trouverez des graphiques intÃ©ressants tels que la perte de clients par mois et par annÃ©e, le pourcentage de clients perdus selon le segment CNOUVSEG, ainsi que des sections dÃ©taillÃ©es pour chaque catÃ©gorie de clients.




AnnÃ©e	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	40109	10.78%
2021	29384	3413	40109	8.51%
2022	26275	3109	40109	7.75%
2023	20360	3189	40109	7.95%




import matplotlib.pyplot as plt
import pandas as pd

# Data for the plots
data_global = {
    'AnnÃ©e': [2019, 2020, 2021, 2022, 2023],
    'Nombre total de clients perdus': [43082, 37769, 34718, 38501, 45615],
    'Pourcentage de perte': [0.1091, 0.0942, 0.0852, 0.0908, 0.1036]
}

data_er = {
    'AnnÃ©e': [2019, 2020, 2021, 2022, 2023],
    'Nombre de clients perdus': [8110, 5831, 5196, 6216, 7799],
    'Pourcentage de perte': [0.1227, 0.0887, 0.0779, 0.0904, 0.1117]
}

data_ac_ag_pl = {
    'AnnÃ©e': [2019, 2020, 2021, 2022, 2023],
    'Nombre de clients perdus': [36982, 33854, 31239, 34118, 40085],
    'Pourcentage de perte': [0.1118, 0.1005, 0.0913, 0.0859, 0.1094]
}

data_case_study = {
    'AnnÃ©e': [2019, 2020, 2021, 2022, 2023],
    'Clients restants': [37119, 32797, 29384, 26275, 20360],
    'Clients perdus': [2990, 4322, 3413, 3109, 3189],
    'Pourcentage de perte': [0.0745, 0.1164, 0.1041, 0.1058, 0.1213]
}

# Creating DataFrames
df_global = pd.DataFrame(data_global)
df_er = pd.DataFrame(data_er)
df_ac_ag_pl = pd.DataFrame(data_ac_ag_pl)
df_case_study = pd.DataFrame(data_case_study)

# Plotting
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Global losses plot
axes[0, 0].plot(df_global['AnnÃ©e'], df_global['Nombre total de clients perdus'], marker='o', linestyle='-')
axes[0, 0].set_title('Perte Globale de Clients')
axes[0, 0].set_xlabel('AnnÃ©e')
axes[0, 0].set_ylabel('Nombre total de clients perdus')

# ER losses plot
axes[0, 1].plot(df_er['AnnÃ©e'], df_er['Nombre de clients perdus'], marker='o', linestyle='-', color='orange')
axes[0, 1].set_title('Perte de Clients pour les Entreprises (ER)')
axes[0, 1].set_xlabel('AnnÃ©e')
axes[0, 1].set_ylabel('Nombre de clients perdus')

# AC AG PL losses plot
axes[1, 0].plot(df_ac_ag_pl['AnnÃ©e'], df_ac_ag_pl['Nombre de clients perdus'], marker='o', linestyle='-', color='green')
axes[1, 0].set_title('Perte de Clients pour les Autres Professionnels (AC AG PL)')
axes[1, 0].set_xlabel('AnnÃ©e')
axes[1, 0].set_ylabel('Nombre de clients perdus')

# Case study losses plot
axes[1, 1].plot(df_case_study['AnnÃ©e'], df_case_study['Clients perdus'], marker='o', linestyle='-', color='red')
axes[1, 1].set_title('Perte de Clients dans l\'Ã‰tude de Cas')
axes[1, 1].set_xlabel('AnnÃ©e')
axes[1, 1].set_ylabel('Nombre de clients perdus')

plt.tight_layout()
plt.show()








Rapport sur la Perte de Clients Professionnels (CCLIKPI)
Introduction:
Ce rapport prÃ©sente les statistiques dÃ©taillÃ©es sur le nombre de clients professionnels perdus par annÃ©e, en se concentrant sur deux axes principaux. Le premier axe examine la perte totale de clients professionnels de 2018 Ã  2023. Le deuxiÃ¨me axe se concentre sur une Ã©tude de cas spÃ©cifique, suivant la perte de clients ayant une anciennetÃ© de 2 mois Ã  partir de 2018.

1. Axe 1 : Perte Totale de Clients Professionnels de 2018 Ã  2023

Perte Globale de Clients
Cette section fournit une vue d'ensemble de la perte totale de clients professionnels au fil des ans. Les chiffres montrent l'Ã©volution annuelle des pertes, ce qui permet d'identifier les tendances et les annÃ©es avec des fluctuations significatives.

AnnÃ©e	Nombre total de clients perdus	Pourcentage de perte
2019	43082	10.91%
2020	37769	9.42%
2021	34718	8.52%
2022	38501	9.08%
2023	45615	10.36%
Observations :

En 2023, le nombre total de clients perdus a augmentÃ©, atteignant 45615, avec un taux de perte de 10.36%.
Les pertes annuelles de clients montrent une lÃ©gÃ¨re fluctuation mais une tendance globale stable avec une lÃ©gÃ¨re hausse en 2023.
Perte de Clients pour les Entreprises (CNOUVSEG=ER)
Cette sous-section se concentre spÃ©cifiquement sur les pertes de clients parmi les entreprises (CNOUVSEG=ER). Les donnÃ©es permettent de comprendre comment ce segment particulier a Ã©voluÃ© par rapport aux autres.

AnnÃ©e	Nombre de clients perdus	Pourcentage de perte
2019	8110	12.27%
2020	5831	8.87%
2021	5196	7.80%
2022	6216	9.04%
2023	7799	11.17%
Observations :

Pour les entreprises, les pertes ont diminuÃ© en 2020 et 2021, puis augmentÃ© en 2022 et 2023, atteignant 11.17% en 2023.
La perte annuelle la plus faible Ã©tait en 2021 avec 7.80%.
Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL)
Cette section examine la perte de clients pour les autres types de clients professionnels (CNOUVSEG=AC AG PL), offrant une vue comparative avec le segment des entreprises.

AnnÃ©e	Nombre de clients perdus	Pourcentage de perte
2019	36982	11.18%
2020	33854	10.05%
2021	31239	9.13%
2022	34118	8.59%
2023	40085	10.94%
Observations :

La perte de clients parmi les autres clients professionnels montre une tendance similaire Ã  celle des entreprises, avec une baisse en 2020 et 2021 et une remontÃ©e en 2022 et 2023.
En 2023, la perte a augmentÃ© Ã  10.94%, indiquant une tendance inquiÃ©tante.
2. Axe 2 : Perte de Clients Professionnels dans le Cadre de l'Ã‰tude de Cas

Cette section suit un groupe spÃ©cifique de clients ayant une anciennetÃ© de 2 mois Ã  partir de 2018, observant leur fidÃ©litÃ© au fil des ans. Cette approche permet de voir comment ces clients spÃ©cifiques ont rÃ©agi sur une pÃ©riode prolongÃ©e.

AnnÃ©e	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	37119	11.64%
2021	29384	3413	32797	10.41%
2022	26275	3109	29384	10.58%
2023	20360	3189	26275	12.13%
Observations :

En 2023, il restait 20360 clients sur les 40109 initiaux, ce qui reprÃ©sente une perte de 42% des clients initiaux.
Les pertes annuelles ont montrÃ© une augmentation progressive, soulignant la nÃ©cessitÃ© de stratÃ©gies de rÃ©tention plus efficaces.
Conclusion :
Les donnÃ©es montrent une tendance inquiÃ©tante de perte de clients dans les deux segments de clients professionnels. Les entreprises et les autres clients professionnels voient tous deux une augmentation progressive des pertes de clients au fil des annÃ©es, avec des pourcentages de perte annuels relativement Ã©levÃ©s. En particulier, l'Ã©tude de cas a rÃ©vÃ©lÃ© une perte de 42% des clients initiaux d'ici 2023. Cette analyse souligne la nÃ©cessitÃ© de stratÃ©gies de rÃ©tention client plus efficaces pour inverser cette tendance.

Pour des analyses plus dÃ©taillÃ©es et interactives, veuillez consulter le tableau de bord Power BI.

Cordialement,


























Rapport sur la Perte de Clients Professionnels (CCLIKPI)
Introduction:
Ce rapport prÃ©sente une analyse dÃ©taillÃ©e de la perte de clients professionnels (CCLIKPI) de 2018 Ã  2023, divisÃ©e en deux axes principaux pour mieux comprendre les tendances et les causes des pertes.

1. Axe 1 : Perte Totale de Clients Professionnels de 2018 Ã  2023

Perte Globale de Clients
Cette section fournit une vue d'ensemble de la perte totale de clients professionnels au fil des ans. Les chiffres montrent l'Ã©volution annuelle des pertes, ce qui permet d'identifier les tendances et les annÃ©es avec des fluctuations significatives.

AnnÃ©e	Nombre total de clients perdus	Pourcentage de perte
2019	43082	10.91%
2020	37769	9.42%
2021	34718	8.52%
2022	38501	9.08%
2023	45615	10.36%
Nous observons que le nombre total de clients perdus a variÃ© chaque annÃ©e, avec une augmentation notable en 2023, atteignant 10.36%. Ces donnÃ©es montrent l'importance de surveiller continuellement les tendances de perte de clients pour identifier les causes sous-jacentes et ajuster les stratÃ©gies de rÃ©tention.

Perte de Clients pour les Entreprises (CNOUVSEG=ER)
Cette sous-section se concentre spÃ©cifiquement sur les pertes de clients parmi les entreprises (CNOUVSEG=ER). Les donnÃ©es permettent de comprendre comment ce segment particulier a Ã©voluÃ© par rapport aux autres.

AnnÃ©e	Nombre de clients perdus	Pourcentage de perte
2019	8110	12.27%
2020	5831	8.87%
2021	5196	7.80%
2022	6216	9.04%
2023	7799	11.17%
Pour les entreprises, nous notons une baisse des pertes en 2020 et 2021, suivie d'une augmentation en 2022 et 2023. Ces fluctuations peuvent Ãªtre dues Ã  divers facteurs Ã©conomiques ou Ã  des changements dans les offres de services.

Perte de Clients pour les Autres Clients Professionnels (CNOUVSEG=AC AG PL)
Cette section examine la perte de clients pour les autres types de clients professionnels (CNOUVSEG=AC AG PL), offrant une vue comparative avec le segment des entreprises.

AnnÃ©e	Nombre de clients perdus	Pourcentage de perte
2019	36982	11.18%
2020	33854	10.05%
2021	31239	9.13%
2022	34118	8.59%
2023	40085	10.94%
La perte de clients parmi les autres clients professionnels montre une tendance similaire Ã  celle des entreprises, avec une baisse en 2020 et 2021 et une remontÃ©e en 2022 et 2023. Ces donnÃ©es peuvent aider Ã  cibler des interventions spÃ©cifiques pour chaque segment.

2. Axe 2 : Perte de Clients Professionnels dans le Cadre de l'Ã‰tude de Cas

Cette section suit un groupe spÃ©cifique de clients ayant une anciennetÃ© de 2 mois Ã  partir de 2018, observant leur fidÃ©litÃ© au fil des ans. Cette approche permet de voir comment ces clients spÃ©cifiques ont rÃ©agi sur une pÃ©riode prolongÃ©e.

AnnÃ©e	Clients restants	Clients perdus	Total de clients initiaux	Pourcentage de perte
2019	37119	2990	40109	7.45%
2020	32797	4322	37119	11.64%
2021	29384	3413	32797	10.41%
2022	26275	3109	29384	10.58%
2023	20360	3189	26275	12.13%
Nous observons que, dans cette Ã©tude de cas, 42% des clients initiaux ont Ã©tÃ© perdus d'ici 2023. Ces chiffres montrent une perte progressive et significative de ce groupe de clients, soulignant l'importance de stratÃ©gies de rÃ©tention ciblÃ©es pour maintenir la fidÃ©litÃ© des clients sur le long terme.

Conclusion :
Environ 42 % des clients initiaux ont Ã©tÃ© perdus dans le cadre de notre Ã©tude de cas d'ici 2023. Ces analyses montrent l'importance de surveiller et de comprendre les tendances de perte de clients pour mieux adapter les stratÃ©gies de rÃ©tention et amÃ©liorer la satisfaction et la fidÃ©litÃ© des clients.

Pour des analyses plus dÃ©taillÃ©es et interactives, veuillez consulter le tableau de bord Power BI.

























import pandas as pd

def summarize_key_existence(original_df, key, stop_dfs):
    summaries = []
    current_df = original_df.copy()

    for i, stop_df in enumerate(stop_dfs):
        unique_keys_original = set(current_df[key].unique())
        unique_keys_stop = set(stop_df[key].unique())

        # Find keys that exist in both dataframes
        keys_exist = unique_keys_original & unique_keys_stop
        
        # Find keys that are in the original dataframe but not in the current stop dataframe
        keys_missing = unique_keys_original - unique_keys_stop

        # Update the current_df to only keep keys that exist in the current stop dataframe
        current_df = current_df[current_df[key].isin(keys_exist)]

        # Summarize the results
        summary = {
            'stop_number': i + 1,
            'keys_exist': len(keys_exist),
            'keys_missing': len(keys_missing),
            'total_keys_left': len(current_df)
        }
        summaries.append(summary)

    return summaries

# Sample dataframes for testing
original_df = pd.DataFrame({
    'key': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

df1 = pd.DataFrame({
    'key': [2, 4, 6, 8, 10],
    'value': ['b', 'd', 'f', 'h', 'j']
})

df2 = pd.DataFrame({
    'key': [2, 3, 6, 7],
    'value': ['b', 'c', 'f', 'g']
})

df3 = pd.DataFrame({
    'key': [1, 3, 5, 7],
    'value': ['a', 'c', 'e', 'g']
})

df4 = pd.DataFrame({
    'key': [1, 2, 3, 9],
    'value': ['a', 'b', 'c', 'i']
})

df5 = pd.DataFrame({
    'key': [1, 5, 9],
    'value': ['a', 'e', 'i']
})

# List of stop dataframes
stop_dfs = [df1, df2, df3, df4, df5]

# Specify the key column
key_column = 'key'

# Get the summaries
summaries = summarize_key_existence(original_df, key_column, stop_dfs)

for summary in summaries:
    print(f"Stop {summary['stop_number']}: Keys exist = {summary['keys_exist']}, Keys missing = {summary['keys_missing']}, Total keys left = {summary['total_keys_left']}")


















import pandas as pd

def check_key_existence(df1, df2, key):
    # Get unique values of the key from the first dataframe
    unique_keys_df1 = set(df1[key].unique())
    
    # Get unique values of the key from the second dataframe
    unique_keys_df2 = set(df2[key].unique())
    
    # Find keys that exist in both dataframes
    keys_exist = unique_keys_df1 & unique_keys_df2
    
    # Find keys that are in the first dataframe but not in the second
    keys_missing = unique_keys_df1 - unique_keys_df2
    
    result = {
        'total_keys_df1': len(unique_keys_df1),
        'keys_exist_in_both': len(keys_exist),
        'keys_missing_in_df2': len(keys_missing)
    }
    
    return result

# Sample dataframes for testing
df1 = pd.DataFrame({
    'key': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'key': [2, 4, 5, 6],
    'value': ['b', 'd', 'e', 'f']
})

# Specify the key column
key_column = 'key'

# Get the result
result = check_key_existence(df1, df2, key_column)

print(result)



















import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'cclikpi': [3, 4, 3, 4, 5],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'cclikpi': [4, 5, 4, 5, 6],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'cclikpi': [5, 6, 5, 6, 7],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of cclikpi to track
cclikpi_list = [1, 2, 3, 4, 5]

# Combine dataframes into one
dataframes = [df1, df2, df3, df4, df5]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Filter combined_df to include only cclikpi from the original list
filtered_df = combined_df[combined_df['cclikpi'].isin(cclikpi_list)]

# Initialize an empty DataFrame to store the results
losses_per_month = pd.DataFrame(columns=['year', 'month', 'losses'])

# Iterate over each year and month to calculate the number of losses
for year in filtered_df['year'].unique():
    for month in range(1, 13):
        if month == 12:
            next_year = year + 1
            next_month = 1
        else:
            next_year = year
            next_month = month + 1
        
        current_month_df = filtered_df[(filtered_df['year'] == year) & (filtered_df['month'] == month)]
        next_month_df = filtered_df[(filtered_df['year'] == next_year) & (filtered_df['month'] == next_month)]
        
        current_clients = set(current_month_df['cclikpi'])
        next_clients = set(next_month_df['cclikpi'])
        
        lost_clients = current_clients - next_clients
        losses_per_month = losses_per_month.append({'year': year, 'month': month, 'losses': len(lost_clients)}, ignore_index=True)

print(losses_per_month)

















import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] <= month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    
    previous_clients.add(cclikpi)

# Mark clients as 'left' if they are not present in the next month or the first month of the next year
for year in combined_df['year'].unique():
    for month in range(1, 13):  # Go through all months in a year
        current_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]
        
        if month < 12:  # Check with the next month in the same year
            next_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month + 1)]
        else:  # Check with the first month of the next year
            next_month_df = combined_df[(combined_df['year'] == year + 1) & (combined_df['month'] == 1)]
        
        current_month_clients = set(current_month_df['cclikpi'])
        next_month_clients = set(next_month_df['cclikpi'])
        
        left_clients = current_month_clients - next_month_clients
        
        for client in left_clients:
            combined_df.loc[(combined_df['cclikpi'] == client) & (combined_df['year'] == year) & (combined_df['month'] == month), 'status'] = 'left'

print(combined_df)











import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] <= month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    
    previous_clients.add(cclikpi)

# Mark clients as 'left' if they are not present in the next month
for year in combined_df['year'].unique():
    for month in range(1, 12):  # Only go up to month 11 to compare with next month
        current_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]
        next_month_df = combined_df[(combined_df['year'] == year) & (combined_df['month'] == month + 1)]
        current_month_clients = set(current_month_df['cclikpi'])
        next_month_clients = set(next_month_df['cclikpi'])
        
        left_clients = current_month_clients - next_month_clients
        
        for client in left_clients:
            combined_df.loc[(combined_df['cclikpi'] == client) & (combined_df['year'] == year) & (combined_df['month'] == month), 'status'] = 'left'

print(combined_df)














import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018_jan = pd.DataFrame({'cclikpi': [1, 2, 3], 'revenue': [100, 200, 300], 'category': ['A', 'B', 'C']})
data_2018_feb = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [150, 250, 350], 'category': ['B', 'C', 'A']})
data_2019_jan = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [200, 300, 400], 'category': ['B', 'C', 'A']})
data_2019_feb = pd.DataFrame({'cclikpi': [3, 4, 5], 'revenue': [250, 350, 450], 'category': ['C', 'A', 'B']})

# List of dataframes and corresponding months and years
dataframes_2018 = [(data_2018_jan, '2018-01'), (data_2018_feb, '2018-02')]
dataframes_2019 = [(data_2019_jan, '2019-01'), (data_2019_feb, '2019-02')]

# Combine dataframes into a list with month information
dataframes = dataframes_2018 + dataframes_2019

# Add a 'month' column to each dataframe
for df, month in dataframes:
    df['month'] = month

# Initialize a list to store lost client details each year
lost_clients_details = []

# Combine dataframes for each year
combined_data = pd.concat([df for df, month in dataframes], ignore_index=True)

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Identify lost clients between each month
for i in range(1, len(dataframes)):
    previous_month_df = dataframes[i-1][0]
    current_month_df = dataframes[i][0]
    
    previous_month_clients = set(previous_month_df['cclikpi'])
    current_month_clients = set(current_month_df['cclikpi'])
    
    lost_clients = previous_month_clients - current_month_clients
    lost_clients_df = previous_month_df[previous_month_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_month'] = dataframes[i][1]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each month
loss_df = pd.DataFrame({
    'Month': [month for _, month in dataframes[1:]],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Month'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()








import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create a list to store the final rows including the left clients
final_rows = []

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    # Get the clients present in the current month
    current_month_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_month_clients:
        row['status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_month_clients:
        row['status'] = 'stayed'
    elif cclikpi in previous_clients and cclikpi not in current_month_clients:
        row['status'] = 'left'
        final_rows.append(row.copy())
        continue  # Skip adding this row to previous_clients
    
    previous_clients.add(cclikpi)
    final_rows.append(row.copy())

# Ensure clients who left are marked as 'left' in the subsequent years and months
for client in previous_clients:
    if client not in current_month_clients:
        for year in range(2021, 2023):  # Adjust years according to the data
            for month in range(1, 13):
                left_row = {'cclikpi': client, 'month': month, 'year': year, 'status': 'left'}
                final_rows.append(left_row)

# Create the final dataframe
final_df = pd.DataFrame(final_rows).sort_values(by=['year', 'month', 'cclikpi'])

print(final_df)












import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
all_clients = set(combined_df['cclikpi'].unique())
previous_clients = set()

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] <= month)]['cclikpi'])
    
    if cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    elif cclikpi in previous_clients and cclikpi not in current_clients:
        combined_df.at[index, 'status'] = 'left'
    
    previous_clients.add(cclikpi)

# Update the status for those who left in the previous year and did not join again
for cclikpi in all_clients:
    if combined_df[(combined_df['cclikpi'] == cclikpi) & (combined_df['status'] == 'left')].empty:
        last_month = combined_df[(combined_df['cclikpi'] == cclikpi)]['month'].max()
        combined_df.loc[(combined_df['cclikpi'] == cclikpi) & (combined_df['month'] > last_month), 'status'] = 'left'

print(combined_df)









import pandas as pd

# Sample data
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
previous_clients = set()
current_clients = set(combined_df['cclikpi'].unique())

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    if cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    elif cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi not in current_clients:
        combined_df.at[index, 'status'] = 'left'
    
    # Update previous_clients for the next iteration
    previous_clients.update(current_clients)
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]['cclikpi'])

# Update the status for those who left in the previous year and did not join again
for cclikpi in previous_clients:
    if cclikpi not in current_clients:
        combined_df.loc[(combined_df['cclikpi'] == cclikpi) & (combined_df['year'] > year), 'status'] = 'left'

print(combined_df)










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Combine all dataframes into one DataFrame
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize a dictionary to track the status of each client
client_status = {}

# Initialize the 'status' column
combined_df['status'] = 'Stayed'

# Update the status and pnb/pnb-rep columns
for index, row in combined_df.iterrows():
    cclikpi = row['CCLIKPI']
    
    if cclikpi in client_status:
        if client_status[cclikpi] == 'Left':
            combined_df.at[index, 'status'] = 'Left'
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
    else:
        # Check if the client is present in the next month's data
        next_month_df = combined_df[(combined_df['year'] == row['year']) & (combined_df['month'] == (row['month'] + 1))]
        
        if cclikpi not in set(next_month_df['CCLIKPI']):
            combined_df.at[index, 'status'] = 'Left'
            client_status[cclikpi] = 'Left'
        else:
            client_status[cclikpi] = 'Stayed'

# Update all remaining occurrences of clients marked as 'Left'
for cclikpi, status in client_status.items():
    if status == 'Left':
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'status'] = 'Left'
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'pnb'] = 0
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'pnb-rep'] = 0

print(combined_df)










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Combine all dataframes into one DataFrame
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Create a column to store the status
combined_df['status'] = 'Stayed'

# Track the status of clients
client_status = {}

# Update status to 'Left' and set pnb and pnb-rep to 0 when a client is lost
for index, row in combined_df.iterrows():
    cclikpi = row['CCLIKPI']
    
    if cclikpi in client_status:
        # If the client was previously marked as 'Left', keep it 'Left'
        if client_status[cclikpi] == 'Left':
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
        else:
            client_status[cclikpi] = 'Stayed'
    else:
        # If the client is not in the current month's data, mark as 'Left'
        current_month_df = combined_df[(combined_df['year'] == row['year']) & (combined_df['month'] == row['month'])]
        current_month_clients = set(current_month_df['CCLIKPI'])
        if cclikpi not in current_month_clients:
            combined_df.at[index, 'status'] = 'Left'
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
            client_status[cclikpi] = 'Left'
        else:
            client_status[cclikpi] = 'Stayed'

# Ensure that the first occurrence of each client has the correct 'Stayed' status
for cclikpi in client_status.keys():
    if client_status[cclikpi] == 'Left':
        first_occurrence_index = combined_df[combined_df['CCLIKPI'] == cclikpi].index[0]
        combined_df.at[first_occurrence_index, 'status'] = 'Stayed'

print(combined_df)






import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Identify lost clients year by year
lost_clients_count_year = []
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['CCLIKPI'])
    current_year_clients = set(current_year_df['CCLIKPI'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_count_year.append(len(lost_clients))

loss_df_year = pd.DataFrame({
    'Year': [df['year'].iloc[0] for df in dataframes[1:]],
    'Lost Clients': lost_clients_count_year
})

print(loss_df_year)

# Combine all dataframes into one with month and year information
combined_df = pd.concat(dataframes, ignore_index=True)

# Identify lost clients month by month
lost_clients_count_month = []
combined_df = combined_df.sort_values(by=['year', 'month'])
for i in range(1, len(combined_df)):
    previous_month_df = combined_df.iloc[:i]
    current_month_df = combined_df.iloc[i:]
    
    previous_month_clients = set(previous_month_df['CCLIKPI'])
    current_month_clients = set(current_month_df['CCLIKPI'])
    
    lost_clients = previous_month_clients - current_month_clients
    
    lost_clients_count_month.append(len(lost_clients))

loss_df_month = pd.DataFrame({
    'Year-Month': combined_df['year'].astype(str) + '-' + combined_df['month'].astype(str),
    'Lost Clients': [0] + lost_clients_count_month  # Adding 0 for the first month
})

print(loss_df_month)

# Plot the number of lost clients each year
plt.figure(figsize=(10, 6))
plt.plot(loss_df_year['Year'], loss_df_year['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Year')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.grid(True)
plt.show()

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df_month['Year-Month'], loss_df_month['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Year-Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()











import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018_jan = pd.DataFrame({'cclikpi': [1, 2, 3], 'revenue': [100, 200, 300], 'category': ['A', 'B', 'C']})
data_2018_feb = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [150, 250, 350], 'category': ['B', 'C', 'A']})
data_2019_jan = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [200, 300, 400], 'category': ['B', 'C', 'A']})
data_2019_feb = pd.DataFrame({'cclikpi': [3, 4, 5], 'revenue': [250, 350, 450], 'category': ['C', 'A', 'B']})

# List of dataframes and corresponding months and years
dataframes_2018 = [(data_2018_jan, '2018-01'), (data_2018_feb, '2018-02')]
dataframes_2019 = [(data_2019_jan, '2019-01'), (data_2019_feb, '2019-02')]

# Combine dataframes into a list with month information
dataframes = dataframes_2018 + dataframes_2019

# Add a 'month' column to each dataframe
for df, month in dataframes:
    df['month'] = month

# Initialize a list to store lost client details each year
lost_clients_details = []

# Combine dataframes for each year
combined_data = pd.concat([df for df, month in dataframes], ignore_index=True)

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Identify lost clients between each month
for i in range(1, len(dataframes)):
    previous_month_df = dataframes[i-1][0]
    current_month_df = dataframes[i][0]
    
    previous_month_clients = set(previous_month_df['cclikpi'])
    current_month_clients = set(current_month_df['cclikpi'])
    
    lost_clients = previous_month_clients - current_month_clients
    lost_clients_df = previous_month_df[previous_month_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_month'] = dataframes[i][1]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each month
loss_df = pd.DataFrame({
    'Month': [month for _, month in dataframes[1:]],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Month'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()





import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each year
loss_df = pd.DataFrame({
    'Year': years[1:],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each year
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Year'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Year')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.grid(True)
plt.show()










Environ 32,2 % des clients ont Ã©tÃ© perdus tous types confondus entre 2019 et 2023.

Pour des analyses plus dÃ©taillÃ©es et interactives, veuillez consulter le tableau de bord Power BI.

Cordialement,



import pandas as pd

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

# Combine all years data for identifying clients who stayed
all_years_data = pd.concat(dataframes, ignore_index=True)

# Identify unique clients who stayed
stayed_clients = all_years_data[~all_years_data['cclikpi'].isin(lost_clients_combined_df['cclikpi'])].drop_duplicates('cclikpi')
stayed_clients['status'] = 'Stayed'
stayed_clients['lost_year'] = None

# Combine lost and stayed clients
combined_df = pd.concat([lost_clients_combined_df, stayed_clients], ignore_index=True)

# Add columns for visualization
combined_df['initial_year'] = combined_df.groupby('cclikpi')['lost_year'].transform('min')
combined_df['final_year'] = combined_df.groupby('cclikpi')['lost_year'].transform('max')
combined_df['total_revenue'] = combined_df[['revenue_2018', 'revenue_2019', 'revenue_2020', 'revenue_2021', 'revenue_2022']].sum(axis=1)

print(combined_df)

# Export the combined DataFrame to a CSV file for use in Power BI
combined_df.to_csv('client_status_analysis.csv', index=False)
print("Data exported to 'client_status_analysis.csv' for use in Power BI.")










import pandas as pd

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

# Isolate the 'cclikpi' column and remove duplicates to get unique lost clients
unique_lost_clients = lost_clients_combined_df['cclikpi'].drop_duplicates()

# Calculate the total number of unique clients lost
total_unique_clients_lost = len(unique_lost_clients)

print("Total number of unique clients lost:", total_unique_clients_lost)











Rapport sur la Perte de Clients Professionnels (CCLIKPI)

Introduction:
Ce rapport prÃ©sente les statistiques dÃ©taillÃ©es sur le nombre de clients professionnels perdus par annÃ©e, en se concentrant sur deux segments : les entreprises (CNOUVSEG=ER) et les autres clients professionnels (CNOUVSEG=AC PL AG). Les donnÃ©es incluent Ã©galement des mesures de frÃ©quence et des pourcentages de pertes annuelles.

1. Perte de Clients pour les Entreprises (CNOUVSEG=ER) :

AnnÃ©e	Nombre de clients perdus	% de perte annuelle
2023	5635	8.8%
2022	5596	8.7%
2021	4776	7.9%
2020	4875	8%
Observations :

En 2023, le nombre de clients perdus pour les entreprises a lÃ©gÃ¨rement augmentÃ©, atteignant 5635, avec un taux de perte de 8.8%.
La perte annuelle de clients a fluctuÃ© entre 7.9% et 8.8% de 2020 Ã  2023, indiquant une tendance stable mais lÃ©gÃ¨rement croissante.
2. Perte de Clients pour les Autres Professionnels (CNOUVSEG=AC PL AG) :

AnnÃ©e	Nombre de clients perdus	% de perte annuelle
2023	35819	10.7%
2022	33410	10.4%
2021	29445	9.5%
2020	30937	10.3%
Observations :

Le nombre de clients perdus dans ce segment a considÃ©rablement augmentÃ© en 2023, atteignant 35819, avec un taux de perte de 10.7%.
La perte annuelle de clients dans ce segment montre une tendance Ã  la hausse, passant de 9.5% en 2021 Ã  10.7% en 2023.
Analyse des Tendances de Perte de Clients :

Entre 2019 et 2020 :

Entreprises (ER) : 8% de clients perdus.
Autres Professionnels (AC AG PL) : 10.3% de clients perdus.
Entre 2020 et 2021 :

Entreprises (ER) : 7.9% de clients perdus.
Autres Professionnels (AC AG PL) : 9.5% de clients perdus.
Entre 2021 et 2022 :

Entreprises (ER) : 8.7% de clients perdus.
Autres Professionnels (AC AG PL) : 10.4% de clients perdus.
Entre 2022 et 2023 :

Entreprises (ER) : 8.8% de clients perdus.
Autres Professionnels (AC AG PL) : 10.7% de clients perdus.
Conclusion:
Les donnÃ©es montrent une tendance inquiÃ©tante de perte de clients dans les deux segments de clients professionnels. Les entreprises et les autres clients professionnels voient tous deux une augmentation progressive des pertes de clients au fil des annÃ©es, avec des pourcentages de perte annuels relativement Ã©levÃ©s. Cette analyse souligne la nÃ©cessitÃ© de stratÃ©gies de rÃ©tention client plus efficaces pour inverser cette tendance.

Cordialement,










import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

print(lost_clients_combined_df)

# Export the combined DataFrame to a CSV file for use in Power BI
lost_clients_combined_df.to_csv('lost_clients_analysis.csv', index=False)
print("Data exported to 'lost_clients_analysis.csv' for use in Power BI.")

# Plot the number of lost clients each year by category
plt.figure(figsize=(10, 6))
sns.countplot(data=lost_clients_combined_df, x='lost_year', hue='category', palette='Set2')
plt.title('Number of Clients Lost Each Year by Category')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.legend(title='Category')
plt.grid(True)
plt.show()

# Plot the distribution of lost client revenues by year and category
plt.figure(figsize=(10, 6))
sns.boxplot(data=lost_clients_combined_df.melt(id_vars=['cclikpi', 'category', 'lost_year'], 
                                               value_vars=['revenue_2018', 'revenue_2019', 'revenue_2020', 'revenue_2021'], 
                                               var_name='year', value_name='revenue'),
            x='year', y='revenue', hue='category', palette='Set2')
plt.title('Distribution of Lost Client Revenues by Year and Category')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt



import csv

def list_to_csv(int_list, filename='output.csv'):
    # Convert each integer to a string and add '0' at the beginning
    str_list = ['0' + str(i) for i in int_list]
    
    # Write the list to a CSV file
    with open(filename, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        # Writing the list as a single row
        csvwriter.writerow(str_list)

# Example usage
int_list = [123, 456, 789]
list_to_csv(int_list)



import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, VotingRegressor, BaggingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B'],  # Example category column
    'target': [1.5, 2.0, 0.5, 2.5, 1.0]  # Example continuous target variable
})

# Discretize the continuous variables as described before
# For simplicity, using already discretized values
data['pnb'] = np.digitize(data['pnb'], bins=[150, 250, 350, 450])
data['pnb2023'] = np.digitize(data['pnb2023'], bins=[200, 300, 400, 500])

# Define features and target
X = data[['pnb', 'pnb2023']]
y = data['target']

# Initialize the models
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),
    'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),
    'Voting Regressor': VotingRegressor(estimators=[
        ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)),
        ('et', ExtraTreesRegressor(n_estimators=50, random_state=42))
    ]),
    'Bagging Regressor': BaggingRegressor(n_estimators=100, random_state=42)
}

# Define cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models
def evaluate_model(model, X, y, cv):
    scoring = {
        'MAE': 'neg_mean_absolute_error',
        'MSE': 'neg_mean_squared_error',
        'R2': 'r2'
    }
    
    results = {
        'MAE': -np.mean(cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')),
        'MSE': -np.mean(cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error')),
        'R2': np.mean(cross_val_score(model, X, y, cv=cv, scoring='r2'))
    }
    return results

# Evaluate all models
results = {}
for model_name, model in models.items():
    results[model_name] = evaluate_model(model, X, y, kf)

# Display results
results_df = pd.DataFrame(results).T
print("Evaluation Results:")
print(results_df)






import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier, BaggingClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B'],  # Example category column
    'target': [1, 0, 1, 0, 1]  # Example binary target variable
})

# Discretize the continuous variables as described before
# For simplicity, using already discretized values
data['pnb'] = np.digitize(data['pnb'], bins=[150, 250, 350, 450])
data['pnb2023'] = np.digitize(data['pnb2023'], bins=[200, 300, 400, 500])

# Define features and target
X = data[['pnb', 'pnb2023']]
y = data['target']

# Initialize the models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),
    'Voting Classifier': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
        ('et', ExtraTreesClassifier(n_estimators=50, random_state=42))
    ], voting='soft'),
    'Bagging Classifier': BaggingClassifier(n_estimators=100, random_state=42)
}

# Define cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models
def evaluate_model(model, X, y, cv):
    scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    results = {
        'Accuracy': np.mean(cross_val_score(model, X, y, cv=cv, scoring='accuracy')),
        'F1 Score': np.mean(cross_val_score(model, X, y, cv=cv, scoring='f1')),
        'Precision': np.mean(cross_val_score(model, X, y, cv=cv, scoring='precision')),
        'Recall': np.mean(cross_val_score(model, X, y, cv=cv, scoring='recall')),
        'ROC-AUC': np.mean(cross_val_score(model, X, y, cv=cv, scoring='roc_auc'))
    }
    return results

# Evaluate all models
results = {}
for model_name, model in models.items():
    results[model_name] = evaluate_model(model, X, y, kf)

# Display results
results_df = pd.DataFrame(results).T
print("Evaluation Results:")
print(results_df)









import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The continuous target variable
target = pd.Series([1.5, 2.0, 0.5, 2.5, 1.0], name='target')

# Function to find split points using a decision tree regressor
def find_splits(data, target):
    tree = DecisionTreeRegressor(criterion='mse', max_leaf_nodes=6)
    tree.fit(data.reshape(-1, 1), target)
    
    thresholds = tree.tree_.threshold
    split_points = sorted(threshold for threshold in thresholds if threshold != -2)
    return split_points

# Apply decision tree regressor to find splits for each continuous column
columns_to_discretize = ['pnb', 'pnb2023']
split_points_dict = {col: find_splits(data[col].values, target.values) for col in columns_to_discretize}

# Discretize the data based on the split points
def discretize(data, split_points):
    return np.digitize(data, split_points, right=True)

df_disc = data.copy()
for col, splits in split_points_dict.items():
    df_disc[col] = discretize(data[col].values, splits)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, df_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(df_disc)














import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Function to find split points using a decision tree
def find_splits(data, target):
    tree = DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=6)
    tree.fit(data.reshape(-1, 1), target)
    
    thresholds = tree.tree_.threshold
    split_points = sorted(threshold for threshold in thresholds if threshold != -2)
    return split_points

# Apply decision tree to find splits for each continuous column
columns_to_discretize = ['pnb', 'pnb2023']
split_points_dict = {col: find_splits(data[col].values, target_shifted.values) for col in columns_to_discretize}

# Discretize the data based on the split points
def discretize(data, split_points):
    return np.digitize(data, split_points, right=True)

df_disc = data.copy()
for col, splits in split_points_dict.items():
    df_disc[col] = discretize(data[col].values, splits)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, df_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(df_disc)









import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mdlp.discretization import MDLP

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target_shifted.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, data_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)












import pandas as pd
from mdlp.discretization import MDLP
import numpy as np

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target_shifted.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)














import pandas as pd
from mdlp.discretization import MDLP

# Sample DataFrame with negative numbers for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [-100, 200, -300, 400, -500],
    'pnb2023': [-150, 250, -350, 450, -550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable
target = pd.Series([0, 1, 0, 1, 0], name='target')

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)










import pandas as pd
from mdlp.discretization import MDLP

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable
target = pd.Series([0, 1, 0, 1, 0], name='target')

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)













import numpy as np
import pandas as pd
from math import log2

def entropy(y):
    """Calculate the entropy of a distribution for the classes in y."""
    if len(y) == 0:
        return 0
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

def mdlp_cut(data, labels):
    """Find the optimal cut point for discretization using MDLPC."""
    best_cut = None
    min_mdlp_value = float('inf')
    
    sorted_idx = np.argsort(data)
    sorted_data = data[sorted_idx]
    sorted_labels = labels[sorted_idx]
    
    for i in range(1, len(data)):
        if sorted_data[i] == sorted_data[i - 1]:
            continue
        
        left_labels = sorted_labels[:i]
        right_labels = sorted_labels[i:]
        
        left_entropy = entropy(left_labels)
        right_entropy = entropy(right_labels)
        
        left_prob = len(left_labels) / len(labels)
        right_prob = len(right_labels) / len(labels)
        
        total_entropy = left_prob * left_entropy + right_prob * right_entropy
        
        mdlp_value = entropy(labels) - total_entropy - (log2(len(data) - 1) / len(data))
        
        if mdlp_value < min_mdlp_value:
            min_mdlp_value = mdlp_value
            best_cut = (sorted_data[i] + sorted_data[i - 1]) / 2
    
    return best_cut

def mdlp_discretize(data, labels):
    """Apply the MDLPC algorithm to discretize the data."""
    data = np.array(data)
    labels = np.array(labels)
    
    cut_points = []
    intervals = [(0, len(data))]
    
    while intervals:
        start, end = intervals.pop()
        best_cut = mdlp_cut(data[start:end], labels[start:end])
        
        if best_cut is not None:
            cut_points.append(best_cut)
            left_end = np.searchsorted(data, best_cut, side='right')
            right_start = np.searchsorted(data, best_cut, side='left')
            
            intervals.append((start, left_end))
            intervals.append((right_start, end))
    
    cut_points = sorted(cut_points)
    
    discretized_data = np.digitize(data, cut_points, right=True)
    
    return discretized_data, cut_points

# Sample usage with Iris dataset
from sklearn.datasets import load_iris

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Apply MDLP discretization to each feature
discretized_features = []
cut_points_dict = {}
for i in range(X.shape[1]):
    discretized_feature, cut_points = mdlp_discretize(X[:, i], y)
    discretized_features.append(discretized_feature)
    cut_points_dict[iris.feature_names[i]] = cut_points

# Create a new DataFrame with the discretized features
df_disc = pd.DataFrame(np.array(discretized_features).T, columns=iris.feature_names)
df_disc['target'] = y

# Print the cut points and the new DataFrame with discretized columns
print("Cut points for each feature:")
for feature, cuts in cut_points_dict.items():
    print(f"{feature}: {cuts}")

print("\nNew DataFrame with discretized columns:")
print(df_disc.head())








import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from mdlp.discretization import MDLP

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Create DataFrame from the Iris dataset
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y

# Apply MDLP discretization
transformer = MDLP()
X_disc = transformer.fit_transform(X, y)

# Create a new DataFrame with the discretized columns
df_disc = pd.DataFrame(X_disc, columns=iris.feature_names)
df_disc['target'] = y

# Function to plot before and after discretization
def plot_discretization(df, df_disc, feature_name):
    # Create a figure with subplots
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot the continuous variable (before discretization)
    axs[0].hist(df[feature_name], bins=30, color='skyblue', edgecolor='black')
    axs[0].set_title(f'Before Discretization: {feature_name}')
    axs[0].set_xlabel(feature_name)
    axs[0].set_ylabel('Frequency')
    
    # Plot the discrete variable (after discretization)
    axs[1].hist(df_disc[feature_name], bins=np.arange(df_disc[feature_name].min(), df_disc[feature_name].max() + 1), color='lightcoral', edgecolor='black')
    axs[1].set_title(f'After Discretization: {feature_name}')
    axs[1].set_xlabel(f'{feature_name} (discretized)')
    axs[1].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot for each feature
for feature_name in iris.feature_names:
    plot_discretization(df, df_disc, feature_name)

# Print the new DataFrame with discretized columns
print("New DataFrame with discretized columns:")
print(df_disc.head())










import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from mdlp.discretization import MDLP

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Apply MDLP discretization
transformer = MDLP()
X_disc = transformer.fit_transform(X, y)

# Function to plot before and after discretization
def plot_discretization(X, X_disc, feature_index):
    feature_name = iris.feature_names[feature_index]
    
    # Create a figure with subplots
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot the continuous variable (before discretization)
    axs[0].hist(X[:, feature_index], bins=30, color='skyblue', edgecolor='black')
    axs[0].set_title(f'Before Discretization: {feature_name}')
    axs[0].set_xlabel(feature_name)
    axs[0].set_ylabel('Frequency')
    
    # Plot the discrete variable (after discretization)
    axs[1].hist(X_disc[:, feature_index], bins=np.arange(X_disc[:, feature_index].min(), X_disc[:, feature_index].max() + 1), color='lightcoral', edgecolor='black')
    axs[1].set_title(f'After Discretization: {feature_name}')
    axs[1].set_xlabel(f'{feature_name} (discretized)')
    axs[1].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot for each feature
for i in range(X.shape[1]):
    plot_discretization(X, X_disc, i)

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def entropy(y):
    """Calculate the entropy of a distribution for the classes in y."""
    if len(y) == 0:
        return 0
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

def mdlp_cut(data, labels):
    """Find the optimal cut point for discretization using MDLPC."""
    best_cut = None
    min_mdlp_value = float('inf')
    
    sorted_idx = np.argsort(data)
    sorted_data = data[sorted_idx]
    sorted_labels = labels[sorted_idx]
    
    for i in range(1, len(data)):
        if sorted_data[i] == sorted_data[i - 1]:
            continue
        
        left_labels = sorted_labels[:i]
        right_labels = sorted_labels[i:]
        
        left_entropy = entropy(left_labels)
        right_entropy = entropy(right_labels)
        
        left_prob = len(left_labels) / len(labels)
        right_prob = len(right_labels) / len(labels)
        
        total_entropy = left_prob * left_entropy + right_prob * right_entropy
        
        mdlp_value = entropy(labels) - total_entropy - (log(len(data) - 1) / len(data))
        
        if mdlp_value < min_mdlp_value:
            min_mdlp_value = mdlp_value
            best_cut = sorted_data[i]
    
    return best_cut

def mdlp_discretize(data, labels):
    """Apply the MDLPC algorithm to discretize the data."""
    data = np.array(data)
    labels = np.array(labels)
    
    cut_points = []
    intervals = [(0, len(data))]
    
    while intervals:
        start, end = intervals.pop()
        best_cut = mdlp_cut(data[start:end], labels[start:end])
        
        if best_cut is not None:
            cut_points.append(best_cut)
            left_end = np.searchsorted(data, best_cut, side='right')
            right_start = np.searchsorted(data, best_cut, side='left')
            
            intervals.append((start, left_end))
            intervals.append((right_start, end))
    
    cut_points = sorted(cut_points)
    
    discretized_data = np.digitize(data, cut_points, right=True)
    
    return discretized_data, cut_points

# Sample usage
data = pd.DataFrame({
    'continuous_variable': np.random.rand(100),
    'label': np.random.choice([0, 1], size=100)
})

discretized_data, cut_points = mdlp_discretize(data['continuous_variable'], data['label'])
data['discretized_variable'] = discretized_data

# Visualization
plt.figure(figsize=(12, 6))

# Histogram of continuous variable
plt.subplot(1, 2, 1)
sns.histplot(data['continuous_variable'], bins=20, kde=True)
plt.title('Continuous Variable Before Discretization')
plt.xlabel('Continuous Variable')
plt.ylabel('Frequency')

# Bar plot of discretized variable
plt.subplot(1, 2, 2)
sns.countplot(x='discretized_variable', data=data)
plt.title('Discretized Variable After MDLPC')
plt.xlabel('Discretized Variable')
plt.ylabel('Count')

plt.tight_layout()
plt.show()














import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi', 'category'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Define a color palette for the categories
palette = sns.color_palette("husl", data_long['category'].nunique())
color_map = {category: palette[i] for i, category in enumerate(data_long['category'].unique())}

# Plot the evolution of PNB for each company with lines colored based on the category
plt.figure(figsize=(10, 6))
for key, grp in data_long.groupby(['cclikpi']):
    category = grp['category'].values[0]
    plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}', color=color_map[category])

# Create a custom legend for the categories
handles = [plt.Line2D([0], [0], color=color_map[cat], lw=4, label=cat) for cat in color_map]
plt.legend(handles=handles, title='Category')

plt.xlabel('Year')
plt.ylabel('PNB Value')
plt.title('Evolution of PNB from 2018 to 2023 for Each Company')
plt.grid(True)
plt.show()














import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi', 'category'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Define a color palette for the categories
palette = sns.color_palette("husl", data_long['category'].nunique())
color_map = {category: palette[i] for i, category in enumerate(data_long['category'].unique())}

# Function to plot batches of companies
def plot_batches(data, batch_size=10):
    unique_ids = data['cclikpi'].unique()
    num_batches = len(unique_ids) // batch_size + (1 if len(unique_ids) % batch_size != 0 else 0)
    
    for i in range(num_batches):
        plt.figure(figsize=(10, 6))
        batch_ids = unique_ids[i * batch_size: (i + 1) * batch_size]
        batch_data = data[data['cclikpi'].isin(batch_ids)]
        
        for key, grp in batch_data.groupby(['cclikpi']):
            category = grp['category'].values[0]
            plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}', color=color_map[category])
        
        plt.xlabel('Year')
        plt.ylabel('PNB Value')
        plt.title(f'Evolution of PNB from 2018 to 2023 for Companies {i * batch_size + 1} to {(i + 1) * batch_size}')
        plt.legend(title='cclikpi')
        plt.grid(True)
        plt.show()

# Plot the data in batches
plot_batches(data_long, batch_size=2)  # Adjust batch_size as needed




















import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550]
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Function to plot batches of companies
def plot_batches(data, batch_size=10):
    unique_ids = data['cclikpi'].unique()
    num_batches = len(unique_ids) // batch_size + (1 if len(unique_ids) % batch_size != 0 else 0)
    
    for i in range(num_batches):
        plt.figure(figsize=(10, 6))
        batch_ids = unique_ids[i * batch_size: (i + 1) * batch_size]
        batch_data = data[data['cclikpi'].isin(batch_ids)]
        
        for key, grp in batch_data.groupby(['cclikpi']):
            plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}')
        
        plt.xlabel('Year')
        plt.ylabel('PNB Value')
        plt.title(f'Evolution of PNB from 2018 to 2023 for Companies {i * batch_size + 1} to {(i + 1) * batch_size}')
        plt.legend(title='cclikpi')
        plt.grid(True)
        plt.show()

# Plot the data in batches
plot_batches(data_long, batch_size=2)  # Adjust batch_size as needed













import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550]
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Plot the evolution of PNB for each company
plt.figure(figsize=(10, 6))
for key, grp in data_long.groupby(['cclikpi']):
    plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}')

plt.xlabel('Year')
plt.ylabel('PNB Value')
plt.title('Evolution of PNB from 2018 to 2023 for Each Company')
plt.legend(title='cclikpi')
plt.grid(True)
plt.show()










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def find_new_rows_added(df1, df2, id_column, merge_column):
    # Perform the merge
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left', indicator=True)
    
    # Find new rows added by checking for rows in df2 but not in df1
    new_rows = merged_df[merged_df['_merge'] == 'right_only']
    
    return new_rows

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Find new rows added during the merge
new_rows_added = find_new_rows_added(df1, df2, id_column, merge_column)

print("New Rows Added:")
print(new_rows_added)














import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def merge_dataframes(df1, df2, id_column, merge_column):
    # Merge the two DataFrames on the id_column
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left')
    
    return merged_df

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Merge the dataframes and add the specified column from df2 to df1
result_df = merge_dataframes(df1, df2, id_column, merge_column)

print("Resulting DataFrame:")
print(result_df)













import pandas as pd

def get_column_types(df):
    # Create a dictionary with column names as keys and their data types as values
    column_types = {col: df[col].dtype for col in df.columns}
    return column_types

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'type_de_client': ['A', 'B', 'C', 'D', 'E'],
    'value1': [10, 20, 30, 40, 50],
    'value2': [100.0, 200.0, 300.0, 400.0, 500.0]
})

# Get the column types
column_types = get_column_types(data)
print("Column Types:")
print(column_types)









import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'category': ['X', 'Y', 'X', 'Y', 'X', 'X', 'Y'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_orders = {
    'type_de_client': ['A', 'B'],  # Priority order for 'type_de_client'
    'category': ['X', 'Y']         # Priority order for 'category'
}

# Custom aggregation function for categorical variables
def agg_priority(series, priority_order):
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Wrapper function to apply the correct priority order
def agg_wrapper(column, priority_orders):
    return lambda series: agg_priority(series, priority_orders[column])

# Perform the groupby operation and apply the custom aggregation functions
agg_dict = {
    'type_de_client': agg_wrapper('type_de_client', priority_orders),
    'category': agg_wrapper('category', priority_orders),
    'value1': 'sum',  # Example aggregation for numerical columns
    'value2': 'mean'  # Example aggregation for numerical columns
}

grouped_data = data.groupby('id').agg(agg_dict).reset_index()

print("Grouped DataFrame:")
print(grouped_data)






import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_order = ['A', 'B']  # You can easily change this list later

# Custom aggregation function for categorical variables
def agg_priority(series):
    # Get the first value in the series that matches the priority order
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Perform the groupby operation and apply the custom aggregation function
grouped_data = data.groupby('id').agg({
    'type_de_client': agg_priority,  # Apply the custom aggregation function
    'value1': 'sum',                 # Example aggregation for numerical columns
    'value2': 'mean'                 # Example aggregation for numerical columns
}).reset_index()

print("Grouped DataFrame:")
print(grouped_data)









import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column, class_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    # Calculate frequency and percentage of class distribution for matched IDs
    matched_class_freq = matched_df[class_column].value_counts()
    matched_class_percentage = matched_df[class_column].value_counts(normalize=True) * 100
    
    # Filter the DataFrame to get rows where the id is not in the id_list
    unmatched_df = df[~df[id_column].isin(id_list)]
    
    # Calculate frequency and percentage of class distribution for unmatched IDs
    unmatched_class_freq = unmatched_df[class_column].value_counts()
    unmatched_class_percentage = unmatched_df[class_column].value_counts(normalize=True) * 100
    
    return (matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
            unmatched_class_freq, unmatched_class_percentage)

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column names for IDs and class
id_column = 'id'
class_column = 'value'

# Get the results
(matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
 unmatched_class_freq, unmatched_class_percentage) = filter_dataframe_and_find_unmatched(large_df, id_list, id_column, class_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)
print("\nMatched Class Frequency:")
print(matched_class_freq)
print("\nMatched Class Percentage:")
print(matched_class_percentage)
print("\nUnmatched Class Frequency:")
print(unmatched_class_freq)
print("\nUnmatched Class Percentage:")
print(unmatched_class_percentage)









import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'ccli': [1, 2, 3, 4, 5],
    'cclikpi': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'avant_ccli': [1, 2, 3, 6],
    'apre_ccli': [10, 20, 30, 60],
    'cclikpi': [100, 200, 300, 400]
})

# Create a dictionary from df2 for quick lookup
cclikpi_map = df2.set_index('avant_ccli')['cclikpi'].to_dict()

# Function to determine the value for cclikpi2
def get_cclikpi2(ccli, cclikpi):
    return cclikpi_map.get(ccli, cclikpi)

# Apply the function to create the new column in df1
df1['cclikpi2'] = df1.apply(lambda row: get_cclikpi2(row['ccli'], row['cclikpi']), axis=1)

print("Resulting DataFrame:")
print(df1)





import pandas as pd

def filter_by_id_pattern(df, id_column, pattern):
    # Convert the id column to string
    df[id_column] = df[id_column].astype(str)
    
    # Filter rows where the id starts with the specific pattern
    filtered_df = df[df[id_column].str.startswith(pattern)]
    
    return filtered_df

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [3001, 3002, 123, 4003, 3004, 567, 3005],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g']
})

# Specify the column name for IDs and the pattern
id_column = 'id'
pattern = '300'

# Get the filtered DataFrame
filtered_df = filter_by_id_pattern(large_df, id_column, pattern)

print("Filtered DataFrame:")
print(filtered_df)




import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    return matched_df, unmatched_ids

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column name for IDs
id_column = 'id'

# Get the results
matched_df, unmatched_ids = filter_dataframe_and_find_unmatched(large_df, id_list, id_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)










Bonjour SolÃ¨ne,

Je suis en train d'agrÃ©ger les donnÃ©es, mais j'ai un problÃ¨me avec l'agrÃ©gation des variables catÃ©gorielles comme "type de foyer" ou "CNOUVSEG". Abidjo et Arthur m'ont donnÃ© une liste d'ordre de prioritÃ© pour certaines catÃ©gories, mais je ne suis pas sÃ»r si cela est toujours correct pour mon cas, et en plus, elle n'est pas exhaustive. Aurais-tu des idÃ©es ou des suggestions pour m'aider Ã  rÃ©soudre ce problÃ¨me?

Merci d'avance pour ton aide.


Aurais-tu une liste de prioritÃ© pour le reste des variables catÃ©gorielles?

Je vous remercie sincÃ¨rement pour votre offre d'aide. Je n'hÃ©siterai pas Ã  faire appel Ã  vous si j'ai besoin. Votre soutien est grandement apprÃ©ciÃ©


Bonjour SolÃ¨ne,

Je suis en train d'agrÃ©ger les donnÃ©es, mais j'ai un problÃ¨me avec l'agrÃ©gation des variables catÃ©gorielles comme "type de foyer" ou "CNOUVSEG". Aurais-tu des idÃ©es ou des suggestions pour m'aider Ã  rÃ©soudre ce problÃ¨me?

Merci d'avance pour ton aide.










import pandas as pd

def get_values_for_same_col1(data, col1, col2):
    # Group by the first column and collect the values of the second column
    result = data.groupby(col1)[col2].apply(list).to_dict()
    return result

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'c', 'd', 'e', 'f', 'g'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to use
col1 = 'col1'
col2 = 'col2'

values_for_same_col1 = get_values_for_same_col1(data, col1, col2)
print(values_for_same_col1)











import pandas as pd

def get_duplicates_across_columns(data, col1, col2):
    # Find duplicate rows based on the specified columns
    duplicate_rows = data[data.duplicated(subset=[col1, col2], keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=[col1, col2])
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to check for duplicates
col1 = 'col1'
col2 = 'col2'

duplicates = get_duplicates_across_columns(data, col1, col2)
print(duplicates)











import pandas as pd

def get_duplicates(data):
    # Find duplicate rows based on all columns
    duplicate_rows = data[data.duplicated(keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=list(data.columns))
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 20, 40, 50, 20, 10]
})

duplicates = get_duplicates(data)
print(duplicates)













import pandas as pd
import numpy as np

def get_indices_not_in_data2(data1, data2, col_name):
    # Drop NaN values from the target column in both dataframes
    data1_clean = data1.dropna(subset=[col_name])
    data2_clean = data2.dropna(subset=[col_name])
    
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2_clean[col_name].astype(data1_clean[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1_clean[~data1_clean[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5, np.nan],
    'value': ['a', 'b', 'c', 'd', 'e', 'f']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0, np.nan],
    'value': ['x', 'y', 'z', 'w']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)







import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2[col_name].astype(data1[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)











import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Get the unique values in the specified column of data2
    data2_values = set(data2[col_name].unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3, 4, 5],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)










import pandas as pd
import numpy as np

# Sample dataframe
data = {
    'ccli': [1, 1, 1, 2, 2, 2, 3, 3, 3, 3],
    'cclikpi': [10, 10, 10, 20, 20, 20, 30, 30, 30, 30],
    'col': [12, 12, 12, 11, 11, 11, 12, 12, 12, 12],  # The column with values from 1 to 12
    'value1': [0, 200, np.nan, 400, 0, 600, 700, 0, np.nan, 0],
    'value2': [100, 0, 0, 300, np.nan, 0, 500, 600, 0, 0]
}

df = pd.DataFrame(data)

# Function to calculate the number of zeros and NaNs in each row
def count_zeros_nans(row):
    return (row == 0).sum() + row.isna().sum()

# Add a column that counts the number of zeros and NaNs in each row
df['zeros_nans_count'] = df.apply(count_zeros_nans, axis=1)

# Sort the dataframe by 'ccli', 'cclikpi', 'col' and 'zeros_nans_count'
df = df.sort_values(by=['ccli', 'cclikpi', 'col', 'zeros_nans_count'])

# Drop duplicates keeping the first one based on 'ccli', 'cclikpi', and 'col' (the one with fewest zeros/nans)
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi', 'col'], keep='first')

# Drop the helper column as it's no longer needed
df_unique = df_unique.drop(columns=['zeros_nans_count'])

# Display the resulting dataframe
print(df_unique)



import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'col': [1, 12, 2, 11, 3, 6, 12],  # The column with values from 1 to 12
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Sort the dataframe by 'ccli', 'cclikpi' and 'col'
df = df.sort_values(by=['ccli', 'cclikpi', 'col'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)















import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-01', '2023-01-02', '2023-01-04'],
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Convert date column to datetime
df['date'] = pd.to_datetime(df['date'])

# Sort the dataframe by 'ccli', 'cclikpi' and 'date'
df = df.sort_values(by=['ccli', 'cclikpi', 'date'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)










Certainly! Here is an in-depth explanation of the first ten laws from Robert Greene's "The 48 Laws of Power":

Law 1: Never Outshine the Master
Concept: Always make those above you feel superior. Do not highlight your own talents to the point of making them feel insecure.
Application: Subtly display your skills while emphasizing your deference to your superiors. Praise their wisdom and ability, ensuring they receive credit.
Example: In the court of Louis XIV, his finance minister, Nicolas Fouquet, outshone the king by throwing an extravagant party. This led to Fouquet's downfall as Louis XIV felt threatened and imprisoned him.
Law 2: Never Put Too Much Trust in Friends, Learn How to Use Enemies
Concept: Friends are more likely to betray you as they become envious or complacent. Enemies, motivated by self-interest, are more reliable when properly managed.
Application: Be cautious with friends and keep a degree of separation. Convert enemies into allies by demonstrating that cooperation is mutually beneficial.
Example: Michael III of the Byzantine Empire relied on his friend Basil the Macedonian, who ultimately betrayed and killed him to take power.
Law 3: Conceal Your Intentions
Concept: Keep people off-balance and in the dark about your plans. This makes them unable to counter your moves.
Application: Use red herrings and decoys to mislead others. Keep your real goals hidden and reveal only enough to maintain trust.
Example: Otto von Bismarck, the Prussian statesman, used various deceptions to keep his opponents guessing about his true political intentions, which allowed him to unify Germany.
Law 4: Always Say Less Than Necessary
Concept: The more you say, the more likely you are to say something foolish. Silence and brevity create an aura of power.
Application: Be concise and deliberate with your words. Let others reveal more than you do, gaining insight into their thoughts and intentions.
Example: When interviewed, Andy Warhol often gave brief, enigmatic answers, which increased his mystique and allure in the art world.
Law 5: So Much Depends on Reputation â€“ Guard It with Your Life
Concept: Your reputation is the cornerstone of your power. A strong reputation increases your presence and influence, while a tarnished reputation diminishes your power.
Application: Protect your reputation at all costs and be vigilant against attacks. Use subtle tactics to undermine the reputations of your rivals.
Example: Julius Caesar carefully cultivated his reputation for being generous and merciful, which helped him gain the loyalty and support of the Roman populace.
Law 6: Court Attention at All Costs
Concept: Stand out and make yourself noticed. Being unseen or forgotten is detrimental to your power.
Application: Create memorable and dramatic events or actions that capture attention. Cultivate an air of mystery and unpredictability.
Example: P.T. Barnum, the famous showman, constantly created public spectacles and stunts to keep himself and his shows in the public eye.
Law 7: Get Others to Do the Work for You, but Always Take the Credit
Concept: Use the skills, knowledge, and effort of others to further your own goals. Taking credit for their work enhances your reputation and power.
Application: Delegate tasks while ensuring that you receive recognition for the results. Reward those who contribute to keep them motivated.
Example: Thomas Edison often employed skilled engineers and researchers who helped him develop his inventions, but he ensured that he was recognized as the primary inventor.
Law 8: Make Other People Come to You â€“ Use Bait If Necessary
Concept: By making others come to you, you control the situation and set the terms. Luring people to you ensures they play by your rules.
Application: Use allurements and incentives to attract people. Once they are drawn in, assert your dominance and influence.
Example: During negotiations, Henry Kissinger would often make foreign diplomats come to him, allowing him to dictate the terms and maintain a position of strength.
Law 9: Win Through Your Actions, Never Through Argument
Concept: Demonstrating your ideas through actions is far more persuasive than arguing. Arguments often lead to resentment and resistance.
Application: Let your actions speak for you. Show, donâ€™t tell, and allow people to see the benefits or consequences of your ideas firsthand.
Example: Instead of arguing with critics, Leonardo da Vinci would demonstrate his inventions and artworks, which left little room for doubt about his genius.
Law 10: Infection: Avoid the Unhappy and Unlucky
Concept: Emotions and attitudes are contagious. Associating with those who are negative or unlucky can bring you down.
Application: Distance yourself from those who have a detrimental effect on your well-being and success. Seek out positive and successful individuals.
Example: The French adventurer, Napoleon Bonaparte, avoided associating with people who were constantly negative or failed, focusing instead on those who were optimistic and successful.
These laws emphasize strategic thinking, careful manipulation, and maintaining control over social and professional interactions to enhance one's power and influen


















import pandas as pd
import numpy as np
from scipy.stats.mstats import winsorize

# Sample DataFrame with numeric data (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.randn(100) * 100,
    'B': np.random.randn(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to apply Winsorization to all numeric columns in the DataFrame
def winsorize_dataframe(df, limits=(0.05, 0.05)):
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    df_winsorized = df.copy()
    
    for col in numeric_cols:
        df_winsorized[col] = winsorize(df[col], limits=limits)
    
    return df_winsorized

# Apply Winsorization to the DataFrame
winsorized_df = winsorize_dataframe(df, limits=(0.05, 0.05))

# Display the original and Winsorized DataFrame
print("Original DataFrame:")
print(df.describe())

print("\nWinsorized DataFrame:")
print(winsorized_df.describe())













import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy.stats import f_oneway

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to perform ANOVA test for each categorical variable against the continuous target variable
def anova_test(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"ANOVA test for {col} in relation to {target_var}:")
        
        # Perform ANOVA using statsmodels
        formula = f'{target_var} ~ C({col})'
        model = ols(formula, data=df).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)
        
        print(anova_table)
        print("\n")

# Specify the target column
target_column = 'Target'

# Perform ANOVA tests
anova_test(df, target_column)



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to generate statistical summaries and plots for categorical variables in relation to a continuous target variable
def categorical_summary_with_target(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col} in relation to {target_var}:")
        summary = df.groupby(col)[target_var].describe()
        print(summary)
        print("\n")
        
        # Plotting the distribution of the continuous target variable for each category using box plot
        plt.figure(figsize=(8, 6))
        sns.boxplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

        # Plotting the distribution of the continuous target variable for each category using violin plot
        plt.figure(figsize=(8, 6))
        sns.violinplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

# Specify the target column
target_column = 'Target'

# Generate and plot summaries for categorical variables in relation to the continuous target variable
categorical_summary_with_target(df, target_column)













import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100)
})

# Function to generate statistical summaries for categorical variables
def categorical_summary(df):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col}:")
        print(df[col].value_counts())
        print("\n")
        
        # Plotting the distribution of the categorical variable
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x=col, palette='viridis')
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.show()

# Generate and plot summaries for categorical variables
categorical_summary(df)




















import pandas as pd
import numpy as np

# Sample data
data = {'values': [1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]}
df = pd.DataFrame(data)

# Central Tendency
mean = df['values'].mean()
median = df['values'].median()
mode = df['values'].mode()

# Variability
range_ = df['values'].max() - df['values'].min()
variance = df['values'].var()
std_dev = df['values'].std()
iqr = df['values'].quantile(0.75) - df['values'].quantile(0.25)

# Shape
skewness = df['values'].skew()
kurtosis = df['values'].kurt()

# Summary
summary = df['values'].describe()

print("Mean:", mean)
print("Median:", median)
print("Mode:", mode[0])
print("Range:", range_)
print("Variance:", variance)
print("Standard Deviation:", std_dev)
print("Interquartile Range (IQR):", iqr)
print("Skewness:", skewness)
print("Kurtosis:", kurtosis)
print("\nSummary:\n", summary)




















import pandas as pd

# Sample data
data = {
    'Column1': ['A', 'B', 'A', 'C', 'B', 'A', 'B', 'C'],
    'Column2': [1, 2, 1, 3, 2, 1, 2, 3]
}

df = pd.DataFrame(data)

# Check for duplicates based on Column1 and Column2
duplicates = df.duplicated(subset=['Column1', 'Column2'])
print("Boolean Series indicating duplicates:")
print(duplicates)

# Get only the rows that are duplicates
duplicate_rows = df[df.duplicated(subset=['Column1', 'Column2'], keep=False)]
print("\nDuplicate rows:")
print(duplicate_rows)















duplicates = df.duplicated(subset=['Column1', 'Column2'])
print(duplicates)









import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Split the data into training (non-missing target values) and testing (missing target values) sets
train_df = prepared_df[prepared_df[target_column].notna()]
test_df = prepared_df[prepared_df[target_column].isna()]

# Define features and target
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]
X_test = test_df.drop(columns=[target_column])

# Train the XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)

# Predict the missing values
y_pred = xgb_model.predict(X_test)

# Evaluate the model (Since we don't have true values for test set, we can't compute RMSE here)
# In practice, you would compare y_pred with the true values if they were known

# Add predictions back to the original DataFrame
prepared_df.loc[prepared_df[target_column].isna(), target_column] = y_pred

# Display the results
print("Predicted values for the missing target variable:")
print(prepared_df[prepared_df[target_column].isna()][[target_column]])

# Optionally, if we had true values, we would compute the RMSE like this:
# y_true = true_values_array_for_missing_entries
# rmse = np.sqrt(mean_squared_error(y_true, y_pred))
# print(f"RMSE for XGBoost predictions: {rmse}")













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    
    # Extract the rows with missing values in the target column
    missing_df = df[df[target_column].isna()]
    
    # Extract the rows with non-missing values in the target column
    non_missing_df = df[df[target_column].notna()]
    
    # Extract the true values of the target column for evaluation
    y_true = non_missing_df[target_column].copy()
    
    for method_name, imputer in imputation_methods.items():
        df_copy = non_missing_df.dropna(axis=1, how='any')
        df_copy[target_column] = non_missing_df[target_column]
        
        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        
        y_pred = imputed_df[target_column]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")























import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    y_true = df[target_column][df[target_column].isna()]

    for method_name, imputer in imputation_methods.items():
        df_copy = df.copy()
        df_copy = df_copy.dropna(axis=1, how='any')
        df_copy[target_column] = df[target_column]

        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)

        y_pred = imputed_df[target_column][df[target_column].isna()]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    # Create a copy of the DataFrame
    df_copy = df.copy()
    
    # Separate the target column
    target = df_copy[[target_column]].copy()
    
    # Drop columns with missing values
    df_copy = df_copy.dropna(axis=1, how='any')
    
    # Add the target column back
    df_copy = pd.concat([df_copy, target], axis=1)
    
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Display the prepared DataFrame
print(prepared_df.head())













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        # Ensure the training data does not contain NaNs
        X_train_imputed = imputer.fit_transform(X_train)
        imputed_train_df = pd.DataFrame(X_train_imputed, columns=X_train.columns)
        
        # Impute missing values in the test set using the trained imputer
        X_test = test_df.drop(columns=[target_column])
        X_test_imputed = imputer.transform(X_test)
        imputed_test_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)
        imputed_test_df[target_column] = test_df[target_column]
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")



















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Drop columns with missing values except for the target column
        df_copy = df_copy.dropna(axis=1, how='any', subset=[col for col in df.columns if col != target_column])
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")






















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques using cross-validation
def evaluate_imputation_methods(df, imputation_methods):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    results = {method: [] for method in imputation_methods.keys()}

    for train_index, test_index in kf.split(df):
        train_df, test_df = df.iloc[train_index], df.iloc[test_index]

        for method_name, imputer in imputation_methods.items():
            # Fit the imputer on the training set
            imputer.fit(train_df)
            
            # Transform both the training and test sets
            imputed_train_df = pd.DataFrame(imputer.transform(train_df), columns=df.columns)
            imputed_test_df = pd.DataFrame(imputer.transform(test_df), columns=df.columns)
            
            for col in df.columns:
                if train_df[col].isna().sum() > 0 or test_df[col].isna().sum() > 0:
                    # Calculate RMSE for each column with missing values
                    y_true = test_df[col].dropna()
                    y_pred = imputed_test_df.loc[y_true.index, col]
                    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                    results[method_name].append(rmse)
    
    # Calculate the mean RMSE for each imputation method
    mean_results = {method: np.mean(rmses) for method, rmses in results.items()}
    
    return mean_results

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, imputation_methods)

# Display the results
print("Mean RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")

















Handle Missing Values: Impute missing values to avoid complications in subsequent steps.
Handle Outliers: Address outliers to ensure they do not skew the scaling and modeling.
Handle Categorical Data: Encode categorical variables.
Scale Data: Normalize or standardize the data.
Handle Multicollinearity: Check for and address multicollinearity.










import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")
























import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5,
    'Target': np.random.rand(100) * 100
})

# Calculate correlation matrix
corr_matrix = df.corr()

# Display correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# Calculate R-squared values
target_var = 'Target'
r2_results = {}

for col in df.columns:
    if col != target_var:
        X = df[[col]]
        y = df[target_var]
        model = LinearRegression().fit(X, y)
        y_pred = model.predict(X)
        r2 = r2_score(y, y_pred)
        r2_results[col] = r2

# Display R-squared results
r2_df = pd.DataFrame.from_dict(r2_results, orient='index', columns=['R-Squared']).sort_values(by='R-Squared', ascending=False)
print(r2_df)

# Plot R-squared values
plt.figure(figsize=(10, 6))
sns.barplot(x=r2_df.index, y='R-Squared', data=r2_df, palette='viridis')
plt.title('R-Squared Values of Variables Explaining Target')
plt.xlabel('Variables')
plt.ylabel('R-Squared')
plt.xticks(rotation=45)
plt.show()














import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to plot distribution and detect outliers
def plot_distribution_and_outliers(df):
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in num_cols:
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Histogram and KDE Plot
        sns.histplot(df[col], kde=True, ax=axes[0], color='blue')
        axes[0].set_title(f'Distribution of {col}')
        axes[0].set_xlabel(col)
        
        # Box Plot for outliers
        sns.boxplot(y=df[col], ax=axes[1], color='orange')
        axes[1].set_title(f'Box Plot of {col}')
        
        plt.tight_layout()
        plt.show()
        
        # Detect outliers using IQR method
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        
        print(f'{col}:')
        print(f'Number of outliers: {outliers.shape[0]}')
        if not outliers.empty:
            print('Outliers:')
            print(outliers[[col]])
        print('\n')

# Plot distributions and outliers
plot_distribution_and_outliers(df)








# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Set font size for the plot
plt.rcParams.update({'font.size': 14})

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5, annot_kws={"size": 14})

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black', fontsize=14)

plt.title('Correlation Matrix with p-values (Filtered)', fontsize=18)
plt.xlabel('Variables', fontsize=16)
plt.ylabel('Variables', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()














import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()
















# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.xticks(rotation=45)
plt.show()
















import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_vif(df):
    # Add a constant to the dataframe
    df_with_const = add_constant(df)
    
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = df_with_const.columns
    vif_data["VIF"] = [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])]
    
    return vif_data

# Calculate VIF
vif_data = calculate_vif(df)

# Display the VIF result
print(vif_data)


import matplotlib.pyplot as plt

# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.show()













import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j:
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values')
plt.show()
































import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Store the results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Store the metrics
    results['Model'].append(model_name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1-Score'].append(f1)
    
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")

# Plot the results
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Comparison')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
axes = axes.flatten()

for i, metric in enumerate(metrics):
    axes[i].bar(results['Model'], results[metric], color=['blue', 'green', 'red'])
    axes[i].set_title(metric)
    axes[i].set_ylim([0, 1])
    axes[i].set_xlabel('Model')
    axes[i].set_ylabel(metric)
    for j, val in enumerate(results[metric]):
        axes[i].text(j, val + 0.02, f'{val:.2f}', ha='center')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()




import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Display the matrices
print("Correlation Matrix:")
print(corr_matrix)
print("\nP-Value Matrix:")
print(p_value_matrix)

# Combine the correlation matrix and p-value matrix into one DataFrame for display
combined_matrix = corr_matrix.astype(str) + " (p=" + p_value_matrix.round(4).astype(str) + ")"

print("\nCombined Matrix:")
print(combined_matrix)
















import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")
