import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Desired number of clusters
num_clusters = 10

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude target column
    feature_name = data.columns[feature_index]
    X = data.drop(columns='target')
    y = data['target']
    
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, max_leaf_nodes=num_clusters, random_state=0)
    dt.fit(X[[feature_name]], y)
    segments = dt.apply(X[[feature_name]])

    # Check for disjoint segments by ensuring non-overlapping intervals
    # This is a simplified example; more sophisticated checks and adjustments may be needed
    unique_segments = np.unique(segments)
    is_disjoint = len(unique_segments) <= num_clusters

    # Convert segments to categorical labels if disjoint
    if is_disjoint:
        label_encoder = LabelEncoder()
        segment_labels = label_encoder.fit_transform(segments)
    else:
        continue  # Skip if the segments are not disjoint

    # Check if this configuration is the best
    if len(unique_segments) > len(set(best_segments)) if best_segments is not None else 0:
        best_segments = segment_labels
        best_feature_index = feature_index

# Add the best segments to the DataFrame as a new column if disjoint segments were found
if best_segments is not None:
    data['best_segments'] = best_segments
    best_feature_name = data.columns[best_feature_index]
    print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Unique Segments: {len(set(best_segments))}")
else:
    print("No disjoint segments found with the given configuration.")

# Plot the target variable colored by the best segmentation if available
if best_segments is not None:
    plt.figure(figsize=(10, 6))
    plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o', label='Target variable')
    plt.title(f'Target Variable Segmentation by Feature {best_feature_name} with {num_clusters} Clusters')
    plt.xlabel('Sample Index')
    plt.ylabel('Target Variable Value')
    plt.colorbar(label='Segment Label')
    plt.grid(True)
    plt.legend()
    plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Example DataFrame with features and target
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.choice([0, 1], size=100)  # Binary target for classification

# Split the data into training and testing sets
X = data.drop(columns='target')
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train the XGBoost classifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X_train, y_train)

# Predict the target values for the test set
y_pred = xgb.predict(X_test)

# Calculate evaluation metrics
conf_matrix = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=xgb.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

# Print the evaluation metrics
print(f"F1 Score: {f1}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
data = pd.DataFrame(np.random.randn(100, 5), columns=[f'feature_{i}' for i in range(5)])
data['target'] = np.random.randn(100)

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(data.shape[1] - 1):  # Exclude target column
    feature_name = data.columns[feature_index]
    X = data.drop(columns='target')
    y = data['target']
    
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[[feature_name]], y)
    segments = dt.apply(X[[feature_name]])

    # Convert segments to categorical labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(segments)

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segment_labels)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segment_labels, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segment_labels

# Add the best segments to the DataFrame as a new column
data['best_segments'] = best_segments

# Output the best feature index and F1 score
best_feature_name = data.columns[best_feature_index]
print(f"Best Feature Index: {best_feature_index} ({best_feature_name}), Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['target'], c=data['best_segments'], cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_name}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic dataset for the example
np.random.seed(0)
X = np.random.randn(100, 5)  # 100 samples, 5 features
y = np.random.randn(100)     # Target variable

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(X.shape[1]):
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[:, feature_index].reshape(-1, 1), y)
    segments = dt.apply(X[:, feature_index].reshape(-1, 1))

    # Convert segments to categorical labels
    label_encoder = LabelEncoder()
    segment_labels = label_encoder.fit_transform(segments)

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segment_labels)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segment_labels, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segment_labels

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index}, Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y)), y, c=best_segments, cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_index}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.preprocessing import KBinsDiscretizer

# Generate a synthetic dataset for the example
np.random.seed(0)
X = np.random.randn(100, 5)  # 100 samples, 5 features
y = np.random.randn(100)     # Target variable

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(X.shape[1]):
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[:, feature_index].reshape(-1, 1), y)
    segments = dt.apply(X[:, feature_index].reshape(-1, 1))

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segments)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segments, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segments

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index}, Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y)), y, c=best_segments, cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_index}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import f1_score
from xgboost import XGBClassifier

# Generate a synthetic dataset for the example
np.random.seed(0)
X = np.random.randn(100, 5)  # 100 samples, 5 features
y = np.random.randn(100)     # Target variable

# Store the best F1 score and corresponding feature index
best_f1 = 0
best_feature_index = -1
best_segments = None

# Iterate over each feature to segment the target variable
for feature_index in range(X.shape[1]):
    # Segment the target variable using the current feature
    dt = DecisionTreeRegressor(min_samples_split=5, random_state=0)
    dt.fit(X[:, feature_index].reshape(-1, 1), y)
    segments = dt.predict(X[:, feature_index].reshape(-1, 1))

    # Use these segments as labels and predict using all features with XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgb.fit(X, segments)

    # Predict the segments and calculate the F1 score
    predicted_segments = xgb.predict(X)
    f1 = f1_score(segments, predicted_segments, average='weighted')

    # Check if this is the best F1 score
    if f1 > best_f1:
        best_f1 = f1
        best_feature_index = feature_index
        best_segments = segments

# Output the best feature index and F1 score
print(f"Best Feature Index: {best_feature_index}, Best F1 Score: {best_f1}")

# Plot the target variable colored by the best segmentation
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y)), y, c=best_segments, cmap='viridis', marker='o', label='Target variable')
plt.title(f'Target Variable Segmentation by Feature {best_feature_index}')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable Value')
plt.colorbar(label='Segment Label')
plt.grid(True)
plt.legend()
plt.show()





import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Generate a sample dataset with multiple features
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=0)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit the Decision Tree Regressor
tree = DecisionTreeRegressor(min_samples_split=5, random_state=0)
tree.fit(X_train, y_train)

# Predict using the decision tree
y_pred_train = tree.predict(X_train)
y_pred_test = tree.predict(X_test)

# Plot the original data and the decision tree segments
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# Training data plot
ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', label='Original data')
ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train, cmap='viridis', marker='x', label='Predicted segments')
ax[0].set_title('Training Data')
ax[0].set_xlabel('Feature 1')
ax[0].set_ylabel('Feature 2')
ax[0].legend()
ax[0].grid(True)

# Testing data plot
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', label='Original data')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, cmap='viridis', marker='x', label='Predicted segments')
ax[1].set_title('Testing Data')
ax[1].set_xlabel('Feature 1')
ax[1].set_ylabel('Feature 2')
ax[1].legend()
ax[1].grid(True)

plt.tight_layout()
plt.show()



import numpy as np
import matplotlib.pyplot as plt

# Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Set the minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Plot the entire data with each segment highlighted
for i, (start, end) in enumerate(segments):
    plt.figure()
    plt.plot(data, 'k-', alpha=0.3)  # Plot the full data in a lighter color
    plt.plot(range(start, end), data[start:end], 'b-', linewidth=2)  # Highlight the current segment
    plt.title(f'Segment {i + 1}: {start} to {end}')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.grid(True)
    plt.show()




import numpy as np
import matplotlib.pyplot as plt

# Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Set the minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Plotting the segments step-by-step
for i in range(len(segments)):
    plt.figure(figsize=(10, 5))
    plt.plot(data, label='Data', alpha=0.5)
    
    # Plot each segment up to the current iteration
    for j in range(i + 1):
        start, end = segments[j]
        plt.plot(range(start, end), data[start:end], marker='o', label=f'Segment {j + 1}')

    plt.title(f'Segmentation Step {i + 1}')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'segment_step_{i + 1}.png')  # Save each plot as an image file
    plt.show()




import numpy as np
import matplotlib.pyplot as plt

# Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Set the minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Plot each segment
for i, (start, end) in enumerate(segments):
    plt.figure()
    x = range(start, end)
    y = data[start:end]
    plt.plot(x, y, marker='o')
    plt.title(f'Segment {i + 1}: {start} to {end}')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.grid(True)
    plt.savefig(f'segment_{i + 1}.png')  # Save each plot as an image file
    plt.show()





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Step 1: Generate the data
np.random.seed(0)  # For reproducibility
data = np.random.randn(100)  # Generate 100 random points

# Step 2: Create segments based on a minimum number of observations per segment
min_obs_per_seg = 5

def generate_segments(data, min_obs_per_seg):
    n = len(data)
    # This function will generate the possible segmentations
    segments = []
    for start in range(n):
        for end in range(start + min_obs_per_seg, n + 1):
            segments.append((start, end))
    return segments

segments = generate_segments(data, min_obs_per_seg)

# Step 3: Animate the plot
fig, ax = plt.subplots()
ax.set_xlim(0, len(data))
ax.set_ylim(np.min(data), np.max(data))
line, = ax.plot([], [], lw=2)
title = ax.set_title('')

def init():
    line.set_data([], [])
    return line,

def update(frame):
    start, end = segments[frame]
    x = range(start, end)
    y = data[start:end]
    line.set_data(x, y)
    title.set_text(f'Segment {frame + 1}/{len(segments)}: {start} to {end}')
    return line, title

ani = FuncAnimation(fig, update, frames=range(len(segments)), init_func=init, blit=True, repeat=False)
plt.show()






import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from itertools import combinations

def generate_possible_segments(y, num_segments, min_obs_per_seg):
    """
    Generates all possible segmentations for the target variable `y` with a given number of segments and minimum observations per segment.
    
    Parameters:
    - y: pd.Series - The target variable.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.
    
    Returns:
    - possible_segments: List of lists containing segment boundaries.
    """
    possible_segments = []
    sorted_values = np.sort(y.unique())
    n_values = len(sorted_values)
    
    for combination in combinations(range(1, n_values), num_segments - 1):
        segments = np.split(sorted_values, combination)
        if all(len(seg) >= min_obs_per_seg for seg in segments):
            segment_boundaries = [seg[-1] for seg in segments[:-1]]  # Get the last value in each segment except the last
            segment_boundaries = [sorted_values[0] - 1] + segment_boundaries + [sorted_values[-1] + 1]
            possible_segments.append(segment_boundaries)
    
    return possible_segments

def optimize_segments(df, target_var, num_segments, min_obs_per_seg):
    """
    Optimizes the segmentation of a target variable in a DataFrame to maximize the F1 score of an XGBoost model.

    Parameters:
    - df: pd.DataFrame - The input DataFrame.
    - target_var: str - The target variable column name.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.

    Returns:
    - df_with_segments: pd.DataFrame - The DataFrame with an added 'segment' column.
    - best_f1: float - The best F1 score achieved.
    """
    data = df.copy()
    X = data.drop(columns=[target_var])
    y = data[target_var]
    
    possible_segments = generate_possible_segments(y, num_segments, min_obs_per_seg)
    best_f1 = -np.inf
    best_segments = None

    for boundaries in possible_segments:
        labels = pd.cut(y, bins=boundaries, labels=False, include_lowest=True)
        data['segment'] = labels

        # Train an XGBoost model and evaluate F1 score
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
        model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_segments)
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        f1 = f1_score(y_test, predictions, average='weighted')

        if f1 > best_f1:
            best_f1 = f1
            best_segments = boundaries

    # Assign the best segments to the DataFrame
    data['segment'] = pd.cut(y, bins=best_segments, labels=False, include_lowest=True)

    # Plotting the target variable with segments
    plt.figure(figsize=(12, 6))
    plt.scatter(range(len(y)), y, c=data['segment'], cmap='viridis', label='Segments', alpha=0.6)
    plt.colorbar(label='Segment')
    plt.title(f'Target Variable Segmentation with Best F1 Score = {best_f1:.4f}')
    plt.xlabel('Index')
    plt.ylabel(target_var)
    plt.grid(True)
    plt.legend()
    plt.show()

    return data, best_f1

# Example usage:
# df = pd.read_csv('your_data.csv')
# df_with_segments, best_f1 = optimize_segments(df, 'target_variable', num_segments=5, min_obs_per_seg=50)
# print("Data with segments:\n", df_with_segments.head())
# print("Best F1 score:", best_f1)








import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def optimize_segments(df, target_var, num_segments, min_obs_per_seg):
    """
    Optimizes the segmentation of a target variable in a DataFrame to maximize the F1 score of an XGBoost model.

    Parameters:
    - df: pd.DataFrame - The input DataFrame.
    - target_var: str - The target variable column name.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.

    Returns:
    - df_with_segments: pd.DataFrame - The DataFrame with an added 'segment' column.
    - best_f1: float - The best F1 score achieved.
    """
    data = df.copy()
    X = data.drop(columns=[target_var])
    y = data[target_var]

    # Create initial segment boundaries
    initial_segment_boundaries = np.linspace(y.min(), y.max(), num_segments + 1)
    best_f1 = -np.inf
    best_segments = initial_segment_boundaries

    def evaluate_segments(boundaries):
        nonlocal best_f1, best_segments
        
        # Create segment labels based on boundaries
        labels = pd.cut(y, bins=boundaries, labels=False, include_lowest=True)
        data['segment'] = labels
        
        # Check if each segment has the minimum required observations
        segment_counts = data['segment'].value_counts()
        if (segment_counts < min_obs_per_seg).any():
            return -np.inf  # Return a very low F1 score if any segment is too small
        
        # Train an XGBoost model and evaluate F1 score
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
        model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_segments)
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        f1 = f1_score(y_test, predictions, average='weighted')
        
        if f1 > best_f1:
            best_f1 = f1
            best_segments = boundaries
        
        return f1

    # Optimization loop - this is a placeholder for a more sophisticated optimization approach
    for _ in range(100):  # Number of iterations, adjust based on needs
        new_boundaries = initial_segment_boundaries + np.random.normal(0, 0.1, size=initial_segment_boundaries.shape)
        new_boundaries = np.sort(np.clip(new_boundaries, y.min(), y.max()))
        evaluate_segments(new_boundaries)

    # Assign the best segments to the DataFrame
    data['segment'] = pd.cut(y, bins=best_segments, labels=False, include_lowest=True)

    # Plotting the target variable with segments
    plt.figure(figsize=(12, 6))
    plt.scatter(range(len(y)), y, c=data['segment'], cmap='viridis', label='Segments', alpha=0.6)
    plt.colorbar(label='Segment')
    plt.title(f'Target Variable Segmentation with F1 Score = {best_f1:.4f}')
    plt.xlabel('Index')
    plt.ylabel(target_var)
    plt.grid(True)
    plt.legend()
    plt.show()

    return data, best_f1

# Example usage:
# df = pd.read_csv('your_data.csv')
# df_with_segments, best_f1 = optimize_segments(df, 'target_variable', num_segments=5, min_obs_per_seg=50)
# print("Data with segments:\n", df_with_segments.head())
# print("Best F1 score:", best_f1)








import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def optimize_segments(df, target_var, num_segments, min_obs_per_seg):
    """
    Optimizes the segmentation of a target variable in a DataFrame to maximize the R² score of an XGBoost model.

    Parameters:
    - df: pd.DataFrame - The input DataFrame.
    - target_var: str - The target variable column name.
    - num_segments: int - The number of segments to create.
    - min_obs_per_seg: int - Minimum number of observations per segment.

    Returns:
    - df_with_segments: pd.DataFrame - The DataFrame with an added 'segment' column.
    - best_r2: float - The best R² score achieved.
    """
    data = df.copy()
    X = data.drop(columns=[target_var])
    y = data[target_var]

    # Create initial segment boundaries
    initial_segment_boundaries = np.linspace(y.min(), y.max(), num_segments + 1)
    best_r2 = -np.inf
    best_segments = initial_segment_boundaries

    def evaluate_segments(boundaries):
        nonlocal best_r2, best_segments
        
        # Create segment labels based on boundaries
        labels = pd.cut(y, bins=boundaries, labels=False, include_lowest=True)
        data['segment'] = labels
        
        # Check if each segment has the minimum required observations
        segment_counts = data['segment'].value_counts()
        if (segment_counts < min_obs_per_seg).any():
            return -np.inf  # Return a very low R² if any segment is too small
        
        # Train an XGBoost model and evaluate R²
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
        model = xgb.XGBRegressor(objective='reg:squarederror')
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        r2 = r2_score(y_test, predictions)
        
        if r2 > best_r2:
            best_r2 = r2
            best_segments = boundaries
        
        return r2

    # Optimization loop - this is a placeholder for a more sophisticated optimization approach
    for _ in range(100):  # Number of iterations, adjust based on needs
        new_boundaries = initial_segment_boundaries + np.random.normal(0, 0.1, size=initial_segment_boundaries.shape)
        new_boundaries = np.sort(np.clip(new_boundaries, y.min(), y.max()))
        evaluate_segments(new_boundaries)

    # Assign the best segments to the DataFrame
    data['segment'] = pd.cut(y, bins=best_segments, labels=False, include_lowest=True)

    # Plotting the target variable with segments
    plt.figure(figsize=(12, 6))
    plt.scatter(range(len(y)), y, c=data['segment'], cmap='viridis', label='Segments', alpha=0.6)
    plt.colorbar(label='Segment')
    plt.title(f'Target Variable Segmentation with R² = {best_r2:.4f}')
    plt.xlabel('Index')
    plt.ylabel(target_var)
    plt.grid(True)
    plt.legend()
    plt.show()

    return data, best_r2

# Example usage:
# df = pd.read_csv('your_data.csv')
# df_with_segments, best_r2 = optimize_segments(df, 'target_variable', num_segments=5, min_obs_per_seg=50)
# print("Data with segments:\n", df_with_segments.head())
# print("Best R² score:", best_r2)






import xgboost as xgb
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def plot_top_k_features(model, feature_names, k=5, title="Feature Importance"):
    # Get feature importances
    feature_importances = model.feature_importances_
    feature_importances_percentage = 100 * (feature_importances / feature_importances.sum())

    # Create a DataFrame for visualization
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importances_percentage
    }).sort_values(by='Importance', ascending=False).head(k)

    # Plot feature importances
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=importance_df)
    plt.xlabel('Importance (%)')
    plt.ylabel('Feature')
    plt.title(title)
    plt.show()

# Generate a sample regression dataset
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train_reg, y_train_reg)

# Plot top k feature importances for regression
plot_top_k_features(regressor, [f'feature_{i}' for i in range(X_reg.shape[1])], k=5, title="Top 5 Features for XGBoost Regression")

# Generate a sample classification dataset
X_clf, y_clf = make_classification(n_samples=1000, n_features=10, random_state=42)
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

# Train an XGBoost classification model
classifier = xgb.XGBClassifier()
classifier.fit(X_train_clf, y_train_clf)

# Plot top k feature importances for classification
plot_top_k_features(classifier, [f'feature_{i}' for i in range(X_clf.shape[1])], k=5, title="Top 5 Features for XGBoost Classification")

# Generate a sample dataset for RandomForestRegressor
X_rf, y_rf = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)
feature_names_rf = [f'feature_{i}' for i in range(X_rf.shape[1])]

# Train a RandomForestRegressor
model_rf = RandomForestRegressor(random_state=42)
model_rf.fit(X_rf, y_rf)

# Plot top k feature importances for RandomForestRegressor
plot_top_k_features(model_rf, feature_names_rf, k=5, title="Top 5 Features for RandomForestRegressor")





import xgboost as xgb
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# Generate a sample regression dataset
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train, y_train)

# Get feature importance
importance = regressor.get_booster().get_score(importance_type='weight')
importance = {k: v / sum(importance.values()) for k, v in importance.items()}  # Calculate percentages

# Sort importance by feature
sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
features = [x[0] for x in sorted_importance]
importance_values = [x[1] * 100 for x in sorted_importance]  # Convert to percentages

# Plot feature importance
fig, ax = plt.subplots()
ax.barh(features, importance_values)
ax.set_title("Feature Importance for XGBoost Regression (Percentage)")
ax.set_xlabel("Percentage")
ax.set_ylabel("Features")
plt.show()






import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample classification dataset
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost classification model
classifier = xgb.XGBClassifier()
classifier.fit(X_train, y_train)

# Plot feature importance
fig, ax = plt.subplots()
plot_importance(classifier, ax=ax)
ax.set_title("Feature Importance for XGBoost Classification")
plt.show()
import xgboost as xgb
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample regression dataset
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train, y_train)

# Plot feature importance
fig, ax = plt.subplots()
plot_importance(regressor, ax=ax)
ax.set_title("Feature Importance for XGBoost Regression")
plt.show()





import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, precision_recall_curve

# Generate a synthetic dataset with class imbalance
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,
                           n_redundant=10, n_clusters_per_class=1, n_classes=10,
                           weights=[0.1]*9 + [0.1], flip_y=0, random_state=42)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize results dictionary
results = {}

# Approach 1: SMOTE (Oversampling)
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
classifier_smote = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
classifier_smote.fit(X_train_smote, y_train_smote)
y_pred_smote = classifier_smote.predict(X_test)
results['SMOTE'] = classification_report(y_test, y_pred_smote, output_dict=True)

# Approach 2: Random Undersampling
rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)
classifier_rus = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
classifier_rus.fit(X_train_rus, y_train_rus)
y_pred_rus = classifier_rus.predict(X_test)
results['Random Undersampling'] = classification_report(y_test, y_pred_rus, output_dict=True)

# Approach 3: Class Weight Adjustment
class_weights = {i: len(y_train) / (10 * sum(y_train == i)) for i in np.unique(y_train)}
classifier_weighted = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', scale_pos_weight=class_weights)
classifier_weighted.fit(X_train, y_train)
y_pred_weighted = classifier_weighted.predict(X_test)
results['Class Weight Adjustment'] = classification_report(y_test, y_pred_weighted, output_dict=True)

# Approach 4: Threshold Moving
classifier_threshold = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
classifier_threshold.fit(X_train, y_train)
y_probs = classifier_threshold.predict_proba(X_test)
thresholds = {}
best_thresholds = {}

for i in range(10):
    precision, recall, thr = precision_recall_curve(y_test == i, y_probs[:, i])
    f1_scores = 2 * (precision * recall) / (precision + recall)
    best_threshold = thr[np.argmax(f1_scores)]
    thresholds[i] = best_threshold
    best_thresholds[i] = best_threshold

y_pred_adjusted = np.zeros_like(y_test)
for i in range(len(y_test)):
    class_probs = y_probs[i]
    class_probs_thresholded = (class_probs >= [thresholds[j] for j in range(10)]).astype(int)
    y_pred_adjusted[i] = np.argmax(class_probs_thresholded)

results['Threshold Moving'] = classification_report(y_test, y_pred_adjusted, output_dict=True)

# Print results
for approach, metrics in results.items():
    print(f"Approach: {approach}")
    print(pd.DataFrame(metrics).transpose())
    print("\n")

# Evaluate the models
import matplotlib.pyplot as plt

approaches = ['SMOTE', 'Random Undersampling', 'Class Weight Adjustment', 'Threshold Moving']
f1_scores = [results[approach]['weighted avg']['f1-score'] for approach in approaches]

plt.figure(figsize=(10, 6))
plt.bar(approaches, f1_scores, color=['blue', 'green', 'red', 'purple'])
plt.xlabel('Approaches')
plt.ylabel('F1-Score')
plt.title('Comparison of Approaches for Handling Imbalanced Data')
plt.show()






import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, precision_recall_curve

# Generate a synthetic dataset with class imbalance
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,
                           n_redundant=10, n_clusters_per_class=1, n_classes=10,
                           weights=[0.05]*9 + [0.55], flip_y=0, random_state=42)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to oversample the minority classes
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Calculate class weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)
class_weights_dict = dict(enumerate(class_weights))

# Train an XGBoost classifier with class weights
classifier = XGBClassifier(scale_pos_weight=class_weights_dict)
classifier.fit(X_train_res, y_train_res)

# Predict probabilities
y_probs = classifier.predict_proba(X_test)

# Example of adjusting the threshold for one class (class 1)
precision, recall, thresholds = precision_recall_curve(y_test == 1, y_probs[:, 1])
f1_scores = 2 * (precision * recall) / (precision + recall)
best_threshold = thresholds[np.argmax(f1_scores)]

# Apply the best threshold
y_pred_adjusted = (y_probs[:, 1] >= best_threshold).astype(int)

# Evaluate the model
print("Classification Report with Adjusted Threshold:")
print(classification_report(y_test, y_pred_adjusted))

# Standard classification report for comparison
y_pred = classifier.predict(X_test)
print("Standard Classification Report:")
print(classification_report(y_test, y_pred))




from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# Generate a synthetic imbalanced dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,
                           n_redundant=10, n_clusters_per_class=1,
                           weights=[0.99], flip_y=0, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Train an XGBoost classifier with balanced class weights
classifier = XGBClassifier(scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))
classifier.fit(X_train_res, y_train_res)

# Evaluate the model
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))






import xgboost as xgb
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample regression dataset
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost regression model
regressor = xgb.XGBRegressor()
regressor.fit(X_train, y_train)

# Plot feature importance
plot_importance(regressor)
plt.title("Feature Importance for XGBoost Regression")
plt.show()

import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
import matplotlib.pyplot as plt

# Generate a sample classification dataset
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost classification model
classifier = xgb.XGBClassifier()
classifier.fit(X_train, y_train)

# Plot feature importance
plot_importance(classifier)
plt.title("Feature Importance for XGBoost Classification")
plt.show()






import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

# Generate a sample dataset
X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)
feature_names = [f'feature_{i}' for i in range(X.shape[1])]
data = pd.DataFrame(X, columns=feature_names)
data['target'] = y

# Train a RandomForestRegressor
model = RandomForestRegressor(random_state=42)
model.fit(data[feature_names], data['target'])

# Get feature importances
feature_importances = model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance')
plt.show()





import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Sample data
data = pd.DataFrame({
    'x': range(10),
    'y1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'y2': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
    'y3': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
})

# Convert data to long format
data_melted = pd.melt(data, ['x'])

# Plot with seaborn
sns.lineplot(data=data_melted, x='x', y='value', hue='variable')

# Show plot
plt.show()





import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, f1_score, ConfusionMatrixDisplay
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Apply k-means clustering to segment the target variable
optimal_clusters = 3  # Example: Adjust based on your previous result
kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
df['Cluster'] = kmeans.fit_predict(df[['Target']])

# Combine the columns for stratification
df['Stratify'] = df['Age'].astype(str) + '_' + df['Income'].round(0).astype(str) + '_' + df['Category1'].astype(str) + '_' + df['Category2'].astype(str)

# Perform the train-test split with stratification
train_df, test_df = train_test_split(df, test_size=0.2, random_state=0, stratify=df['Stratify'])

# Drop the stratification column from the train and test sets
train_df = train_df.drop(columns=['Stratify']).reset_index(drop=True)
test_df = test_df.drop(columns=['Stratify']).reset_index(drop=True)

# Prepare training and testing sets
X_train = train_df.drop(columns=['Target', 'Cluster'])
y_train = train_df['Cluster']
X_test = test_df.drop(columns=['Target', 'Cluster'])
y_test = test_df['Cluster']

# Define classifiers and their hyperparameters
classifiers = {
    'XGBoost': XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='mlogloss'),
    'LightGBM': LGBMClassifier(random_state=0),
    'CatBoost': CatBoostClassifier(random_state=0, verbose=0)
}

param_grids = {
    'XGBoost': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]},
    'LightGBM': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'num_leaves': [31, 50]},
    'CatBoost': {'iterations': [50, 100], 'learning_rate': [0.01, 0.1], 'depth': [3, 5]}
}

# Train, evaluate and plot results for each classifier
results = {}

for clf_name, clf in classifiers.items():
    print(f"Training {clf_name}...")
    grid_search = GridSearchCV(clf, param_grids[clf_name], cv=5, scoring='f1_macro', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_clf = grid_search.best_estimator_
    y_pred = best_clf.predict(X_test)
    
    # Compute confusion matrix and F1 score
    conf_matrix = confusion_matrix(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    
    # Store results
    results[clf_name] = {
        'conf_matrix': conf_matrix,
        'f1_score': f1
    }
    
    # Plot confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=best_clf.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f'{clf_name} Confusion Matrix')
    plt.show()
    
    # Print F1 score
    print(f'{clf_name} F1 Score: {f1:.2f}')

# Plot F1 scores for all classifiers
f1_scores = {clf_name: res['f1_score'] for clf_name, res in results.items()}
plt.figure(figsize=(10, 6))
plt.bar(f1_scores.keys(), f1_scores.values())
plt.xlabel('Classifier')
plt.ylabel('F1 Score')
plt.title('F1 Scores for Different Classifiers')
plt.show()






import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Combine the columns for stratification
df['Stratify'] = df['Age'].astype(str) + '_' + df['Income'].round(0).astype(str) + '_' + df['Category1'].astype(str) + '_' + df['Category2'].astype(str)

# Perform the train-test split with stratification
train_df, test_df = train_test_split(df, test_size=0.2, random_state=0, stratify=df['Stratify'])

# Drop the stratification column from the train and test sets
train_df = train_df.drop(columns=['Stratify']).reset_index(drop=True)
test_df = test_df.drop(columns=['Stratify']).reset_index(drop=True)

# Check the distribution
print("Training set distribution:\n", train_df['Stratify'].value_counts(normalize=True))
print("\nTesting set distribution:\n", test_df['Stratify'].value_counts(normalize=True))

# Proceed with your further code (clustering, modeling, etc.)





import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Plot AIC and BIC for different number of clusters
n_clusters = range(1, 11)
aic_values = []
bic_values = []
silhouette_scores = []

for n in n_clusters:
    gmm = GaussianMixture(n_components=n, random_state=0)
    gmm.fit(df[['Target']])
    aic_values.append(gmm.aic(df[['Target']]))
    bic_values.append(gmm.bic(df[['Target']]))
    if n > 1:
        silhouette_scores.append(silhouette_score(df[['Target']], gmm.predict(df[['Target']])))

plt.figure(figsize=(12, 6))
plt.plot(n_clusters, aic_values, label='AIC')
plt.plot(n_clusters, bic_values, label='BIC')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('AIC and BIC for different number of clusters')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(n_clusters[1:], silhouette_scores, label='Silhouette Score')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('Silhouette Score for different number of clusters')
plt.legend()
plt.show()

# Choose the optimal number of clusters based on BIC (you can also choose based on AIC or silhouette score)
optimal_clusters = np.argmin(bic_values) + 1
print(f'Optimal number of clusters: {optimal_clusters}')

# Apply k-means clustering to segment the target variable
kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
df['Cluster'] = kmeans.fit_predict(df[['Target']])

# Separate data into clusters based on k-means clustering
clusters = [df[df['Cluster'] == i] for i in range(optimal_clusters)]

# Calculate and display intervals for each cluster
intervals = []
for cluster_id, cluster in enumerate(clusters):
    min_value = cluster['Target'].min()
    max_value = cluster['Target'].max()
    intervals.append((min_value, max_value))

# Sort intervals to ensure they are in order
intervals.sort()

# Adjust intervals to ensure no gaps
adjusted_intervals = []
for i, (min_value, max_value) in enumerate(intervals):
    if i == 0:
        adjusted_intervals.append((min_value, max_value))
    else:
        previous_max = adjusted_intervals[-1][1]
        adjusted_intervals.append((previous_max, max_value))

# Print adjusted intervals
for i, (min_value, max_value) in enumerate(adjusted_intervals):
    print(f'Cluster {i}: {min_value:.2f} to {max_value:.2f}')

# Define models to train
models = {
    'RandomForest': RandomForestRegressor(random_state=0),
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Huber': HuberRegressor()
}

param_grid = {
    'RandomForest': {'n_estimators': [50, 100], 'max_depth': [3, 5]},
    'Ridge': {'alpha': [0.01, 0.1, 1]},
    'Lasso': {'alpha': [0.01, 0.1, 1]},
    'Huber': {'epsilon': [1.35, 1.5]}
}

# Function to train and evaluate models
def train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid):
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    return r2

# Train and evaluate models for each cluster
results = {}
for cluster_id, cluster in enumerate(clusters):
    X = cluster.drop(columns=['Target', 'Cluster'])
    y = cluster['Target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    cluster_results = {}
    for model_name, model in models.items():
        r2 = train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid.get(model_name, {}))
        cluster_results[model_name] = r2
    results[f'Cluster {cluster_id}'] = cluster_results

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

# Plot R² scores for each model across different clusters
results_df.plot(kind='bar', figsize=(12, 6))
plt.xlabel('Cluster')
plt.ylabel('R² Score')
plt.title('R² Scores for Different Models across Clusters')
plt.legend()
plt.show()











import xgboost as xgb
from sklearn.metrics import r2_score
import itertools

# Define the parameter grid
etas = [0.01, 0.1, 0.3]
max_depths = [2, 4, 6]
subsamples = [0.8, 1.0]
colsample_bytree = [1.0, 0.8]

# List to store results
results = []

# Iterate over all parameter combinations
for eta, max_depth, subsample, colsample in itertools.product(etas, max_depths, subsamples, colsample_bytree):
    xgb_params = {
        'eta': eta,
        'max_depth': max_depth,
        'subsample': subsample,
        'colsample_bytree': colsample,
        'objective': 'reg:squarederror',
        'seed': 1989
    }
    
    xgb_cv = xgb.cv(
        params=xgb_params,
        dtrain=train_xgb,
        num_boost_round=10000,
        nfold=5,
        early_stopping_rounds=10,
        verbose_eval=0
    )
    
    # Extract the best number of boosting rounds
    best_num_boost_rounds = len(xgb_cv['train-rmse-mean'])
    
    # Train the model with the best number of boosting rounds
    model = xgb.train(
        params=xgb_params,
        dtrain=train_xgb,
        num_boost_round=best_num_boost_rounds
    )
    
    # Make predictions
    y_pred = model.predict(test_xgb)
    
    # Evaluate using R2 score
    r2 = r2_score(y_test, y_pred)
    
    # Store the results
    results.append({
        'eta': eta,
        'max_depth': max_depth,
        'subsample': subsample,
        'colsample_bytree': colsample,
        'best_num_boost_rounds': best_num_boost_rounds,
        'r2': r2
    })

# Find the best result
best_result = max(results, key=lambda x: x['r2'])

# Print the best parameters and corresponding R2 score
print("Best parameters found: ", best_result)
print("Best R2 score: ", best_result['r2'])











import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Age': np.random.randint(18, 70, size=100),
    'Income': np.random.uniform(30000, 100000, size=100),
    'Category1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['Category1'] = df['Category1'].astype(str)
df['Category2'] = df['Category2'].astype(str)

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Plot AIC and BIC for different number of clusters
n_clusters = range(1, 11)
aic_values = []
bic_values = []
silhouette_scores = []

for n in n_clusters:
    gmm = GaussianMixture(n_components=n, random_state=0)
    gmm.fit(df[['Target']])
    aic_values.append(gmm.aic(df[['Target']]))
    bic_values.append(gmm.bic(df[['Target']]))
    if n > 1:
        silhouette_scores.append(silhouette_score(df[['Target']], gmm.predict(df[['Target']])))

plt.figure(figsize=(12, 6))
plt.plot(n_clusters, aic_values, label='AIC')
plt.plot(n_clusters, bic_values, label='BIC')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('AIC and BIC for different number of clusters')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(n_clusters[1:], silhouette_scores, label='Silhouette Score')
plt.xlabel('Number of clusters')
plt.ylabel('Score')
plt.title('Silhouette Score for different number of clusters')
plt.legend()
plt.show()

# Choose the optimal number of clusters based on BIC (you can also choose based on AIC or silhouette score)
optimal_clusters = np.argmin(bic_values) + 1
print(f'Optimal number of clusters: {optimal_clusters}')

# Apply k-means clustering to segment the target variable
kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
df['Cluster'] = kmeans.fit_predict(df[['Target']])

# Separate data into clusters based on k-means clustering
clusters = [df[df['Cluster'] == i] for i in range(optimal_clusters)]

# Calculate and display intervals for each cluster
intervals = {}
for cluster_id, cluster in enumerate(clusters):
    min_value = cluster['Target'].min()
    max_value = cluster['Target'].max()
    intervals[f'Cluster {cluster_id}'] = (min_value, max_value)
    print(f'Cluster {cluster_id}: {min_value:.2f} to {max_value:.2f}')

# Define models to train
models = {
    'RandomForest': RandomForestRegressor(random_state=0),
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Huber': HuberRegressor()
}

param_grid = {
    'RandomForest': {'n_estimators': [50, 100], 'max_depth': [3, 5]},
    'Ridge': {'alpha': [0.01, 0.1, 1]},
    'Lasso': {'alpha': [0.01, 0.1, 1]},
    'Huber': {'epsilon': [1.35, 1.5]}
}

# Function to train and evaluate models
def train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid):
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    return r2

# Train and evaluate models for each cluster
results = {}
for cluster_id, cluster in enumerate(clusters):
    X = cluster.drop(columns=['Target', 'Cluster'])
    y = cluster['Target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    cluster_results = {}
    for model_name, model in models.items():
        r2 = train_evaluate_models(X_train, X_test, y_train, y_test, model, param_grid.get(model_name, {}))
        cluster_results[model_name] = r2
    results[f'Cluster {cluster_id}'] = cluster_results

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

# Plot R² scores for each model across different clusters
results_df.plot(kind='bar', figsize=(12, 6))
plt.xlabel('Cluster')
plt.ylabel('R² Score')
plt.title('R² Scores for Different Models across Clusters')
plt.legend()
plt.show()














import pandas as pd
import numpy as np

# Load your dataset
# Replace 'your_dataset.csv' with the path to your dataset and 'target' with the target variable name
df = pd.read_csv('your_dataset.csv')

# Define the target variable
target = 'your_target_variable'

# Calculate the IQR (Interquartile Range) to identify outliers
Q1 = df[target].quantile(0.25)
Q3 = df[target].quantile(0.75)
IQR = Q3 - Q1

# Define the bounds for identifying outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Create the first dataset with outliers and negative target values
outliers_negative = df[(df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0)]

# Create the second dataset with the rest
non_outliers = df[~((df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0))]

# Save the datasets to CSV files (optional)
outliers_negative.to_csv('outliers_negative.csv', index=False)
non_outliers.to_csv('non_outliers.csv', index=False)

# Display the datasets
print("Outliers and Negative Target Values Dataset")
print(outliers_negative.head())

print("\nNon-Outliers Dataset")
print(non_outliers.head())



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Feature': np.random.randn(100) * 20,
    'Target': np.random.uniform(0, 100, size=100)
})

# Split the data into features and target
X = df[['Feature']]
y = df['Target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, return_train_score=True)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()

# Display results in a dataframe
results = {
    'MSE Train': mse_train,
    'MSE Test': mse_test,
    'R^2 Train': r2_train,
    'R^2 Test': r2_test
}
results_df = pd.DataFrame([results])
print(results_df)














import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Feature': np.random.randn(100) * 20,
    'Target': np.random.uniform(0, 100, size=100)
})

# Split the data into features and target
X = df[['Feature']]
y = df['Target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, return_train_score=True)
grid_search.fit(X_train, y_train)

# Get the results in a DataFrame
results = pd.DataFrame(grid_search.cv_results_)

# Calculate RMSE for train and test sets
results['mean_train_rmse'] = np.sqrt(-results['mean_train_score'])
results['mean_test_rmse'] = np.sqrt(-results['mean_test_score'])

# Plot R² scores
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
for learning_rate in param_grid['learning_rate']:
    subset = results[results.param_learning_rate == learning_rate]
    plt.plot(subset['param_n_estimators'], subset['mean_train_score'], label=f'Train - lr={learning_rate}')
    plt.plot(subset['param_n_estimators'], subset['mean_test_score'], linestyle='--', label=f'Test - lr={learning_rate}')
plt.xlabel('Number of Estimators')
plt.ylabel('Neg MSE')
plt.title('Neg MSE vs Number of Estimators')
plt.legend()

# Plot RMSE scores
plt.subplot(1, 2, 2)
for learning_rate in param_grid['learning_rate']:
    subset = results[results.param_learning_rate == learning_rate]
    plt.plot(subset['param_n_estimators'], subset['mean_train_rmse'], label=f'Train - lr={learning_rate}')
    plt.plot(subset['param_n_estimators'], subset['mean_test_rmse'], linestyle='--', label=f'Test - lr={learning_rate}')
plt.xlabel('Number of Estimators')
plt.ylabel('RMSE')
plt.title('RMSE vs Number of Estimators')
plt.legend()

plt.tight_layout()
plt.show()

# Print the best parameters
print("Best Parameters:", grid_search.best_params_)
print("Best R² Score:", grid_search.best_score_)








import xgboost as xgb
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, make_scorer
import pandas as pd
import matplotlib.pyplot as plt

# Sample data preparation (assuming you have your dataset as X and y)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define XGBoost regressor
xgb_reg = xgb.XGBRegressor()

# Define parameter grid
param_grid = {
    'eta': [0.01, 0.02, 0.03],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300]
}

# Define R^2 scorer
r2_scorer = make_scorer(r2_score)

# Perform GridSearchCV
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, scoring=r2_scorer, cv=5, return_train_score=True)
grid_search.fit(X_train, y_train)

# Extract results
results = pd.DataFrame(grid_search.cv_results_)

# Plotting the evolution of R^2
plt.figure(figsize=(14, 7))

plt.plot(results['param_eta'], results['mean_train_score'], label='Train R^2')
plt.plot(results['param_eta'], results['mean_test_score'], label='Test R^2')

plt.xlabel('Learning Rate (eta)')
plt.ylabel('R^2 Score')
plt.title('Evolution of R^2 for different Learning Rates')
plt.legend()
plt.grid(True)
plt.show()






import xgboost as xgb
import pandas as pd
from sklearn.metrics import r2_score

# Define XGBoost parameters
xgb_para = {
    'eta': 0.02,
    'booster': 'gbtree',
    'max_depth': 10
}

# Perform cross-validation
xgb_cv = xgb.cv(
    params=xgb_para,
    dtrain=train_xgb,
    num_boost_round=500,
    nfold=5,
    metrics={'rmse'},  # RMSE is used for early stopping
    as_pandas=True,
    seed=42
)

# Extract the train and test predictions from the cross-validation
train_predictions = xgb_cv['train-rmse-mean']
test_predictions = xgb_cv['test-rmse-mean']

# Calculate R^2 for train and test
train_r2 = [r2_score(train_xgb.get_label(), train_predictions)]
test_r2 = [r2_score(test_xgb.get_label(), test_predictions)]

# Convert R^2 scores to a DataFrame for plotting
r2_df = pd.DataFrame({
    'train-r2': train_r2,
    'test-r2': test_r2
})

# Plot R^2 scores
r2_df.plot()










import xgboost as xgb
from sklearn.model_selection import GridSearchCV
import pandas as pd

# Define the parameter grid
param_grid = {
    'eta': [0.01, 0.02, 0.1],
    'booster': ['gbtree', 'gblinear'],
    'max_depth': [6, 10, 15]
}

# Convert your training data to DMatrix format
dtrain = xgb.DMatrix(data=train_data, label=train_labels)

# Define the model
xgb_model = xgb.XGBRegressor()

# Perform the grid search
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search
grid_search.fit(train_data, train_labels)

# Get the results
cv_results = pd.DataFrame(grid_search.cv_results_)

# Print the best parameters
print("Best parameters:", grid_search.best_params_)

# Plot the results
cv_results[['mean_test_score', 'std_test_score']].plot()
















Rapport sur la Valeur de R² et son Interprétation Statistique
Introduction
La valeur de R², ou coefficient de détermination, est souvent mal comprise. En particulier, beaucoup de gens pensent que R² ne peut être qu'entre 0 et 1. Cependant, il peut être négatif dans certains cas. Ce rapport explique pourquoi et comment cela peut se produire, avec une explication détaillée des calculs de R² et une illustration pratique.

Calcul de R²
Pour comprendre pourquoi R² peut être négatif, nous devons examiner comment il est calculé. Nous utilisons trois variables clés dans ce calcul : RSS (Residual Sum of Squares), TSS (Total Sum of Squares), et ESS (Explained Sum of Squares).

Calcul de RSS :
Pour chaque variable indépendante 
𝑥
x, nous avons une variable dépendante 
𝑦
y. Nous traçons une ligne de régression linéaire qui prédit les valeurs de 
𝑦
y pour chaque valeur de 
𝑥
x. Appelons les valeurs prédites 
𝑦
^
y
^
​
 . L'erreur entre ce que la ligne prédit et les valeurs réelles de 
𝑦
y est calculée par soustraction. Toutes ces différences sont mises au carré et additionnées, ce qui donne la somme des carrés des résidus, RSS.

𝑅
𝑆
𝑆
=
∑
(
𝑦
−
𝑦
^
)
2
RSS=∑(y− 
y
^
​
 ) 
2
 
Calcul de TSS :
Nous pouvons calculer la valeur moyenne de 
𝑦
y, appelée 
𝑦
ˉ
y
ˉ
​
 . Si nous traçons 
𝑦
ˉ
y
ˉ
​
 , c'est simplement une ligne horizontale à travers les données. En soustrayant 
𝑦
ˉ
y
ˉ
​
  de chaque valeur réelle de 
𝑦
y, nous obtenons la somme totale des carrés, TSS.

𝑇
𝑆
𝑆
=
∑
(
𝑦
−
𝑦
ˉ
)
2
TSS=∑(y− 
y
ˉ
​
 ) 
2
 
Calcul de ESS :
Les différences entre les valeurs prédites 
𝑦
^
y
^
​
  et la valeur moyenne 
𝑦
ˉ
y
ˉ
​
  sont mises au carré et additionnées. Ceci est la somme expliquée des carrés, ESS.

𝐸
𝑆
𝑆
=
∑
(
𝑦
^
−
𝑦
ˉ
)
2
ESS=∑( 
y
^
​
 − 
y
ˉ
​
 ) 
2
 
Relation entre TSS, RSS et ESS
Lorsque nous avons une ligne de régression avec une interception, la relation suivante est toujours vraie :

𝑇
𝑆
𝑆
=
𝑅
𝑆
𝑆
+
𝐸
𝑆
𝑆
TSS=RSS+ESS
En divisant tous les termes par TSS et en réarrangeant, nous obtenons :

𝑅
2
=
1
−
𝑅
𝑆
𝑆
𝑇
𝑆
𝑆
R 
2
 =1− 
TSS
RSS
​
 
Dans ce cas, R² est toujours positif.

Cas où R² peut être négatif
Cependant, sans interception, la relation ci-dessus change. La formule devient :

𝑇
𝑆
𝑆
=
𝑅
𝑆
𝑆
+
𝐸
𝑆
𝑆
+
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
TSS=RSS+ESS+2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 )
En divisant tous les termes par TSS, nous obtenons :

𝑅
2
=
𝐸
𝑆
𝑆
+
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
𝑇
𝑆
𝑆
R 
2
 = 
TSS
ESS+2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 )
​
 
Le terme supplémentaire peut rendre le numérateur négatif, ce qui fait que R² peut être négatif. Cela se produit lorsque la ligne horizontale 
𝑦
ˉ
y
ˉ
​
  explique mieux les données que la ligne de régression.

Quand le terme 
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 ) est nul ou non nul
Nul : Ce terme est nul lorsque la ligne de régression passe par le point moyen des données 
(
𝑥
ˉ
,
𝑦
ˉ
)
( 
x
ˉ
 , 
y
ˉ
​
 ), ce qui se produit lorsque le modèle inclut une interception. Dans ce cas, les erreurs 
(
𝑦
−
𝑦
^
)
(y− 
y
^
​
 ) et les différences 
(
𝑦
^
−
𝑦
ˉ
)
( 
y
^
​
 − 
y
ˉ
​
 ) sont orthogonales, ce qui signifie que leur produit est en moyenne nul.
Non nul : Ce terme n'est pas nécessairement nul lorsque la régression ne comprend pas d'interception. Dans ce cas, la ligne de régression peut ne pas passer par le point moyen des données, et les erreurs et les différences peuvent avoir une covariance non nulle, conduisant ainsi à un terme supplémentaire qui peut être positif ou négatif.
Exemple Simple
Considérons un exemple simple pour illustrer ce concept. Supposons que nous ayons les données suivantes :

x	y
1	1
2	2
3	1.3
4	3.75
5	2.25
Calculons la régression linéaire avec et sans interception :

Régression avec interception :
La ligne de régression a une interception et la formule obtenue est :

𝑦
^
=
0.425
+
0.475
𝑥
y
^
​
 =0.425+0.475x
Régression sans interception :
La ligne de régression passe par l'origine (sans interception) et la formule obtenue est :

𝑦
^
=
0.63
𝑥
y
^
​
 =0.63x
La moyenne des 
𝑦
y est 
𝑦
ˉ
=
2.06
y
ˉ
​
 =2.06.

En traçant les lignes de régression et la moyenne de 
𝑦
y sur un graphique, nous observons que la ligne de régression sans interception ne passe pas par la moyenne des données, ce qui peut rendre le terme 
2
∑
(
𝑦
−
𝑦
^
)
(
𝑦
^
−
𝑦
ˉ
)
2∑(y− 
y
^
​
 )( 
y
^
​
 − 
y
ˉ
​
 ) non nul et potentiellement négatif, rendant ainsi 
𝑅
2
R 
2
  négatif.

Conclusion
La valeur de R² peut être négative si le modèle de régression n'a pas d'interception et s'adapte très mal aux données. Dans ce cas, une ligne horizontale pourrait fournir une meilleure explication des variations dans les données que le modèle lui-même. Comprendre cette nuance est crucial pour une interprétation correcte des résultats de régression linéaire.

Visualisation avec Matplotlib
Voici un code Python qui illustre cet exemple :

python
Copy code
import matplotlib.pyplot as plt
import numpy as np

# Données simples
x = np.array([1, 2, 3, 4, 5])
y = np.array([1, 2, 1.3, 3.75, 2.25])

# Calcul de la régression linéaire avec interception
coeffs_with_intercept = np.polyfit(x, y, 1)
y_hat_with_intercept = np.polyval(coeffs_with_intercept, x)

# Calcul de la régression linéaire sans interception
coeffs_without_intercept = np.polyfit(x, y, 1, full=True)[0]
y_hat_without_intercept = np.polyval([coeffs_without_intercept[0]], x)

# Calcul de la moyenne de y
y_bar = np.mean(y)

# Visualisation des données et des lignes de régression
plt.scatter(x, y, label='Données')
plt.plot(x, y_hat_with_intercept, label='Régression avec interception', color='green')
plt.plot(x, y_hat_without_intercept, label='Régression sans interception', color='red')
plt.axhline(y=y_bar, color='blue', linestyle='--', label='Moyenne de y')

plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Régression Linéaire avec et sans Interception')
plt.show()
Ce graphique montre les données avec deux lignes de régression : une avec interception (en vert) et une sans interception (en rouge), ainsi qu'une ligne horizontale représentant la moyenne de 
𝑦
y (en bleu). La ligne rouge ne passe pas par le point moyen des données, ce qui explique pourquoi 
𝑅
2
R 
2
  peut être négatif dans ce cas.





import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate data that follows the line y = x
np.random.seed(0)
X = np.linspace(-10, 10, 100).reshape(-1, 1)
y = X.flatten()  # y = x

# Fit a linear regression model with the equation y = -x
model = LinearRegression()
model.fit(X, -X)  # The model is trained with y = -x

# Predict using the fitted model
y_pred = model.predict(X)

# Compute R^2 value
r2 = r2_score(y, y_pred)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='black', label='Data: y = x')
plt.plot(X, y_pred, color='red', linewidth=2, label='Model: y = -x')
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f'Linear Regression with y = -x\nR² = {r2:.2f}')
plt.legend()
plt.grid(True)
plt.show()

print(f"R²: {r2:.2f}")












import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate non-linear data
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(80) * 0.5

# Fit linear model
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

# Calculate R^2
r2 = r2_score(y, y_pred)
print(f"R^2: {r2}")

# Plot
plt.scatter(X, y, color='black')
plt.plot(X, y_pred, color='blue', linewidth=3)
plt.title(f'Linear Regression on Non-linear Data\nR^2 = {r2:.2f}')
plt.show()


from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit model on training data
model.fit(X_train, y_train)
y_test_pred = model.predict(X_test)

# Calculate R^2 on test data
r2_test = r2_score(y_test, y_test_pred)
print(f"R^2 on test data: {r2_test}")
















import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
quantiles = df[df['Outlier'] == 'Inlier']['Value'].quantile([0.05, 0.1, 0.6, 0.7, 0.8, 0.9])
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()

# Replace outliers based on specified rules
def replace_outliers(row, quantiles, mean_value):
    noise = np.random.normal(0, 1)
    if row['Outlier'] == 'Outlier':
        if row['Value'] > mean_value:
            if row['Value'] > quantiles[0.9]:
                return quantiles[0.7] + noise
            elif row['Value'] > quantiles[0.8]:
                return quantiles[0.6] + noise
        else:
            return quantiles[0.1] + noise if row['Value'] < quantiles[0.05] else quantiles[0.05] + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, quantiles, mean_value), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'min_child_weight': [1, 5, 10]
}

# Initialize and train XGBoost model with GridSearchCV
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict and evaluate the best model
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'Best Parameters: {grid_search.best_params_}')
print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model with enable_categorical set to True
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0, enable_categorical=False)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()









import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())












import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor, plot_tree
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree and plot the tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group

        # Plot the Decision Tree
        plt.figure(figsize=(12, 8))
        plot_tree(dt_reg, feature_names=[cat_var], filled=True, rounded=True, fontsize=10)
        plt.title(f'Decision Tree for {cat_var}')
        plt.show()
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()









import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()
