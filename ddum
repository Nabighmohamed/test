import numpy as np
import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Load the Boston housing dataset
boston = load_boston()
X = pd.DataFrame(boston.data, columns=boston.feature_names)
y = boston.target

# Remove rows with missing values
X = X.dropna()
y = y[:len(X)]  # Ensure the target variable matches the dimensions of X after dropping rows

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Train a Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Train an XGBoost Regressor
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)

# Evaluate the models
def evaluate_model(model, X_test, y_test, model_name):
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    print(f"{model_name} Performance:")
    print(f"Mean Squared Error: {mse}")
    print(f"R^2 Score: {r2}")
    print()

evaluate_model(rf_model, X_test, y_test, "Random Forest")
evaluate_model(lr_model, X_test, y_test, "Linear Regression")
evaluate_model(xgb_model, X_test, y_test, "XGBoost")

# Use SHAP to explain the Random Forest model
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Dependence plot for a specific feature (e.g., 'RM' - average number of rooms per dwelling)
shap.dependence_plot('RM', shap_values, X_test)

# Force plot for the first instance in the test set
shap.force_plot(explainer.expected_value, shap_values[0, :], X_test.iloc[0, :], matplotlib=True)
plt.show()












import numpy as np
import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# Load the Boston housing dataset
boston = load_boston()
X = pd.DataFrame(boston.data, columns=boston.feature_names)
y = boston.target

# Remove rows with missing values
X = X.dropna()
y = y[:len(X)]  # Ensure the target variable matches the dimensions of X after dropping rows

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Create a SHAP explainer object
explainer = shap.TreeExplainer(model)

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Dependence plot for a specific feature (e.g., 'RM' - average number of rooms per dwelling)
shap.dependence_plot('RM', shap_values, X_test)

# Force plot for the first instance in the test set
shap.force_plot(explainer.expected_value, shap_values[0, :], X_test.iloc[0, :], matplotlib=True)
plt.show()



Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable. Understanding feature importance can help in feature selection, improving the model, and providing insights into the data. Here's an explanation of how feature importance works, focusing on a few common methods:

1. Model-Based Feature Importance
Many machine learning models provide inherent ways to assess feature importance. Here are some popular model-based methods:

a. Decision Trees and Ensembles (e.g., Random Forest, Gradient Boosting)
How it works: These models split the data based on features to reduce impurity (like Gini impurity or entropy in classification trees).
Feature importance: The importance of a feature is usually calculated as the total reduction of the criterion (e.g., Gini impurity) brought by that feature, weighted by the number of samples it splits. In ensemble methods like Random Forest or Gradient Boosting, feature importance is averaged over all the trees.
Usage: Helps identify which features contribute the most to the predictive power of the model.
b. Linear Models (e.g., Linear Regression, Logistic Regression)
How it works: These models assume a linear relationship between the features and the target variable.
Feature importance: In linear models, the absolute value of the coefficients can be used to gauge feature importance. Larger coefficients in absolute terms indicate more important features.
Usage: Useful for models where interpretability of coefficients is crucial.
2. Permutation Feature Importance
How it works: This method measures the increase in the model's prediction error after permuting the feature’s values, which breaks the relationship between the feature and the target.
Steps:
Train the model on the original dataset.
Measure the baseline performance metric (e.g., accuracy, RMSE).
For each feature, shuffle its values and predict the target using the model.
Measure the performance metric again.
The change in the performance metric indicates the importance of the feature.
Usage: This method is model-agnostic and can be applied to any model, providing a straightforward way to assess feature importance.
3. SHAP (SHapley Additive exPlanations) Values
How it works: SHAP values come from cooperative game theory and provide a way to fairly distribute the "payout" (prediction) among the features.
Steps:
Calculate the contribution of each feature to every possible combination of features.
Average these contributions to get the SHAP value for each feature.
Feature importance: The mean absolute SHAP value of a feature across all predictions indicates its importance.
Usage: SHAP values offer a detailed and consistent method for interpreting individual predictions and overall feature importance.
Example
Let's say we have a dataset with features like age, income, and education level, and we want to predict whether a person will default on a loan. Here’s how we might determine feature importance:

Using a Random Forest:

Train a Random Forest model.
Extract feature importance scores from the model.
Find that income has the highest importance score, indicating it is the most influential feature.
Using Permutation Importance:

Train the model and measure its baseline accuracy.
Shuffle the values of the income feature and measure the accuracy drop.
A significant drop in accuracy suggests that income is important.
Using SHAP Values:

Train the model and calculate SHAP values for each feature.
Plot the SHAP values to see that income consistently has high values, indicating strong influence on predictions.
Conclusion
Feature importance is a powerful concept in machine learning, enabling better model understanding, feature selection, and data insights. Depending on the model and the goal, different methods can be employed to determine which features are most influential in making accurate predictions.








import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.inspection import permutation_importance
from sklearn.metrics import r2_score

# Sample DataFrame (replace with your actual dataset)
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Preprocess data: Drop columns with missing values and rows where target is missing
df.drop(columns=['Feature_2'], inplace=True)
df.dropna(subset=['Target'], inplace=True)

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Automatically detect numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

# Define preprocessor
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models = [
    ('Random Forest', RandomForestRegressor()),
    ('Gradient Boosting', GradientBoostingRegressor()),
    ('Lasso', Lasso()),
    ('Ridge', Ridge())
]

# Initialize DataFrame to store feature importances
feature_importance_df = pd.DataFrame()

# Evaluate feature importances
for name, model in models:
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)

    # Retrieve feature names after fitting the pipeline
    numeric_feature_names = pipeline.named_steps['preprocessor'].transformers_[0][1].named_steps['scaler'].get_feature_names_out(numeric_features).tolist()
    categorical_feature_names = pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()
    feature_names = numeric_feature_names + categorical_feature_names

    # Feature importances
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importances = model.coef_
    else:
        importances = permutation_importance(pipeline, X_test, y_test, n_repeats=10, random_state=42).importances_mean

    # Create DataFrame of importances
    importance_df = pd.DataFrame({'Feature': feature_names, f'{name} Importance': importances})
    feature_importance_df = pd.concat([feature_importance_df, importance_df.set_index('Feature')], axis=1)

# Print top 10 features per metric
for col in feature_importance_df.columns:
    print(f"\nTop 10 features for {col}:")
    print(feature_importance_df[col].sort_values(ascending=False).head(10))















import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import r2_score

# Sample DataFrame (replace with your actual dataset)
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Preprocess data
df.drop(columns=['Feature_2'], inplace=True)  # Drop columns with missing values other than target
df.dropna(subset=['Target'], inplace=True)    # Drop rows where target is missing

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Define preprocessor
# Automatically detect numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

# Define preprocessor
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models = [
    ('Linear Regression', LinearRegression()),
    ('Ridge Regression', Ridge()),
    ('Lasso Regression', Lasso()),
    ('ElasticNet', ElasticNet()),
    ('Decision Tree', DecisionTreeRegressor()),
    ('Random Forest', RandomForestRegressor()),
    ('Gradient Boosting', GradientBoostingRegressor()),
    ('SVR', SVR()),
    ('KNeighbors', KNeighborsRegressor())
]

# Evaluate models
best_model = None
best_score = -np.inf
best_model_name = ""
for name, model in models:
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    score = r2_score(y_test, y_pred)
    print(f"{name}: R-squared = {score:.4f}")
    if score > best_score:
        best_score = score
        best_model = pipeline
        best_model_name = name

print(f"\nBest model: {best_model_name} with R-squared = {best_score:.4f}")

# Print the summary of the best model
if best_model is not None:
    print("\nSummary of the best model:")
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)
    print(f"Training R-squared: {r2_score(y_train, y_pred_train):.4f}")
    print(f"Test R-squared: {r2_score(y_test, y_pred_test):.4f}")


























import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# Sample DataFrame
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Select the target variable with missing values
target_variable = 'Target'

# Drop other variables with missing values
df = df.drop(columns=['Feature_2'])

# Create dataset for training (without missing target) and predicting (with missing target)
train_data = df.dropna(subset=[target_variable])
predict_data = df[df[target_variable].isnull()]

# Split features and target
X_train = train_data.drop(columns=[target_variable])
y_train = train_data[target_variable]

# One-hot encode categorical variables
X_train = pd.get_dummies(X_train, drop_first=True)
predict_features = pd.get_dummies(predict_data.drop(columns=[target_variable]), drop_first=True)

# Align columns of prediction set to match the training set
predict_features = predict_features.reindex(columns=X_train.columns, fill_value=0)

# Train the regression model
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predict missing values
y_pred = regressor.predict(predict_features)
df.loc[df[target_variable].isnull(), target_variable] = y_pred

# Model summary using statsmodels
X_train_sm = sm.add_constant(X_train)  # Add constant term for intercept
model = sm.OLS(y_train, X_train_sm)
results = model.fit()

# Print the summary
print(results.summary())

# Print the DataFrame with imputed values
print("\nDataFrame with Imputed Values:\n", df)














































import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame (replace with your actual dataset)
data = {
    'Category_1': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Category_2': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z', 'X'],
    'Value': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    'Target': [15.1, 25.3, 35.2, 45.5, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Value']),
        ('cat', OneHotEncoder(), ['Category_1', 'Category_2'])
    ])

# Define the model pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))
])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Model MSE: {mse:.4f}')
print(f'Model R-squared: {r2:.4f}')
