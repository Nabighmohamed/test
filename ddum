import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.inspection import permutation_importance
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Sample DataFrame (replace with your actual dataset)
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Preprocess data
df.drop(columns=['Feature_2'], inplace=True)  # Drop columns with missing values other than target
df.dropna(subset=['Target'], inplace=True)    # Drop rows where target is missing

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Automatically detect numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

# Define preprocessor
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models = [
    ('Random Forest', RandomForestRegressor()),
    ('Gradient Boosting', GradientBoostingRegressor()),
    ('Lasso', Lasso()),
    ('Ridge', Ridge())
]

# Initialize DataFrame to store feature importances
feature_importance_df = pd.DataFrame()

# Evaluate feature importances
for name, model in models:
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)

    # Feature importances
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importances = model.coef_
    else:
        importances = permutation_importance(pipeline, X_test, y_test, n_repeats=10, random_state=42).importances_mean

    # Create DataFrame of importances
    feature_names = pipeline.named_steps['preprocessor'].transformers_[0][1].named_steps['scaler'].get_feature_names_out(numeric_features).tolist()
    feature_names += pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()
    importance_df = pd.DataFrame({'Feature': feature_names, f'{name} Importance': importances})
    feature_importance_df = pd.concat([feature_importance_df, importance_df.set_index('Feature')], axis=1)

# Print top 10 features per metric
for col in feature_importance_df.columns:
    print(f"\nTop 10 features for {col}:")
    print(feature_importance_df[col].sort_values(ascending=False).head(10))














import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import r2_score

# Sample DataFrame (replace with your actual dataset)
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Preprocess data
df.drop(columns=['Feature_2'], inplace=True)  # Drop columns with missing values other than target
df.dropna(subset=['Target'], inplace=True)    # Drop rows where target is missing

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Define preprocessor
# Automatically detect numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

# Define preprocessor
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models = [
    ('Linear Regression', LinearRegression()),
    ('Ridge Regression', Ridge()),
    ('Lasso Regression', Lasso()),
    ('ElasticNet', ElasticNet()),
    ('Decision Tree', DecisionTreeRegressor()),
    ('Random Forest', RandomForestRegressor()),
    ('Gradient Boosting', GradientBoostingRegressor()),
    ('SVR', SVR()),
    ('KNeighbors', KNeighborsRegressor())
]

# Evaluate models
best_model = None
best_score = -np.inf
best_model_name = ""
for name, model in models:
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    score = r2_score(y_test, y_pred)
    print(f"{name}: R-squared = {score:.4f}")
    if score > best_score:
        best_score = score
        best_model = pipeline
        best_model_name = name

print(f"\nBest model: {best_model_name} with R-squared = {best_score:.4f}")

# Print the summary of the best model
if best_model is not None:
    print("\nSummary of the best model:")
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)
    print(f"Training R-squared: {r2_score(y_train, y_pred_train):.4f}")
    print(f"Test R-squared: {r2_score(y_test, y_pred_test):.4f}")


























import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# Sample DataFrame
data = {
    'Feature_1': [1, 2, 3, 4, 5, 6, 7, 8, np.nan, 10],
    'Feature_2': [1.1, 2.2, np.nan, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],
    'Feature_3': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Target': [15.1, 25.3, 35.2, np.nan, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Select the target variable with missing values
target_variable = 'Target'

# Drop other variables with missing values
df = df.drop(columns=['Feature_2'])

# Create dataset for training (without missing target) and predicting (with missing target)
train_data = df.dropna(subset=[target_variable])
predict_data = df[df[target_variable].isnull()]

# Split features and target
X_train = train_data.drop(columns=[target_variable])
y_train = train_data[target_variable]

# One-hot encode categorical variables
X_train = pd.get_dummies(X_train, drop_first=True)
predict_features = pd.get_dummies(predict_data.drop(columns=[target_variable]), drop_first=True)

# Align columns of prediction set to match the training set
predict_features = predict_features.reindex(columns=X_train.columns, fill_value=0)

# Train the regression model
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predict missing values
y_pred = regressor.predict(predict_features)
df.loc[df[target_variable].isnull(), target_variable] = y_pred

# Model summary using statsmodels
X_train_sm = sm.add_constant(X_train)  # Add constant term for intercept
model = sm.OLS(y_train, X_train_sm)
results = model.fit()

# Print the summary
print(results.summary())

# Print the DataFrame with imputed values
print("\nDataFrame with Imputed Values:\n", df)














































import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame (replace with your actual dataset)
data = {
    'Category_1': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],
    'Category_2': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z', 'X'],
    'Value': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    'Target': [15.1, 25.3, 35.2, 45.5, 55.4, 65.7, 75.3, 85.1, 95.0, 105.6]
}
df = pd.DataFrame(data)

# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Value']),
        ('cat', OneHotEncoder(), ['Category_1', 'Category_2'])
    ])

# Define the model pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))
])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Model MSE: {mse:.4f}')
print(f'Model R-squared: {r2:.4f}')
