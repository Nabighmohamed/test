import pandas as pd

# Sample data
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 1, 2, 3],
    'month': [1, 1, 2, 2, 2],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df2 = pd.DataFrame({
    'cclikpi': [2, 3, 2, 3, 4],
    'month': [1, 1, 2, 2, 2],
    'year': [2021, 2021, 2021, 2021, 2021]
})

# Combine dataframes into one
dataframes = [df1, df2]
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize the 'status' column
combined_df['status'] = 'stayed'

# Create sets to keep track of clients across months and years
previous_clients = set()
current_clients = set(combined_df['cclikpi'].unique())

# Iterate through the dataframe to determine the status
for index, row in combined_df.iterrows():
    cclikpi = row['cclikpi']
    year = row['year']
    month = row['month']
    
    if cclikpi in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'stayed'
    elif cclikpi not in previous_clients and cclikpi in current_clients:
        combined_df.at[index, 'status'] = 'joined'
    elif cclikpi in previous_clients and cclikpi not in current_clients:
        combined_df.at[index, 'status'] = 'left'
    
    # Update previous_clients for the next iteration
    previous_clients.update(current_clients)
    current_clients = set(combined_df[(combined_df['year'] == year) & (combined_df['month'] == month)]['cclikpi'])

# Update the status for those who left in the previous year and did not join again
for cclikpi in previous_clients:
    if cclikpi not in current_clients:
        combined_df.loc[(combined_df['cclikpi'] == cclikpi) & (combined_df['year'] > year), 'status'] = 'left'

print(combined_df)










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Combine all dataframes into one DataFrame
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Initialize a dictionary to track the status of each client
client_status = {}

# Initialize the 'status' column
combined_df['status'] = 'Stayed'

# Update the status and pnb/pnb-rep columns
for index, row in combined_df.iterrows():
    cclikpi = row['CCLIKPI']
    
    if cclikpi in client_status:
        if client_status[cclikpi] == 'Left':
            combined_df.at[index, 'status'] = 'Left'
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
    else:
        # Check if the client is present in the next month's data
        next_month_df = combined_df[(combined_df['year'] == row['year']) & (combined_df['month'] == (row['month'] + 1))]
        
        if cclikpi not in set(next_month_df['CCLIKPI']):
            combined_df.at[index, 'status'] = 'Left'
            client_status[cclikpi] = 'Left'
        else:
            client_status[cclikpi] = 'Stayed'

# Update all remaining occurrences of clients marked as 'Left'
for cclikpi, status in client_status.items():
    if status == 'Left':
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'status'] = 'Left'
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'pnb'] = 0
        combined_df.loc[combined_df['CCLIKPI'] == cclikpi, 'pnb-rep'] = 0

print(combined_df)










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Combine all dataframes into one DataFrame
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df = combined_df.sort_values(by=['year', 'month'])

# Create a column to store the status
combined_df['status'] = 'Stayed'

# Track the status of clients
client_status = {}

# Update status to 'Left' and set pnb and pnb-rep to 0 when a client is lost
for index, row in combined_df.iterrows():
    cclikpi = row['CCLIKPI']
    
    if cclikpi in client_status:
        # If the client was previously marked as 'Left', keep it 'Left'
        if client_status[cclikpi] == 'Left':
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
        else:
            client_status[cclikpi] = 'Stayed'
    else:
        # If the client is not in the current month's data, mark as 'Left'
        current_month_df = combined_df[(combined_df['year'] == row['year']) & (combined_df['month'] == row['month'])]
        current_month_clients = set(current_month_df['CCLIKPI'])
        if cclikpi not in current_month_clients:
            combined_df.at[index, 'status'] = 'Left'
            combined_df.at[index, 'pnb'] = 0
            combined_df.at[index, 'pnb-rep'] = 0
            client_status[cclikpi] = 'Left'
        else:
            client_status[cclikpi] = 'Stayed'

# Ensure that the first occurrence of each client has the correct 'Stayed' status
for cclikpi in client_status.keys():
    if client_status[cclikpi] == 'Left':
        first_occurrence_index = combined_df[combined_df['CCLIKPI'] == cclikpi].index[0]
        combined_df.at[first_occurrence_index, 'status'] = 'Stayed'

print(combined_df)






import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
df1 = pd.DataFrame({
    'CCLIKPI': [1, 2, 3, 4, 5],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [100, 200, 300, 400, 500],
    'pnb-rep': [10, 20, 30, 40, 50],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2018, 2018, 2018, 2018, 2018]
})

df2 = pd.DataFrame({
    'CCLIKPI': [2, 3, 4, 5, 6],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [150, 250, 350, 450, 550],
    'pnb-rep': [15, 25, 35, 45, 55],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2019, 2019, 2019, 2019, 2019]
})

df3 = pd.DataFrame({
    'CCLIKPI': [3, 4, 5, 6, 7],
    'Cnouvseg': ['C', 'A', 'B', 'C', 'A'],
    'pnb': [200, 300, 400, 500, 600],
    'pnb-rep': [20, 30, 40, 50, 60],
    'section': ['Z', 'X', 'Y', 'Z', 'X'],
    'month': [1, 2, 3, 4, 5],
    'year': [2020, 2020, 2020, 2020, 2020]
})

df4 = pd.DataFrame({
    'CCLIKPI': [4, 5, 6, 7, 8],
    'Cnouvseg': ['A', 'B', 'C', 'A', 'B'],
    'pnb': [250, 350, 450, 550, 650],
    'pnb-rep': [25, 35, 45, 55, 65],
    'section': ['X', 'Y', 'Z', 'X', 'Y'],
    'month': [1, 2, 3, 4, 5],
    'year': [2021, 2021, 2021, 2021, 2021]
})

df5 = pd.DataFrame({
    'CCLIKPI': [5, 6, 7, 8, 9],
    'Cnouvseg': ['B', 'C', 'A', 'B', 'C'],
    'pnb': [300, 400, 500, 600, 700],
    'pnb-rep': [30, 40, 50, 60, 70],
    'section': ['Y', 'Z', 'X', 'Y', 'Z'],
    'month': [1, 2, 3, 4, 5],
    'year': [2022, 2022, 2022, 2022, 2022]
})

# List of dataframes
dataframes = [df1, df2, df3, df4, df5]

# Identify lost clients year by year
lost_clients_count_year = []
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['CCLIKPI'])
    current_year_clients = set(current_year_df['CCLIKPI'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_count_year.append(len(lost_clients))

loss_df_year = pd.DataFrame({
    'Year': [df['year'].iloc[0] for df in dataframes[1:]],
    'Lost Clients': lost_clients_count_year
})

print(loss_df_year)

# Combine all dataframes into one with month and year information
combined_df = pd.concat(dataframes, ignore_index=True)

# Identify lost clients month by month
lost_clients_count_month = []
combined_df = combined_df.sort_values(by=['year', 'month'])
for i in range(1, len(combined_df)):
    previous_month_df = combined_df.iloc[:i]
    current_month_df = combined_df.iloc[i:]
    
    previous_month_clients = set(previous_month_df['CCLIKPI'])
    current_month_clients = set(current_month_df['CCLIKPI'])
    
    lost_clients = previous_month_clients - current_month_clients
    
    lost_clients_count_month.append(len(lost_clients))

loss_df_month = pd.DataFrame({
    'Year-Month': combined_df['year'].astype(str) + '-' + combined_df['month'].astype(str),
    'Lost Clients': [0] + lost_clients_count_month  # Adding 0 for the first month
})

print(loss_df_month)

# Plot the number of lost clients each year
plt.figure(figsize=(10, 6))
plt.plot(loss_df_year['Year'], loss_df_year['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Year')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.grid(True)
plt.show()

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df_month['Year-Month'], loss_df_month['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Year-Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()











import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018_jan = pd.DataFrame({'cclikpi': [1, 2, 3], 'revenue': [100, 200, 300], 'category': ['A', 'B', 'C']})
data_2018_feb = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [150, 250, 350], 'category': ['B', 'C', 'A']})
data_2019_jan = pd.DataFrame({'cclikpi': [2, 3, 4], 'revenue': [200, 300, 400], 'category': ['B', 'C', 'A']})
data_2019_feb = pd.DataFrame({'cclikpi': [3, 4, 5], 'revenue': [250, 350, 450], 'category': ['C', 'A', 'B']})

# List of dataframes and corresponding months and years
dataframes_2018 = [(data_2018_jan, '2018-01'), (data_2018_feb, '2018-02')]
dataframes_2019 = [(data_2019_jan, '2019-01'), (data_2019_feb, '2019-02')]

# Combine dataframes into a list with month information
dataframes = dataframes_2018 + dataframes_2019

# Add a 'month' column to each dataframe
for df, month in dataframes:
    df['month'] = month

# Initialize a list to store lost client details each year
lost_clients_details = []

# Combine dataframes for each year
combined_data = pd.concat([df for df, month in dataframes], ignore_index=True)

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Identify lost clients between each month
for i in range(1, len(dataframes)):
    previous_month_df = dataframes[i-1][0]
    current_month_df = dataframes[i][0]
    
    previous_month_clients = set(previous_month_df['cclikpi'])
    current_month_clients = set(current_month_df['cclikpi'])
    
    lost_clients = previous_month_clients - current_month_clients
    lost_clients_df = previous_month_df[previous_month_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_month'] = dataframes[i][1]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each month
loss_df = pd.DataFrame({
    'Month': [month for _, month in dataframes[1:]],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each month
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Month'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Month')
plt.xlabel('Month')
plt.ylabel('Number of Lost Clients')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()





import pandas as pd
import matplotlib.pyplot as plt

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store the number of lost clients each year
lost_clients_count = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_count.append(len(lost_clients))

# Create a DataFrame to store the number of lost clients each year
loss_df = pd.DataFrame({
    'Year': years[1:],
    'Lost Clients': lost_clients_count
})

# Print the results
print(loss_df)

# Plot the number of lost clients each year
plt.figure(figsize=(10, 6))
plt.plot(loss_df['Year'], loss_df['Lost Clients'], marker='o', linestyle='-', color='r')
plt.title('Number of Clients Lost Each Year')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.grid(True)
plt.show()










Environ 32,2 % des clients ont été perdus tous types confondus entre 2019 et 2023.

Pour des analyses plus détaillées et interactives, veuillez consulter le tableau de bord Power BI.

Cordialement,



import pandas as pd

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    lost_clients_df['status'] = 'Left'
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

# Combine all years data for identifying clients who stayed
all_years_data = pd.concat(dataframes, ignore_index=True)

# Identify unique clients who stayed
stayed_clients = all_years_data[~all_years_data['cclikpi'].isin(lost_clients_combined_df['cclikpi'])].drop_duplicates('cclikpi')
stayed_clients['status'] = 'Stayed'
stayed_clients['lost_year'] = None

# Combine lost and stayed clients
combined_df = pd.concat([lost_clients_combined_df, stayed_clients], ignore_index=True)

# Add columns for visualization
combined_df['initial_year'] = combined_df.groupby('cclikpi')['lost_year'].transform('min')
combined_df['final_year'] = combined_df.groupby('cclikpi')['lost_year'].transform('max')
combined_df['total_revenue'] = combined_df[['revenue_2018', 'revenue_2019', 'revenue_2020', 'revenue_2021', 'revenue_2022']].sum(axis=1)

print(combined_df)

# Export the combined DataFrame to a CSV file for use in Power BI
combined_df.to_csv('client_status_analysis.csv', index=False)
print("Data exported to 'client_status_analysis.csv' for use in Power BI.")










import pandas as pd

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

# Isolate the 'cclikpi' column and remove duplicates to get unique lost clients
unique_lost_clients = lost_clients_combined_df['cclikpi'].drop_duplicates()

# Calculate the total number of unique clients lost
total_unique_clients_lost = len(unique_lost_clients)

print("Total number of unique clients lost:", total_unique_clients_lost)











Rapport sur la Perte de Clients Professionnels (CCLIKPI)

Introduction:
Ce rapport présente les statistiques détaillées sur le nombre de clients professionnels perdus par année, en se concentrant sur deux segments : les entreprises (CNOUVSEG=ER) et les autres clients professionnels (CNOUVSEG=AC PL AG). Les données incluent également des mesures de fréquence et des pourcentages de pertes annuelles.

1. Perte de Clients pour les Entreprises (CNOUVSEG=ER) :

Année	Nombre de clients perdus	% de perte annuelle
2023	5635	8.8%
2022	5596	8.7%
2021	4776	7.9%
2020	4875	8%
Observations :

En 2023, le nombre de clients perdus pour les entreprises a légèrement augmenté, atteignant 5635, avec un taux de perte de 8.8%.
La perte annuelle de clients a fluctué entre 7.9% et 8.8% de 2020 à 2023, indiquant une tendance stable mais légèrement croissante.
2. Perte de Clients pour les Autres Professionnels (CNOUVSEG=AC PL AG) :

Année	Nombre de clients perdus	% de perte annuelle
2023	35819	10.7%
2022	33410	10.4%
2021	29445	9.5%
2020	30937	10.3%
Observations :

Le nombre de clients perdus dans ce segment a considérablement augmenté en 2023, atteignant 35819, avec un taux de perte de 10.7%.
La perte annuelle de clients dans ce segment montre une tendance à la hausse, passant de 9.5% en 2021 à 10.7% en 2023.
Analyse des Tendances de Perte de Clients :

Entre 2019 et 2020 :

Entreprises (ER) : 8% de clients perdus.
Autres Professionnels (AC AG PL) : 10.3% de clients perdus.
Entre 2020 et 2021 :

Entreprises (ER) : 7.9% de clients perdus.
Autres Professionnels (AC AG PL) : 9.5% de clients perdus.
Entre 2021 et 2022 :

Entreprises (ER) : 8.7% de clients perdus.
Autres Professionnels (AC AG PL) : 10.4% de clients perdus.
Entre 2022 et 2023 :

Entreprises (ER) : 8.8% de clients perdus.
Autres Professionnels (AC AG PL) : 10.7% de clients perdus.
Conclusion:
Les données montrent une tendance inquiétante de perte de clients dans les deux segments de clients professionnels. Les entreprises et les autres clients professionnels voient tous deux une augmentation progressive des pertes de clients au fil des années, avec des pourcentages de perte annuels relativement élevés. Cette analyse souligne la nécessité de stratégies de rétention client plus efficaces pour inverser cette tendance.

Cordialement,










import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample data for testing
data_2018 = pd.DataFrame({'cclikpi': [1, 2, 3, 4, 5], 'revenue_2018': [100, 200, 300, 400, 500], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2019 = pd.DataFrame({'cclikpi': [2, 3, 4, 5, 6], 'revenue_2019': [150, 250, 350, 450, 550], 'category': ['B', 'C', 'A', 'B', 'C']})
data_2020 = pd.DataFrame({'cclikpi': [3, 4, 5, 6, 7], 'revenue_2020': [200, 300, 400, 500, 600], 'category': ['C', 'A', 'B', 'C', 'A']})
data_2021 = pd.DataFrame({'cclikpi': [4, 5, 6, 7, 8], 'revenue_2021': [250, 350, 450, 550, 650], 'category': ['A', 'B', 'C', 'A', 'B']})
data_2022 = pd.DataFrame({'cclikpi': [5, 6, 7, 8, 9], 'revenue_2022': [300, 400, 500, 600, 700], 'category': ['B', 'C', 'A', 'B', 'C']})

# List of dataframes and corresponding years
dataframes = [data_2018, data_2019, data_2020, data_2021, data_2022]
years = [2018, 2019, 2020, 2021, 2022]

# Initialize a list to store lost client details each year
lost_clients_details = []

# Compare each year's clients with the previous year's clients to identify losses
for i in range(1, len(dataframes)):
    previous_year_df = dataframes[i-1]
    current_year_df = dataframes[i]
    
    previous_year_clients = set(previous_year_df['cclikpi'])
    current_year_clients = set(current_year_df['cclikpi'])
    
    lost_clients = previous_year_clients - current_year_clients
    
    lost_clients_df = previous_year_df[previous_year_df['cclikpi'].isin(lost_clients)].copy()
    lost_clients_df['lost_year'] = years[i]
    
    lost_clients_details.append(lost_clients_df)

# Combine all the lost clients details into a single DataFrame
lost_clients_combined_df = pd.concat(lost_clients_details, ignore_index=True)

print(lost_clients_combined_df)

# Export the combined DataFrame to a CSV file for use in Power BI
lost_clients_combined_df.to_csv('lost_clients_analysis.csv', index=False)
print("Data exported to 'lost_clients_analysis.csv' for use in Power BI.")

# Plot the number of lost clients each year by category
plt.figure(figsize=(10, 6))
sns.countplot(data=lost_clients_combined_df, x='lost_year', hue='category', palette='Set2')
plt.title('Number of Clients Lost Each Year by Category')
plt.xlabel('Year')
plt.ylabel('Number of Lost Clients')
plt.legend(title='Category')
plt.grid(True)
plt.show()

# Plot the distribution of lost client revenues by year and category
plt.figure(figsize=(10, 6))
sns.boxplot(data=lost_clients_combined_df.melt(id_vars=['cclikpi', 'category', 'lost_year'], 
                                               value_vars=['revenue_2018', 'revenue_2019', 'revenue_2020', 'revenue_2021'], 
                                               var_name='year', value_name='revenue'),
            x='year', y='revenue', hue='category', palette='Set2')
plt.title('Distribution of Lost Client Revenues by Year and Category')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt



import csv

def list_to_csv(int_list, filename='output.csv'):
    # Convert each integer to a string and add '0' at the beginning
    str_list = ['0' + str(i) for i in int_list]
    
    # Write the list to a CSV file
    with open(filename, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        # Writing the list as a single row
        csvwriter.writerow(str_list)

# Example usage
int_list = [123, 456, 789]
list_to_csv(int_list)



import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, VotingRegressor, BaggingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B'],  # Example category column
    'target': [1.5, 2.0, 0.5, 2.5, 1.0]  # Example continuous target variable
})

# Discretize the continuous variables as described before
# For simplicity, using already discretized values
data['pnb'] = np.digitize(data['pnb'], bins=[150, 250, 350, 450])
data['pnb2023'] = np.digitize(data['pnb2023'], bins=[200, 300, 400, 500])

# Define features and target
X = data[['pnb', 'pnb2023']]
y = data['target']

# Initialize the models
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),
    'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),
    'Voting Regressor': VotingRegressor(estimators=[
        ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)),
        ('et', ExtraTreesRegressor(n_estimators=50, random_state=42))
    ]),
    'Bagging Regressor': BaggingRegressor(n_estimators=100, random_state=42)
}

# Define cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models
def evaluate_model(model, X, y, cv):
    scoring = {
        'MAE': 'neg_mean_absolute_error',
        'MSE': 'neg_mean_squared_error',
        'R2': 'r2'
    }
    
    results = {
        'MAE': -np.mean(cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')),
        'MSE': -np.mean(cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error')),
        'R2': np.mean(cross_val_score(model, X, y, cv=cv, scoring='r2'))
    }
    return results

# Evaluate all models
results = {}
for model_name, model in models.items():
    results[model_name] = evaluate_model(model, X, y, kf)

# Display results
results_df = pd.DataFrame(results).T
print("Evaluation Results:")
print(results_df)






import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier, BaggingClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B'],  # Example category column
    'target': [1, 0, 1, 0, 1]  # Example binary target variable
})

# Discretize the continuous variables as described before
# For simplicity, using already discretized values
data['pnb'] = np.digitize(data['pnb'], bins=[150, 250, 350, 450])
data['pnb2023'] = np.digitize(data['pnb2023'], bins=[200, 300, 400, 500])

# Define features and target
X = data[['pnb', 'pnb2023']]
y = data['target']

# Initialize the models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),
    'Voting Classifier': VotingClassifier(estimators=[
        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
        ('et', ExtraTreesClassifier(n_estimators=50, random_state=42))
    ], voting='soft'),
    'Bagging Classifier': BaggingClassifier(n_estimators=100, random_state=42)
}

# Define cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models
def evaluate_model(model, X, y, cv):
    scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    results = {
        'Accuracy': np.mean(cross_val_score(model, X, y, cv=cv, scoring='accuracy')),
        'F1 Score': np.mean(cross_val_score(model, X, y, cv=cv, scoring='f1')),
        'Precision': np.mean(cross_val_score(model, X, y, cv=cv, scoring='precision')),
        'Recall': np.mean(cross_val_score(model, X, y, cv=cv, scoring='recall')),
        'ROC-AUC': np.mean(cross_val_score(model, X, y, cv=cv, scoring='roc_auc'))
    }
    return results

# Evaluate all models
results = {}
for model_name, model in models.items():
    results[model_name] = evaluate_model(model, X, y, kf)

# Display results
results_df = pd.DataFrame(results).T
print("Evaluation Results:")
print(results_df)









import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The continuous target variable
target = pd.Series([1.5, 2.0, 0.5, 2.5, 1.0], name='target')

# Function to find split points using a decision tree regressor
def find_splits(data, target):
    tree = DecisionTreeRegressor(criterion='mse', max_leaf_nodes=6)
    tree.fit(data.reshape(-1, 1), target)
    
    thresholds = tree.tree_.threshold
    split_points = sorted(threshold for threshold in thresholds if threshold != -2)
    return split_points

# Apply decision tree regressor to find splits for each continuous column
columns_to_discretize = ['pnb', 'pnb2023']
split_points_dict = {col: find_splits(data[col].values, target.values) for col in columns_to_discretize}

# Discretize the data based on the split points
def discretize(data, split_points):
    return np.digitize(data, split_points, right=True)

df_disc = data.copy()
for col, splits in split_points_dict.items():
    df_disc[col] = discretize(data[col].values, splits)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, df_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(df_disc)














import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Function to find split points using a decision tree
def find_splits(data, target):
    tree = DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=6)
    tree.fit(data.reshape(-1, 1), target)
    
    thresholds = tree.tree_.threshold
    split_points = sorted(threshold for threshold in thresholds if threshold != -2)
    return split_points

# Apply decision tree to find splits for each continuous column
columns_to_discretize = ['pnb', 'pnb2023']
split_points_dict = {col: find_splits(data[col].values, target_shifted.values) for col in columns_to_discretize}

# Discretize the data based on the split points
def discretize(data, split_points):
    return np.digitize(data, split_points, right=True)

df_disc = data.copy()
for col, splits in split_points_dict.items():
    df_disc[col] = discretize(data[col].values, splits)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, df_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(df_disc)









import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mdlp.discretization import MDLP

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target_shifted.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

# Function to plot the original and discretized distributions
def plot_discretization_comparison(data, data_disc, column_name):
    plt.figure(figsize=(14, 6))
    
    # Plot the original distribution
    plt.subplot(1, 2, 1)
    plt.hist(data[column_name], bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Original Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    
    # Plot the discretized distribution
    plt.subplot(1, 2, 2)
    plt.hist(data_disc[column_name], bins=np.arange(data_disc[column_name].min(), data_disc[column_name].max() + 1), color='lightcoral', edgecolor='black', align='left')
    plt.title(f'Discretized Distribution of {column_name}')
    plt.xlabel(f'{column_name} (discretized)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot comparison for each column
for column in columns_to_discretize:
    plot_discretization_comparison(data, data_disc, column)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)












import pandas as pd
from mdlp.discretization import MDLP
import numpy as np

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable with negative numbers
target = pd.Series([-1, -2, 0, 1, -1], name='target')

# Normalize the target variable to be non-negative
target_shift = target - target.min()
target_shifted = target_shift

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target_shifted.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)














import pandas as pd
from mdlp.discretization import MDLP

# Sample DataFrame with negative numbers for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [-100, 200, -300, 400, -500],
    'pnb2023': [-150, 250, -350, 450, -550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable
target = pd.Series([0, 1, 0, 1, 0], name='target')

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)










import pandas as pd
from mdlp.discretization import MDLP

# Sample DataFrame for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# The target variable
target = pd.Series([0, 1, 0, 1, 0], name='target')

# Apply MDLP discretization
transformer = MDLP()

# Assuming 'pnb' and 'pnb2023' are the columns to be discretized
columns_to_discretize = ['pnb', 'pnb2023']

# Apply discretization and create a new DataFrame
X_disc = transformer.fit_transform(data[columns_to_discretize].values, target.values)

# Convert the discretized data back to a DataFrame with the same column names
df_disc = pd.DataFrame(X_disc, columns=columns_to_discretize)

# Combine the discretized columns with the original DataFrame (excluding the original columns)
data_disc = pd.concat([data.drop(columns=columns_to_discretize), df_disc], axis=1)

print("Original DataFrame:")
print(data)

print("\nDiscretized DataFrame:")
print(data_disc)













import numpy as np
import pandas as pd
from math import log2

def entropy(y):
    """Calculate the entropy of a distribution for the classes in y."""
    if len(y) == 0:
        return 0
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

def mdlp_cut(data, labels):
    """Find the optimal cut point for discretization using MDLPC."""
    best_cut = None
    min_mdlp_value = float('inf')
    
    sorted_idx = np.argsort(data)
    sorted_data = data[sorted_idx]
    sorted_labels = labels[sorted_idx]
    
    for i in range(1, len(data)):
        if sorted_data[i] == sorted_data[i - 1]:
            continue
        
        left_labels = sorted_labels[:i]
        right_labels = sorted_labels[i:]
        
        left_entropy = entropy(left_labels)
        right_entropy = entropy(right_labels)
        
        left_prob = len(left_labels) / len(labels)
        right_prob = len(right_labels) / len(labels)
        
        total_entropy = left_prob * left_entropy + right_prob * right_entropy
        
        mdlp_value = entropy(labels) - total_entropy - (log2(len(data) - 1) / len(data))
        
        if mdlp_value < min_mdlp_value:
            min_mdlp_value = mdlp_value
            best_cut = (sorted_data[i] + sorted_data[i - 1]) / 2
    
    return best_cut

def mdlp_discretize(data, labels):
    """Apply the MDLPC algorithm to discretize the data."""
    data = np.array(data)
    labels = np.array(labels)
    
    cut_points = []
    intervals = [(0, len(data))]
    
    while intervals:
        start, end = intervals.pop()
        best_cut = mdlp_cut(data[start:end], labels[start:end])
        
        if best_cut is not None:
            cut_points.append(best_cut)
            left_end = np.searchsorted(data, best_cut, side='right')
            right_start = np.searchsorted(data, best_cut, side='left')
            
            intervals.append((start, left_end))
            intervals.append((right_start, end))
    
    cut_points = sorted(cut_points)
    
    discretized_data = np.digitize(data, cut_points, right=True)
    
    return discretized_data, cut_points

# Sample usage with Iris dataset
from sklearn.datasets import load_iris

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Apply MDLP discretization to each feature
discretized_features = []
cut_points_dict = {}
for i in range(X.shape[1]):
    discretized_feature, cut_points = mdlp_discretize(X[:, i], y)
    discretized_features.append(discretized_feature)
    cut_points_dict[iris.feature_names[i]] = cut_points

# Create a new DataFrame with the discretized features
df_disc = pd.DataFrame(np.array(discretized_features).T, columns=iris.feature_names)
df_disc['target'] = y

# Print the cut points and the new DataFrame with discretized columns
print("Cut points for each feature:")
for feature, cuts in cut_points_dict.items():
    print(f"{feature}: {cuts}")

print("\nNew DataFrame with discretized columns:")
print(df_disc.head())








import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from mdlp.discretization import MDLP

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Create DataFrame from the Iris dataset
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y

# Apply MDLP discretization
transformer = MDLP()
X_disc = transformer.fit_transform(X, y)

# Create a new DataFrame with the discretized columns
df_disc = pd.DataFrame(X_disc, columns=iris.feature_names)
df_disc['target'] = y

# Function to plot before and after discretization
def plot_discretization(df, df_disc, feature_name):
    # Create a figure with subplots
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot the continuous variable (before discretization)
    axs[0].hist(df[feature_name], bins=30, color='skyblue', edgecolor='black')
    axs[0].set_title(f'Before Discretization: {feature_name}')
    axs[0].set_xlabel(feature_name)
    axs[0].set_ylabel('Frequency')
    
    # Plot the discrete variable (after discretization)
    axs[1].hist(df_disc[feature_name], bins=np.arange(df_disc[feature_name].min(), df_disc[feature_name].max() + 1), color='lightcoral', edgecolor='black')
    axs[1].set_title(f'After Discretization: {feature_name}')
    axs[1].set_xlabel(f'{feature_name} (discretized)')
    axs[1].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot for each feature
for feature_name in iris.feature_names:
    plot_discretization(df, df_disc, feature_name)

# Print the new DataFrame with discretized columns
print("New DataFrame with discretized columns:")
print(df_disc.head())










import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from mdlp.discretization import MDLP

# Load Iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Apply MDLP discretization
transformer = MDLP()
X_disc = transformer.fit_transform(X, y)

# Function to plot before and after discretization
def plot_discretization(X, X_disc, feature_index):
    feature_name = iris.feature_names[feature_index]
    
    # Create a figure with subplots
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot the continuous variable (before discretization)
    axs[0].hist(X[:, feature_index], bins=30, color='skyblue', edgecolor='black')
    axs[0].set_title(f'Before Discretization: {feature_name}')
    axs[0].set_xlabel(feature_name)
    axs[0].set_ylabel('Frequency')
    
    # Plot the discrete variable (after discretization)
    axs[1].hist(X_disc[:, feature_index], bins=np.arange(X_disc[:, feature_index].min(), X_disc[:, feature_index].max() + 1), color='lightcoral', edgecolor='black')
    axs[1].set_title(f'After Discretization: {feature_name}')
    axs[1].set_xlabel(f'{feature_name} (discretized)')
    axs[1].set_ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

# Plot for each feature
for i in range(X.shape[1]):
    plot_discretization(X, X_disc, i)

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def entropy(y):
    """Calculate the entropy of a distribution for the classes in y."""
    if len(y) == 0:
        return 0
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

def mdlp_cut(data, labels):
    """Find the optimal cut point for discretization using MDLPC."""
    best_cut = None
    min_mdlp_value = float('inf')
    
    sorted_idx = np.argsort(data)
    sorted_data = data[sorted_idx]
    sorted_labels = labels[sorted_idx]
    
    for i in range(1, len(data)):
        if sorted_data[i] == sorted_data[i - 1]:
            continue
        
        left_labels = sorted_labels[:i]
        right_labels = sorted_labels[i:]
        
        left_entropy = entropy(left_labels)
        right_entropy = entropy(right_labels)
        
        left_prob = len(left_labels) / len(labels)
        right_prob = len(right_labels) / len(labels)
        
        total_entropy = left_prob * left_entropy + right_prob * right_entropy
        
        mdlp_value = entropy(labels) - total_entropy - (log(len(data) - 1) / len(data))
        
        if mdlp_value < min_mdlp_value:
            min_mdlp_value = mdlp_value
            best_cut = sorted_data[i]
    
    return best_cut

def mdlp_discretize(data, labels):
    """Apply the MDLPC algorithm to discretize the data."""
    data = np.array(data)
    labels = np.array(labels)
    
    cut_points = []
    intervals = [(0, len(data))]
    
    while intervals:
        start, end = intervals.pop()
        best_cut = mdlp_cut(data[start:end], labels[start:end])
        
        if best_cut is not None:
            cut_points.append(best_cut)
            left_end = np.searchsorted(data, best_cut, side='right')
            right_start = np.searchsorted(data, best_cut, side='left')
            
            intervals.append((start, left_end))
            intervals.append((right_start, end))
    
    cut_points = sorted(cut_points)
    
    discretized_data = np.digitize(data, cut_points, right=True)
    
    return discretized_data, cut_points

# Sample usage
data = pd.DataFrame({
    'continuous_variable': np.random.rand(100),
    'label': np.random.choice([0, 1], size=100)
})

discretized_data, cut_points = mdlp_discretize(data['continuous_variable'], data['label'])
data['discretized_variable'] = discretized_data

# Visualization
plt.figure(figsize=(12, 6))

# Histogram of continuous variable
plt.subplot(1, 2, 1)
sns.histplot(data['continuous_variable'], bins=20, kde=True)
plt.title('Continuous Variable Before Discretization')
plt.xlabel('Continuous Variable')
plt.ylabel('Frequency')

# Bar plot of discretized variable
plt.subplot(1, 2, 2)
sns.countplot(x='discretized_variable', data=data)
plt.title('Discretized Variable After MDLPC')
plt.xlabel('Discretized Variable')
plt.ylabel('Count')

plt.tight_layout()
plt.show()














import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi', 'category'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Define a color palette for the categories
palette = sns.color_palette("husl", data_long['category'].nunique())
color_map = {category: palette[i] for i, category in enumerate(data_long['category'].unique())}

# Plot the evolution of PNB for each company with lines colored based on the category
plt.figure(figsize=(10, 6))
for key, grp in data_long.groupby(['cclikpi']):
    category = grp['category'].values[0]
    plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}', color=color_map[category])

# Create a custom legend for the categories
handles = [plt.Line2D([0], [0], color=color_map[cat], lw=4, label=cat) for cat in color_map]
plt.legend(handles=handles, title='Category')

plt.xlabel('Year')
plt.ylabel('PNB Value')
plt.title('Evolution of PNB from 2018 to 2023 for Each Company')
plt.grid(True)
plt.show()














import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550],
    'category': ['A', 'B', 'A', 'C', 'B']  # Example category column
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi', 'category'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Define a color palette for the categories
palette = sns.color_palette("husl", data_long['category'].nunique())
color_map = {category: palette[i] for i, category in enumerate(data_long['category'].unique())}

# Function to plot batches of companies
def plot_batches(data, batch_size=10):
    unique_ids = data['cclikpi'].unique()
    num_batches = len(unique_ids) // batch_size + (1 if len(unique_ids) % batch_size != 0 else 0)
    
    for i in range(num_batches):
        plt.figure(figsize=(10, 6))
        batch_ids = unique_ids[i * batch_size: (i + 1) * batch_size]
        batch_data = data[data['cclikpi'].isin(batch_ids)]
        
        for key, grp in batch_data.groupby(['cclikpi']):
            category = grp['category'].values[0]
            plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}', color=color_map[category])
        
        plt.xlabel('Year')
        plt.ylabel('PNB Value')
        plt.title(f'Evolution of PNB from 2018 to 2023 for Companies {i * batch_size + 1} to {(i + 1) * batch_size}')
        plt.legend(title='cclikpi')
        plt.grid(True)
        plt.show()

# Plot the data in batches
plot_batches(data_long, batch_size=2)  # Adjust batch_size as needed




















import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550]
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Function to plot batches of companies
def plot_batches(data, batch_size=10):
    unique_ids = data['cclikpi'].unique()
    num_batches = len(unique_ids) // batch_size + (1 if len(unique_ids) % batch_size != 0 else 0)
    
    for i in range(num_batches):
        plt.figure(figsize=(10, 6))
        batch_ids = unique_ids[i * batch_size: (i + 1) * batch_size]
        batch_data = data[data['cclikpi'].isin(batch_ids)]
        
        for key, grp in batch_data.groupby(['cclikpi']):
            plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}')
        
        plt.xlabel('Year')
        plt.ylabel('PNB Value')
        plt.title(f'Evolution of PNB from 2018 to 2023 for Companies {i * batch_size + 1} to {(i + 1) * batch_size}')
        plt.legend(title='cclikpi')
        plt.grid(True)
        plt.show()

# Plot the data in batches
plot_batches(data_long, batch_size=2)  # Adjust batch_size as needed













import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550]
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Plot the evolution of PNB for each company
plt.figure(figsize=(10, 6))
for key, grp in data_long.groupby(['cclikpi']):
    plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}')

plt.xlabel('Year')
plt.ylabel('PNB Value')
plt.title('Evolution of PNB from 2018 to 2023 for Each Company')
plt.legend(title='cclikpi')
plt.grid(True)
plt.show()










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def find_new_rows_added(df1, df2, id_column, merge_column):
    # Perform the merge
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left', indicator=True)
    
    # Find new rows added by checking for rows in df2 but not in df1
    new_rows = merged_df[merged_df['_merge'] == 'right_only']
    
    return new_rows

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Find new rows added during the merge
new_rows_added = find_new_rows_added(df1, df2, id_column, merge_column)

print("New Rows Added:")
print(new_rows_added)














import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def merge_dataframes(df1, df2, id_column, merge_column):
    # Merge the two DataFrames on the id_column
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left')
    
    return merged_df

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Merge the dataframes and add the specified column from df2 to df1
result_df = merge_dataframes(df1, df2, id_column, merge_column)

print("Resulting DataFrame:")
print(result_df)













import pandas as pd

def get_column_types(df):
    # Create a dictionary with column names as keys and their data types as values
    column_types = {col: df[col].dtype for col in df.columns}
    return column_types

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'type_de_client': ['A', 'B', 'C', 'D', 'E'],
    'value1': [10, 20, 30, 40, 50],
    'value2': [100.0, 200.0, 300.0, 400.0, 500.0]
})

# Get the column types
column_types = get_column_types(data)
print("Column Types:")
print(column_types)









import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'category': ['X', 'Y', 'X', 'Y', 'X', 'X', 'Y'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_orders = {
    'type_de_client': ['A', 'B'],  # Priority order for 'type_de_client'
    'category': ['X', 'Y']         # Priority order for 'category'
}

# Custom aggregation function for categorical variables
def agg_priority(series, priority_order):
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Wrapper function to apply the correct priority order
def agg_wrapper(column, priority_orders):
    return lambda series: agg_priority(series, priority_orders[column])

# Perform the groupby operation and apply the custom aggregation functions
agg_dict = {
    'type_de_client': agg_wrapper('type_de_client', priority_orders),
    'category': agg_wrapper('category', priority_orders),
    'value1': 'sum',  # Example aggregation for numerical columns
    'value2': 'mean'  # Example aggregation for numerical columns
}

grouped_data = data.groupby('id').agg(agg_dict).reset_index()

print("Grouped DataFrame:")
print(grouped_data)






import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_order = ['A', 'B']  # You can easily change this list later

# Custom aggregation function for categorical variables
def agg_priority(series):
    # Get the first value in the series that matches the priority order
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Perform the groupby operation and apply the custom aggregation function
grouped_data = data.groupby('id').agg({
    'type_de_client': agg_priority,  # Apply the custom aggregation function
    'value1': 'sum',                 # Example aggregation for numerical columns
    'value2': 'mean'                 # Example aggregation for numerical columns
}).reset_index()

print("Grouped DataFrame:")
print(grouped_data)









import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column, class_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    # Calculate frequency and percentage of class distribution for matched IDs
    matched_class_freq = matched_df[class_column].value_counts()
    matched_class_percentage = matched_df[class_column].value_counts(normalize=True) * 100
    
    # Filter the DataFrame to get rows where the id is not in the id_list
    unmatched_df = df[~df[id_column].isin(id_list)]
    
    # Calculate frequency and percentage of class distribution for unmatched IDs
    unmatched_class_freq = unmatched_df[class_column].value_counts()
    unmatched_class_percentage = unmatched_df[class_column].value_counts(normalize=True) * 100
    
    return (matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
            unmatched_class_freq, unmatched_class_percentage)

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column names for IDs and class
id_column = 'id'
class_column = 'value'

# Get the results
(matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
 unmatched_class_freq, unmatched_class_percentage) = filter_dataframe_and_find_unmatched(large_df, id_list, id_column, class_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)
print("\nMatched Class Frequency:")
print(matched_class_freq)
print("\nMatched Class Percentage:")
print(matched_class_percentage)
print("\nUnmatched Class Frequency:")
print(unmatched_class_freq)
print("\nUnmatched Class Percentage:")
print(unmatched_class_percentage)









import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'ccli': [1, 2, 3, 4, 5],
    'cclikpi': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'avant_ccli': [1, 2, 3, 6],
    'apre_ccli': [10, 20, 30, 60],
    'cclikpi': [100, 200, 300, 400]
})

# Create a dictionary from df2 for quick lookup
cclikpi_map = df2.set_index('avant_ccli')['cclikpi'].to_dict()

# Function to determine the value for cclikpi2
def get_cclikpi2(ccli, cclikpi):
    return cclikpi_map.get(ccli, cclikpi)

# Apply the function to create the new column in df1
df1['cclikpi2'] = df1.apply(lambda row: get_cclikpi2(row['ccli'], row['cclikpi']), axis=1)

print("Resulting DataFrame:")
print(df1)





import pandas as pd

def filter_by_id_pattern(df, id_column, pattern):
    # Convert the id column to string
    df[id_column] = df[id_column].astype(str)
    
    # Filter rows where the id starts with the specific pattern
    filtered_df = df[df[id_column].str.startswith(pattern)]
    
    return filtered_df

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [3001, 3002, 123, 4003, 3004, 567, 3005],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g']
})

# Specify the column name for IDs and the pattern
id_column = 'id'
pattern = '300'

# Get the filtered DataFrame
filtered_df = filter_by_id_pattern(large_df, id_column, pattern)

print("Filtered DataFrame:")
print(filtered_df)




import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    return matched_df, unmatched_ids

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column name for IDs
id_column = 'id'

# Get the results
matched_df, unmatched_ids = filter_dataframe_and_find_unmatched(large_df, id_list, id_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)










Bonjour Solène,

Je suis en train d'agréger les données, mais j'ai un problème avec l'agrégation des variables catégorielles comme "type de foyer" ou "CNOUVSEG". Abidjo et Arthur m'ont donné une liste d'ordre de priorité pour certaines catégories, mais je ne suis pas sûr si cela est toujours correct pour mon cas, et en plus, elle n'est pas exhaustive. Aurais-tu des idées ou des suggestions pour m'aider à résoudre ce problème?

Merci d'avance pour ton aide.


Aurais-tu une liste de priorité pour le reste des variables catégorielles?

Je vous remercie sincèrement pour votre offre d'aide. Je n'hésiterai pas à faire appel à vous si j'ai besoin. Votre soutien est grandement apprécié


Bonjour Solène,

Je suis en train d'agréger les données, mais j'ai un problème avec l'agrégation des variables catégorielles comme "type de foyer" ou "CNOUVSEG". Aurais-tu des idées ou des suggestions pour m'aider à résoudre ce problème?

Merci d'avance pour ton aide.










import pandas as pd

def get_values_for_same_col1(data, col1, col2):
    # Group by the first column and collect the values of the second column
    result = data.groupby(col1)[col2].apply(list).to_dict()
    return result

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'c', 'd', 'e', 'f', 'g'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to use
col1 = 'col1'
col2 = 'col2'

values_for_same_col1 = get_values_for_same_col1(data, col1, col2)
print(values_for_same_col1)











import pandas as pd

def get_duplicates_across_columns(data, col1, col2):
    # Find duplicate rows based on the specified columns
    duplicate_rows = data[data.duplicated(subset=[col1, col2], keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=[col1, col2])
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to check for duplicates
col1 = 'col1'
col2 = 'col2'

duplicates = get_duplicates_across_columns(data, col1, col2)
print(duplicates)











import pandas as pd

def get_duplicates(data):
    # Find duplicate rows based on all columns
    duplicate_rows = data[data.duplicated(keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=list(data.columns))
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 20, 40, 50, 20, 10]
})

duplicates = get_duplicates(data)
print(duplicates)













import pandas as pd
import numpy as np

def get_indices_not_in_data2(data1, data2, col_name):
    # Drop NaN values from the target column in both dataframes
    data1_clean = data1.dropna(subset=[col_name])
    data2_clean = data2.dropna(subset=[col_name])
    
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2_clean[col_name].astype(data1_clean[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1_clean[~data1_clean[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5, np.nan],
    'value': ['a', 'b', 'c', 'd', 'e', 'f']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0, np.nan],
    'value': ['x', 'y', 'z', 'w']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)







import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2[col_name].astype(data1[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)











import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Get the unique values in the specified column of data2
    data2_values = set(data2[col_name].unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3, 4, 5],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)










import pandas as pd
import numpy as np

# Sample dataframe
data = {
    'ccli': [1, 1, 1, 2, 2, 2, 3, 3, 3, 3],
    'cclikpi': [10, 10, 10, 20, 20, 20, 30, 30, 30, 30],
    'col': [12, 12, 12, 11, 11, 11, 12, 12, 12, 12],  # The column with values from 1 to 12
    'value1': [0, 200, np.nan, 400, 0, 600, 700, 0, np.nan, 0],
    'value2': [100, 0, 0, 300, np.nan, 0, 500, 600, 0, 0]
}

df = pd.DataFrame(data)

# Function to calculate the number of zeros and NaNs in each row
def count_zeros_nans(row):
    return (row == 0).sum() + row.isna().sum()

# Add a column that counts the number of zeros and NaNs in each row
df['zeros_nans_count'] = df.apply(count_zeros_nans, axis=1)

# Sort the dataframe by 'ccli', 'cclikpi', 'col' and 'zeros_nans_count'
df = df.sort_values(by=['ccli', 'cclikpi', 'col', 'zeros_nans_count'])

# Drop duplicates keeping the first one based on 'ccli', 'cclikpi', and 'col' (the one with fewest zeros/nans)
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi', 'col'], keep='first')

# Drop the helper column as it's no longer needed
df_unique = df_unique.drop(columns=['zeros_nans_count'])

# Display the resulting dataframe
print(df_unique)



import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'col': [1, 12, 2, 11, 3, 6, 12],  # The column with values from 1 to 12
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Sort the dataframe by 'ccli', 'cclikpi' and 'col'
df = df.sort_values(by=['ccli', 'cclikpi', 'col'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)















import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-01', '2023-01-02', '2023-01-04'],
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Convert date column to datetime
df['date'] = pd.to_datetime(df['date'])

# Sort the dataframe by 'ccli', 'cclikpi' and 'date'
df = df.sort_values(by=['ccli', 'cclikpi', 'date'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)










Certainly! Here is an in-depth explanation of the first ten laws from Robert Greene's "The 48 Laws of Power":

Law 1: Never Outshine the Master
Concept: Always make those above you feel superior. Do not highlight your own talents to the point of making them feel insecure.
Application: Subtly display your skills while emphasizing your deference to your superiors. Praise their wisdom and ability, ensuring they receive credit.
Example: In the court of Louis XIV, his finance minister, Nicolas Fouquet, outshone the king by throwing an extravagant party. This led to Fouquet's downfall as Louis XIV felt threatened and imprisoned him.
Law 2: Never Put Too Much Trust in Friends, Learn How to Use Enemies
Concept: Friends are more likely to betray you as they become envious or complacent. Enemies, motivated by self-interest, are more reliable when properly managed.
Application: Be cautious with friends and keep a degree of separation. Convert enemies into allies by demonstrating that cooperation is mutually beneficial.
Example: Michael III of the Byzantine Empire relied on his friend Basil the Macedonian, who ultimately betrayed and killed him to take power.
Law 3: Conceal Your Intentions
Concept: Keep people off-balance and in the dark about your plans. This makes them unable to counter your moves.
Application: Use red herrings and decoys to mislead others. Keep your real goals hidden and reveal only enough to maintain trust.
Example: Otto von Bismarck, the Prussian statesman, used various deceptions to keep his opponents guessing about his true political intentions, which allowed him to unify Germany.
Law 4: Always Say Less Than Necessary
Concept: The more you say, the more likely you are to say something foolish. Silence and brevity create an aura of power.
Application: Be concise and deliberate with your words. Let others reveal more than you do, gaining insight into their thoughts and intentions.
Example: When interviewed, Andy Warhol often gave brief, enigmatic answers, which increased his mystique and allure in the art world.
Law 5: So Much Depends on Reputation – Guard It with Your Life
Concept: Your reputation is the cornerstone of your power. A strong reputation increases your presence and influence, while a tarnished reputation diminishes your power.
Application: Protect your reputation at all costs and be vigilant against attacks. Use subtle tactics to undermine the reputations of your rivals.
Example: Julius Caesar carefully cultivated his reputation for being generous and merciful, which helped him gain the loyalty and support of the Roman populace.
Law 6: Court Attention at All Costs
Concept: Stand out and make yourself noticed. Being unseen or forgotten is detrimental to your power.
Application: Create memorable and dramatic events or actions that capture attention. Cultivate an air of mystery and unpredictability.
Example: P.T. Barnum, the famous showman, constantly created public spectacles and stunts to keep himself and his shows in the public eye.
Law 7: Get Others to Do the Work for You, but Always Take the Credit
Concept: Use the skills, knowledge, and effort of others to further your own goals. Taking credit for their work enhances your reputation and power.
Application: Delegate tasks while ensuring that you receive recognition for the results. Reward those who contribute to keep them motivated.
Example: Thomas Edison often employed skilled engineers and researchers who helped him develop his inventions, but he ensured that he was recognized as the primary inventor.
Law 8: Make Other People Come to You – Use Bait If Necessary
Concept: By making others come to you, you control the situation and set the terms. Luring people to you ensures they play by your rules.
Application: Use allurements and incentives to attract people. Once they are drawn in, assert your dominance and influence.
Example: During negotiations, Henry Kissinger would often make foreign diplomats come to him, allowing him to dictate the terms and maintain a position of strength.
Law 9: Win Through Your Actions, Never Through Argument
Concept: Demonstrating your ideas through actions is far more persuasive than arguing. Arguments often lead to resentment and resistance.
Application: Let your actions speak for you. Show, don’t tell, and allow people to see the benefits or consequences of your ideas firsthand.
Example: Instead of arguing with critics, Leonardo da Vinci would demonstrate his inventions and artworks, which left little room for doubt about his genius.
Law 10: Infection: Avoid the Unhappy and Unlucky
Concept: Emotions and attitudes are contagious. Associating with those who are negative or unlucky can bring you down.
Application: Distance yourself from those who have a detrimental effect on your well-being and success. Seek out positive and successful individuals.
Example: The French adventurer, Napoleon Bonaparte, avoided associating with people who were constantly negative or failed, focusing instead on those who were optimistic and successful.
These laws emphasize strategic thinking, careful manipulation, and maintaining control over social and professional interactions to enhance one's power and influen


















import pandas as pd
import numpy as np
from scipy.stats.mstats import winsorize

# Sample DataFrame with numeric data (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.randn(100) * 100,
    'B': np.random.randn(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to apply Winsorization to all numeric columns in the DataFrame
def winsorize_dataframe(df, limits=(0.05, 0.05)):
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    df_winsorized = df.copy()
    
    for col in numeric_cols:
        df_winsorized[col] = winsorize(df[col], limits=limits)
    
    return df_winsorized

# Apply Winsorization to the DataFrame
winsorized_df = winsorize_dataframe(df, limits=(0.05, 0.05))

# Display the original and Winsorized DataFrame
print("Original DataFrame:")
print(df.describe())

print("\nWinsorized DataFrame:")
print(winsorized_df.describe())













import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy.stats import f_oneway

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to perform ANOVA test for each categorical variable against the continuous target variable
def anova_test(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"ANOVA test for {col} in relation to {target_var}:")
        
        # Perform ANOVA using statsmodels
        formula = f'{target_var} ~ C({col})'
        model = ols(formula, data=df).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)
        
        print(anova_table)
        print("\n")

# Specify the target column
target_column = 'Target'

# Perform ANOVA tests
anova_test(df, target_column)



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to generate statistical summaries and plots for categorical variables in relation to a continuous target variable
def categorical_summary_with_target(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col} in relation to {target_var}:")
        summary = df.groupby(col)[target_var].describe()
        print(summary)
        print("\n")
        
        # Plotting the distribution of the continuous target variable for each category using box plot
        plt.figure(figsize=(8, 6))
        sns.boxplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

        # Plotting the distribution of the continuous target variable for each category using violin plot
        plt.figure(figsize=(8, 6))
        sns.violinplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

# Specify the target column
target_column = 'Target'

# Generate and plot summaries for categorical variables in relation to the continuous target variable
categorical_summary_with_target(df, target_column)













import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100)
})

# Function to generate statistical summaries for categorical variables
def categorical_summary(df):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col}:")
        print(df[col].value_counts())
        print("\n")
        
        # Plotting the distribution of the categorical variable
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x=col, palette='viridis')
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.show()

# Generate and plot summaries for categorical variables
categorical_summary(df)




















import pandas as pd
import numpy as np

# Sample data
data = {'values': [1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]}
df = pd.DataFrame(data)

# Central Tendency
mean = df['values'].mean()
median = df['values'].median()
mode = df['values'].mode()

# Variability
range_ = df['values'].max() - df['values'].min()
variance = df['values'].var()
std_dev = df['values'].std()
iqr = df['values'].quantile(0.75) - df['values'].quantile(0.25)

# Shape
skewness = df['values'].skew()
kurtosis = df['values'].kurt()

# Summary
summary = df['values'].describe()

print("Mean:", mean)
print("Median:", median)
print("Mode:", mode[0])
print("Range:", range_)
print("Variance:", variance)
print("Standard Deviation:", std_dev)
print("Interquartile Range (IQR):", iqr)
print("Skewness:", skewness)
print("Kurtosis:", kurtosis)
print("\nSummary:\n", summary)




















import pandas as pd

# Sample data
data = {
    'Column1': ['A', 'B', 'A', 'C', 'B', 'A', 'B', 'C'],
    'Column2': [1, 2, 1, 3, 2, 1, 2, 3]
}

df = pd.DataFrame(data)

# Check for duplicates based on Column1 and Column2
duplicates = df.duplicated(subset=['Column1', 'Column2'])
print("Boolean Series indicating duplicates:")
print(duplicates)

# Get only the rows that are duplicates
duplicate_rows = df[df.duplicated(subset=['Column1', 'Column2'], keep=False)]
print("\nDuplicate rows:")
print(duplicate_rows)















duplicates = df.duplicated(subset=['Column1', 'Column2'])
print(duplicates)









import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Split the data into training (non-missing target values) and testing (missing target values) sets
train_df = prepared_df[prepared_df[target_column].notna()]
test_df = prepared_df[prepared_df[target_column].isna()]

# Define features and target
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]
X_test = test_df.drop(columns=[target_column])

# Train the XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)

# Predict the missing values
y_pred = xgb_model.predict(X_test)

# Evaluate the model (Since we don't have true values for test set, we can't compute RMSE here)
# In practice, you would compare y_pred with the true values if they were known

# Add predictions back to the original DataFrame
prepared_df.loc[prepared_df[target_column].isna(), target_column] = y_pred

# Display the results
print("Predicted values for the missing target variable:")
print(prepared_df[prepared_df[target_column].isna()][[target_column]])

# Optionally, if we had true values, we would compute the RMSE like this:
# y_true = true_values_array_for_missing_entries
# rmse = np.sqrt(mean_squared_error(y_true, y_pred))
# print(f"RMSE for XGBoost predictions: {rmse}")













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    
    # Extract the rows with missing values in the target column
    missing_df = df[df[target_column].isna()]
    
    # Extract the rows with non-missing values in the target column
    non_missing_df = df[df[target_column].notna()]
    
    # Extract the true values of the target column for evaluation
    y_true = non_missing_df[target_column].copy()
    
    for method_name, imputer in imputation_methods.items():
        df_copy = non_missing_df.dropna(axis=1, how='any')
        df_copy[target_column] = non_missing_df[target_column]
        
        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        
        y_pred = imputed_df[target_column]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")























import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    y_true = df[target_column][df[target_column].isna()]

    for method_name, imputer in imputation_methods.items():
        df_copy = df.copy()
        df_copy = df_copy.dropna(axis=1, how='any')
        df_copy[target_column] = df[target_column]

        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)

        y_pred = imputed_df[target_column][df[target_column].isna()]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    # Create a copy of the DataFrame
    df_copy = df.copy()
    
    # Separate the target column
    target = df_copy[[target_column]].copy()
    
    # Drop columns with missing values
    df_copy = df_copy.dropna(axis=1, how='any')
    
    # Add the target column back
    df_copy = pd.concat([df_copy, target], axis=1)
    
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Display the prepared DataFrame
print(prepared_df.head())













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        # Ensure the training data does not contain NaNs
        X_train_imputed = imputer.fit_transform(X_train)
        imputed_train_df = pd.DataFrame(X_train_imputed, columns=X_train.columns)
        
        # Impute missing values in the test set using the trained imputer
        X_test = test_df.drop(columns=[target_column])
        X_test_imputed = imputer.transform(X_test)
        imputed_test_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)
        imputed_test_df[target_column] = test_df[target_column]
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")



















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Drop columns with missing values except for the target column
        df_copy = df_copy.dropna(axis=1, how='any', subset=[col for col in df.columns if col != target_column])
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")






















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques using cross-validation
def evaluate_imputation_methods(df, imputation_methods):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    results = {method: [] for method in imputation_methods.keys()}

    for train_index, test_index in kf.split(df):
        train_df, test_df = df.iloc[train_index], df.iloc[test_index]

        for method_name, imputer in imputation_methods.items():
            # Fit the imputer on the training set
            imputer.fit(train_df)
            
            # Transform both the training and test sets
            imputed_train_df = pd.DataFrame(imputer.transform(train_df), columns=df.columns)
            imputed_test_df = pd.DataFrame(imputer.transform(test_df), columns=df.columns)
            
            for col in df.columns:
                if train_df[col].isna().sum() > 0 or test_df[col].isna().sum() > 0:
                    # Calculate RMSE for each column with missing values
                    y_true = test_df[col].dropna()
                    y_pred = imputed_test_df.loc[y_true.index, col]
                    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                    results[method_name].append(rmse)
    
    # Calculate the mean RMSE for each imputation method
    mean_results = {method: np.mean(rmses) for method, rmses in results.items()}
    
    return mean_results

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, imputation_methods)

# Display the results
print("Mean RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")

















Handle Missing Values: Impute missing values to avoid complications in subsequent steps.
Handle Outliers: Address outliers to ensure they do not skew the scaling and modeling.
Handle Categorical Data: Encode categorical variables.
Scale Data: Normalize or standardize the data.
Handle Multicollinearity: Check for and address multicollinearity.










import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")
























import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5,
    'Target': np.random.rand(100) * 100
})

# Calculate correlation matrix
corr_matrix = df.corr()

# Display correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# Calculate R-squared values
target_var = 'Target'
r2_results = {}

for col in df.columns:
    if col != target_var:
        X = df[[col]]
        y = df[target_var]
        model = LinearRegression().fit(X, y)
        y_pred = model.predict(X)
        r2 = r2_score(y, y_pred)
        r2_results[col] = r2

# Display R-squared results
r2_df = pd.DataFrame.from_dict(r2_results, orient='index', columns=['R-Squared']).sort_values(by='R-Squared', ascending=False)
print(r2_df)

# Plot R-squared values
plt.figure(figsize=(10, 6))
sns.barplot(x=r2_df.index, y='R-Squared', data=r2_df, palette='viridis')
plt.title('R-Squared Values of Variables Explaining Target')
plt.xlabel('Variables')
plt.ylabel('R-Squared')
plt.xticks(rotation=45)
plt.show()














import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to plot distribution and detect outliers
def plot_distribution_and_outliers(df):
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in num_cols:
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Histogram and KDE Plot
        sns.histplot(df[col], kde=True, ax=axes[0], color='blue')
        axes[0].set_title(f'Distribution of {col}')
        axes[0].set_xlabel(col)
        
        # Box Plot for outliers
        sns.boxplot(y=df[col], ax=axes[1], color='orange')
        axes[1].set_title(f'Box Plot of {col}')
        
        plt.tight_layout()
        plt.show()
        
        # Detect outliers using IQR method
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        
        print(f'{col}:')
        print(f'Number of outliers: {outliers.shape[0]}')
        if not outliers.empty:
            print('Outliers:')
            print(outliers[[col]])
        print('\n')

# Plot distributions and outliers
plot_distribution_and_outliers(df)








# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Set font size for the plot
plt.rcParams.update({'font.size': 14})

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5, annot_kws={"size": 14})

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black', fontsize=14)

plt.title('Correlation Matrix with p-values (Filtered)', fontsize=18)
plt.xlabel('Variables', fontsize=16)
plt.ylabel('Variables', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()














import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()
















# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.xticks(rotation=45)
plt.show()
















import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_vif(df):
    # Add a constant to the dataframe
    df_with_const = add_constant(df)
    
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = df_with_const.columns
    vif_data["VIF"] = [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])]
    
    return vif_data

# Calculate VIF
vif_data = calculate_vif(df)

# Display the VIF result
print(vif_data)


import matplotlib.pyplot as plt

# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.show()













import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j:
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values')
plt.show()
































import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Store the results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Store the metrics
    results['Model'].append(model_name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1-Score'].append(f1)
    
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")

# Plot the results
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Comparison')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
axes = axes.flatten()

for i, metric in enumerate(metrics):
    axes[i].bar(results['Model'], results[metric], color=['blue', 'green', 'red'])
    axes[i].set_title(metric)
    axes[i].set_ylim([0, 1])
    axes[i].set_xlabel('Model')
    axes[i].set_ylabel(metric)
    for j, val in enumerate(results[metric]):
        axes[i].text(j, val + 0.02, f'{val:.2f}', ha='center')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()




import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Display the matrices
print("Correlation Matrix:")
print(corr_matrix)
print("\nP-Value Matrix:")
print(p_value_matrix)

# Combine the correlation matrix and p-value matrix into one DataFrame for display
combined_matrix = corr_matrix.astype(str) + " (p=" + p_value_matrix.round(4).astype(str) + ")"

print("\nCombined Matrix:")
print(combined_matrix)
















import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")
