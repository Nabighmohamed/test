import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe for testing
data = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'pnb': [100, 200, 300, 400, 500],
    'pnb2023': [150, 250, 350, 450, 550]
})

# Reshape the data to have a long format
data_long = pd.melt(data, id_vars=['cclikpi'], value_vars=['pnb', 'pnb2023'],
                    var_name='year', value_name='pnb_value')

# Map the variable names to actual years
data_long['year'] = data_long['year'].map({'pnb': 2018, 'pnb2023': 2023})

# Plot the evolution of PNB for each company
plt.figure(figsize=(10, 6))
for key, grp in data_long.groupby(['cclikpi']):
    plt.plot(grp['year'], grp['pnb_value'], marker='o', label=f'Company {key}')

plt.xlabel('Year')
plt.ylabel('PNB Value')
plt.title('Evolution of PNB from 2018 to 2023 for Each Company')
plt.legend(title='cclikpi')
plt.grid(True)
plt.show()










import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def find_new_rows_added(df1, df2, id_column, merge_column):
    # Perform the merge
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left', indicator=True)
    
    # Find new rows added by checking for rows in df2 but not in df1
    new_rows = merged_df[merged_df['_merge'] == 'right_only']
    
    return new_rows

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Find new rows added during the merge
new_rows_added = find_new_rows_added(df1, df2, id_column, merge_column)

print("New Rows Added:")
print(new_rows_added)














import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'cclikpi': [1, 2, 3, 4, 5],
    'value1': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'cclikpi': [3, 4, 5, 6, 7],
    'value2': [100, 200, 300, 400, 500]
})

def merge_dataframes(df1, df2, id_column, merge_column):
    # Merge the two DataFrames on the id_column
    merged_df = pd.merge(df1, df2[[id_column, merge_column]], on=id_column, how='left')
    
    return merged_df

# Specify the column names
id_column = 'cclikpi'
merge_column = 'value2'

# Merge the dataframes and add the specified column from df2 to df1
result_df = merge_dataframes(df1, df2, id_column, merge_column)

print("Resulting DataFrame:")
print(result_df)













import pandas as pd

def get_column_types(df):
    # Create a dictionary with column names as keys and their data types as values
    column_types = {col: df[col].dtype for col in df.columns}
    return column_types

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 2, 3, 4, 5],
    'type_de_client': ['A', 'B', 'C', 'D', 'E'],
    'value1': [10, 20, 30, 40, 50],
    'value2': [100.0, 200.0, 300.0, 400.0, 500.0]
})

# Get the column types
column_types = get_column_types(data)
print("Column Types:")
print(column_types)









import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'category': ['X', 'Y', 'X', 'Y', 'X', 'X', 'Y'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_orders = {
    'type_de_client': ['A', 'B'],  # Priority order for 'type_de_client'
    'category': ['X', 'Y']         # Priority order for 'category'
}

# Custom aggregation function for categorical variables
def agg_priority(series, priority_order):
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Wrapper function to apply the correct priority order
def agg_wrapper(column, priority_orders):
    return lambda series: agg_priority(series, priority_orders[column])

# Perform the groupby operation and apply the custom aggregation functions
agg_dict = {
    'type_de_client': agg_wrapper('type_de_client', priority_orders),
    'category': agg_wrapper('category', priority_orders),
    'value1': 'sum',  # Example aggregation for numerical columns
    'value2': 'mean'  # Example aggregation for numerical columns
}

grouped_data = data.groupby('id').agg(agg_dict).reset_index()

print("Grouped DataFrame:")
print(grouped_data)






import pandas as pd

# Sample dataframe for testing
data = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 3],
    'type_de_client': ['A', 'B', 'A', 'A', 'B', 'B', 'A'],
    'value1': [10, 20, 30, 40, 50, 60, 70],
    'value2': [100, 200, 300, 400, 500, 600, 700]
})

# Define priority order for categorical variables
priority_order = ['A', 'B']  # You can easily change this list later

# Custom aggregation function for categorical variables
def agg_priority(series):
    # Get the first value in the series that matches the priority order
    for priority in priority_order:
        if priority in series.values:
            return priority
    return series.values[0]  # Fallback to the first value if no priority match found

# Perform the groupby operation and apply the custom aggregation function
grouped_data = data.groupby('id').agg({
    'type_de_client': agg_priority,  # Apply the custom aggregation function
    'value1': 'sum',                 # Example aggregation for numerical columns
    'value2': 'mean'                 # Example aggregation for numerical columns
}).reset_index()

print("Grouped DataFrame:")
print(grouped_data)









import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column, class_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    # Calculate frequency and percentage of class distribution for matched IDs
    matched_class_freq = matched_df[class_column].value_counts()
    matched_class_percentage = matched_df[class_column].value_counts(normalize=True) * 100
    
    # Filter the DataFrame to get rows where the id is not in the id_list
    unmatched_df = df[~df[id_column].isin(id_list)]
    
    # Calculate frequency and percentage of class distribution for unmatched IDs
    unmatched_class_freq = unmatched_df[class_column].value_counts()
    unmatched_class_percentage = unmatched_df[class_column].value_counts(normalize=True) * 100
    
    return (matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
            unmatched_class_freq, unmatched_class_percentage)

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column names for IDs and class
id_column = 'id'
class_column = 'value'

# Get the results
(matched_df, unmatched_ids, matched_class_freq, matched_class_percentage,
 unmatched_class_freq, unmatched_class_percentage) = filter_dataframe_and_find_unmatched(large_df, id_list, id_column, class_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)
print("\nMatched Class Frequency:")
print(matched_class_freq)
print("\nMatched Class Percentage:")
print(matched_class_percentage)
print("\nUnmatched Class Frequency:")
print(unmatched_class_freq)
print("\nUnmatched Class Percentage:")
print(unmatched_class_percentage)









import pandas as pd

# Sample data for testing
df1 = pd.DataFrame({
    'ccli': [1, 2, 3, 4, 5],
    'cclikpi': ['a', 'b', 'c', 'd', 'e']
})

df2 = pd.DataFrame({
    'avant_ccli': [1, 2, 3, 6],
    'apre_ccli': [10, 20, 30, 60],
    'cclikpi': [100, 200, 300, 400]
})

# Create a dictionary from df2 for quick lookup
cclikpi_map = df2.set_index('avant_ccli')['cclikpi'].to_dict()

# Function to determine the value for cclikpi2
def get_cclikpi2(ccli, cclikpi):
    return cclikpi_map.get(ccli, cclikpi)

# Apply the function to create the new column in df1
df1['cclikpi2'] = df1.apply(lambda row: get_cclikpi2(row['ccli'], row['cclikpi']), axis=1)

print("Resulting DataFrame:")
print(df1)





import pandas as pd

def filter_by_id_pattern(df, id_column, pattern):
    # Convert the id column to string
    df[id_column] = df[id_column].astype(str)
    
    # Filter rows where the id starts with the specific pattern
    filtered_df = df[df[id_column].str.startswith(pattern)]
    
    return filtered_df

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [3001, 3002, 123, 4003, 3004, 567, 3005],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g']
})

# Specify the column name for IDs and the pattern
id_column = 'id'
pattern = '300'

# Get the filtered DataFrame
filtered_df = filter_by_id_pattern(large_df, id_column, pattern)

print("Filtered DataFrame:")
print(filtered_df)




import pandas as pd

def filter_dataframe_and_find_unmatched(df, id_list, id_column):
    # Filter the DataFrame to get rows where the id is in the id_list
    matched_df = df[df[id_column].isin(id_list)]
    
    # Find the IDs from the id_list that don't have a match in the DataFrame
    matched_ids = matched_df[id_column].unique()
    unmatched_ids = [id for id in id_list if id not in matched_ids]
    
    return matched_df, unmatched_ids

# Sample large dataframe for testing
large_df = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'value': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
})

# Sample list of IDs
id_list = [2, 4, 6, 11]

# Specify the column name for IDs
id_column = 'id'

# Get the results
matched_df, unmatched_ids = filter_dataframe_and_find_unmatched(large_df, id_list, id_column)

print("Matched DataFrame:")
print(matched_df)
print("\nUnmatched IDs:")
print(unmatched_ids)










Bonjour Solène,

Je suis en train d'agréger les données, mais j'ai un problème avec l'agrégation des variables catégorielles comme "type de foyer" ou "CNOUVSEG". Abidjo et Arthur m'ont donné une liste d'ordre de priorité pour certaines catégories, mais je ne suis pas sûr si cela est toujours correct pour mon cas, et en plus, elle n'est pas exhaustive. Aurais-tu des idées ou des suggestions pour m'aider à résoudre ce problème?

Merci d'avance pour ton aide.


Aurais-tu une liste de priorité pour le reste des variables catégorielles?

Je vous remercie sincèrement pour votre offre d'aide. Je n'hésiterai pas à faire appel à vous si j'ai besoin. Votre soutien est grandement apprécié


Bonjour Solène,

Je suis en train d'agréger les données, mais j'ai un problème avec l'agrégation des variables catégorielles comme "type de foyer" ou "CNOUVSEG". Aurais-tu des idées ou des suggestions pour m'aider à résoudre ce problème?

Merci d'avance pour ton aide.










import pandas as pd

def get_values_for_same_col1(data, col1, col2):
    # Group by the first column and collect the values of the second column
    result = data.groupby(col1)[col2].apply(list).to_dict()
    return result

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'c', 'd', 'e', 'f', 'g'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to use
col1 = 'col1'
col2 = 'col2'

values_for_same_col1 = get_values_for_same_col1(data, col1, col2)
print(values_for_same_col1)











import pandas as pd

def get_duplicates_across_columns(data, col1, col2):
    # Find duplicate rows based on the specified columns
    duplicate_rows = data[data.duplicated(subset=[col1, col2], keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=[col1, col2])
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 30, 40, 50, 60, 70]
})

# Specify the columns to check for duplicates
col1 = 'col1'
col2 = 'col2'

duplicates = get_duplicates_across_columns(data, col1, col2)
print(duplicates)











import pandas as pd

def get_duplicates(data):
    # Find duplicate rows based on all columns
    duplicate_rows = data[data.duplicated(keep=False)]
    
    # Sort the duplicate rows to place duplicates next to each other
    duplicate_rows_sorted = duplicate_rows.sort_values(by=list(data.columns))
    
    return duplicate_rows_sorted

# Sample dataframe for testing
data = pd.DataFrame({
    'col1': [1, 2, 2, 4, 5, 2, 1],
    'col2': ['a', 'b', 'b', 'd', 'e', 'b', 'a'],
    'col3': [10, 20, 20, 40, 50, 20, 10]
})

duplicates = get_duplicates(data)
print(duplicates)













import pandas as pd
import numpy as np

def get_indices_not_in_data2(data1, data2, col_name):
    # Drop NaN values from the target column in both dataframes
    data1_clean = data1.dropna(subset=[col_name])
    data2_clean = data2.dropna(subset=[col_name])
    
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2_clean[col_name].astype(data1_clean[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1_clean[~data1_clean[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5, np.nan],
    'value': ['a', 'b', 'c', 'd', 'e', 'f']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0, np.nan],
    'value': ['x', 'y', 'z', 'w']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)







import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Convert the column in data2 to the same type as in data1
    data2_converted = data2[col_name].astype(data1[col_name].dtype)
    
    # Get the unique values in the converted column of data2
    data2_values = set(data2_converted.unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3.0, 4.0, 5.0],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)











import pandas as pd

def get_indices_not_in_data2(data1, data2, col_name):
    # Get the unique values in the specified column of data2
    data2_values = set(data2[col_name].unique())
    
    # Find the indices in data1 where the values in the specified column are not in data2_values
    indices_not_in_data2 = data1[~data1[col_name].isin(data2_values)].index.tolist()
    
    return indices_not_in_data2

# Sample data for testing
data1 = pd.DataFrame({
    'col': [1, 2, 3, 4, 5],
    'value': ['a', 'b', 'c', 'd', 'e']
})

data2 = pd.DataFrame({
    'col': [3, 4, 5],
    'value': ['x', 'y', 'z']
})

col_name = 'col'
indices = get_indices_not_in_data2(data1, data2, col_name)
print(indices)










import pandas as pd
import numpy as np

# Sample dataframe
data = {
    'ccli': [1, 1, 1, 2, 2, 2, 3, 3, 3, 3],
    'cclikpi': [10, 10, 10, 20, 20, 20, 30, 30, 30, 30],
    'col': [12, 12, 12, 11, 11, 11, 12, 12, 12, 12],  # The column with values from 1 to 12
    'value1': [0, 200, np.nan, 400, 0, 600, 700, 0, np.nan, 0],
    'value2': [100, 0, 0, 300, np.nan, 0, 500, 600, 0, 0]
}

df = pd.DataFrame(data)

# Function to calculate the number of zeros and NaNs in each row
def count_zeros_nans(row):
    return (row == 0).sum() + row.isna().sum()

# Add a column that counts the number of zeros and NaNs in each row
df['zeros_nans_count'] = df.apply(count_zeros_nans, axis=1)

# Sort the dataframe by 'ccli', 'cclikpi', 'col' and 'zeros_nans_count'
df = df.sort_values(by=['ccli', 'cclikpi', 'col', 'zeros_nans_count'])

# Drop duplicates keeping the first one based on 'ccli', 'cclikpi', and 'col' (the one with fewest zeros/nans)
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi', 'col'], keep='first')

# Drop the helper column as it's no longer needed
df_unique = df_unique.drop(columns=['zeros_nans_count'])

# Display the resulting dataframe
print(df_unique)



import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'col': [1, 12, 2, 11, 3, 6, 12],  # The column with values from 1 to 12
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Sort the dataframe by 'ccli', 'cclikpi' and 'col'
df = df.sort_values(by=['ccli', 'cclikpi', 'col'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)















import pandas as pd

# Sample dataframe
data = {
    'ccli': [1, 1, 2, 2, 3, 3, 3],
    'cclikpi': [10, 10, 20, 20, 30, 30, 30],
    'date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-01', '2023-01-02', '2023-01-04'],
    'value': [100, 200, 300, 400, 500, 600, 700]
}

df = pd.DataFrame(data)

# Convert date column to datetime
df['date'] = pd.to_datetime(df['date'])

# Sort the dataframe by 'ccli', 'cclikpi' and 'date'
df = df.sort_values(by=['ccli', 'cclikpi', 'date'])

# Drop duplicates keeping the last one based on 'ccli' and 'cclikpi'
df_unique = df.drop_duplicates(subset=['ccli', 'cclikpi'], keep='last')

# Display the resulting dataframe
print(df_unique)










Certainly! Here is an in-depth explanation of the first ten laws from Robert Greene's "The 48 Laws of Power":

Law 1: Never Outshine the Master
Concept: Always make those above you feel superior. Do not highlight your own talents to the point of making them feel insecure.
Application: Subtly display your skills while emphasizing your deference to your superiors. Praise their wisdom and ability, ensuring they receive credit.
Example: In the court of Louis XIV, his finance minister, Nicolas Fouquet, outshone the king by throwing an extravagant party. This led to Fouquet's downfall as Louis XIV felt threatened and imprisoned him.
Law 2: Never Put Too Much Trust in Friends, Learn How to Use Enemies
Concept: Friends are more likely to betray you as they become envious or complacent. Enemies, motivated by self-interest, are more reliable when properly managed.
Application: Be cautious with friends and keep a degree of separation. Convert enemies into allies by demonstrating that cooperation is mutually beneficial.
Example: Michael III of the Byzantine Empire relied on his friend Basil the Macedonian, who ultimately betrayed and killed him to take power.
Law 3: Conceal Your Intentions
Concept: Keep people off-balance and in the dark about your plans. This makes them unable to counter your moves.
Application: Use red herrings and decoys to mislead others. Keep your real goals hidden and reveal only enough to maintain trust.
Example: Otto von Bismarck, the Prussian statesman, used various deceptions to keep his opponents guessing about his true political intentions, which allowed him to unify Germany.
Law 4: Always Say Less Than Necessary
Concept: The more you say, the more likely you are to say something foolish. Silence and brevity create an aura of power.
Application: Be concise and deliberate with your words. Let others reveal more than you do, gaining insight into their thoughts and intentions.
Example: When interviewed, Andy Warhol often gave brief, enigmatic answers, which increased his mystique and allure in the art world.
Law 5: So Much Depends on Reputation – Guard It with Your Life
Concept: Your reputation is the cornerstone of your power. A strong reputation increases your presence and influence, while a tarnished reputation diminishes your power.
Application: Protect your reputation at all costs and be vigilant against attacks. Use subtle tactics to undermine the reputations of your rivals.
Example: Julius Caesar carefully cultivated his reputation for being generous and merciful, which helped him gain the loyalty and support of the Roman populace.
Law 6: Court Attention at All Costs
Concept: Stand out and make yourself noticed. Being unseen or forgotten is detrimental to your power.
Application: Create memorable and dramatic events or actions that capture attention. Cultivate an air of mystery and unpredictability.
Example: P.T. Barnum, the famous showman, constantly created public spectacles and stunts to keep himself and his shows in the public eye.
Law 7: Get Others to Do the Work for You, but Always Take the Credit
Concept: Use the skills, knowledge, and effort of others to further your own goals. Taking credit for their work enhances your reputation and power.
Application: Delegate tasks while ensuring that you receive recognition for the results. Reward those who contribute to keep them motivated.
Example: Thomas Edison often employed skilled engineers and researchers who helped him develop his inventions, but he ensured that he was recognized as the primary inventor.
Law 8: Make Other People Come to You – Use Bait If Necessary
Concept: By making others come to you, you control the situation and set the terms. Luring people to you ensures they play by your rules.
Application: Use allurements and incentives to attract people. Once they are drawn in, assert your dominance and influence.
Example: During negotiations, Henry Kissinger would often make foreign diplomats come to him, allowing him to dictate the terms and maintain a position of strength.
Law 9: Win Through Your Actions, Never Through Argument
Concept: Demonstrating your ideas through actions is far more persuasive than arguing. Arguments often lead to resentment and resistance.
Application: Let your actions speak for you. Show, don’t tell, and allow people to see the benefits or consequences of your ideas firsthand.
Example: Instead of arguing with critics, Leonardo da Vinci would demonstrate his inventions and artworks, which left little room for doubt about his genius.
Law 10: Infection: Avoid the Unhappy and Unlucky
Concept: Emotions and attitudes are contagious. Associating with those who are negative or unlucky can bring you down.
Application: Distance yourself from those who have a detrimental effect on your well-being and success. Seek out positive and successful individuals.
Example: The French adventurer, Napoleon Bonaparte, avoided associating with people who were constantly negative or failed, focusing instead on those who were optimistic and successful.
These laws emphasize strategic thinking, careful manipulation, and maintaining control over social and professional interactions to enhance one's power and influen


















import pandas as pd
import numpy as np
from scipy.stats.mstats import winsorize

# Sample DataFrame with numeric data (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.randn(100) * 100,
    'B': np.random.randn(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to apply Winsorization to all numeric columns in the DataFrame
def winsorize_dataframe(df, limits=(0.05, 0.05)):
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    df_winsorized = df.copy()
    
    for col in numeric_cols:
        df_winsorized[col] = winsorize(df[col], limits=limits)
    
    return df_winsorized

# Apply Winsorization to the DataFrame
winsorized_df = winsorize_dataframe(df, limits=(0.05, 0.05))

# Display the original and Winsorized DataFrame
print("Original DataFrame:")
print(df.describe())

print("\nWinsorized DataFrame:")
print(winsorized_df.describe())













import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy.stats import f_oneway

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to perform ANOVA test for each categorical variable against the continuous target variable
def anova_test(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"ANOVA test for {col} in relation to {target_var}:")
        
        # Perform ANOVA using statsmodels
        formula = f'{target_var} ~ C({col})'
        model = ols(formula, data=df).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)
        
        print(anova_table)
        print("\n")

# Specify the target column
target_column = 'Target'

# Perform ANOVA tests
anova_test(df, target_column)



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables and a continuous target variable
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100),
    'Target': np.random.rand(100) * 100  # Continuous target variable
})

# Function to generate statistical summaries and plots for categorical variables in relation to a continuous target variable
def categorical_summary_with_target(df, target_var):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col} in relation to {target_var}:")
        summary = df.groupby(col)[target_var].describe()
        print(summary)
        print("\n")
        
        # Plotting the distribution of the continuous target variable for each category using box plot
        plt.figure(figsize=(8, 6))
        sns.boxplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

        # Plotting the distribution of the continuous target variable for each category using violin plot
        plt.figure(figsize=(8, 6))
        sns.violinplot(data=df, x=col, y=target_var, palette='viridis')
        plt.title(f'{target_var} distribution by {col}')
        plt.xlabel(col)
        plt.ylabel(target_var)
        plt.xticks(rotation=45)
        plt.show()

# Specify the target column
target_column = 'Target'

# Generate and plot summaries for categorical variables in relation to the continuous target variable
categorical_summary_with_target(df, target_column)













import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100)
})

# Function to generate statistical summaries for categorical variables
def categorical_summary(df):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col}:")
        print(df[col].value_counts())
        print("\n")
        
        # Plotting the distribution of the categorical variable
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x=col, palette='viridis')
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.show()

# Generate and plot summaries for categorical variables
categorical_summary(df)




















import pandas as pd
import numpy as np

# Sample data
data = {'values': [1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]}
df = pd.DataFrame(data)

# Central Tendency
mean = df['values'].mean()
median = df['values'].median()
mode = df['values'].mode()

# Variability
range_ = df['values'].max() - df['values'].min()
variance = df['values'].var()
std_dev = df['values'].std()
iqr = df['values'].quantile(0.75) - df['values'].quantile(0.25)

# Shape
skewness = df['values'].skew()
kurtosis = df['values'].kurt()

# Summary
summary = df['values'].describe()

print("Mean:", mean)
print("Median:", median)
print("Mode:", mode[0])
print("Range:", range_)
print("Variance:", variance)
print("Standard Deviation:", std_dev)
print("Interquartile Range (IQR):", iqr)
print("Skewness:", skewness)
print("Kurtosis:", kurtosis)
print("\nSummary:\n", summary)




















import pandas as pd

# Sample data
data = {
    'Column1': ['A', 'B', 'A', 'C', 'B', 'A', 'B', 'C'],
    'Column2': [1, 2, 1, 3, 2, 1, 2, 3]
}

df = pd.DataFrame(data)

# Check for duplicates based on Column1 and Column2
duplicates = df.duplicated(subset=['Column1', 'Column2'])
print("Boolean Series indicating duplicates:")
print(duplicates)

# Get only the rows that are duplicates
duplicate_rows = df[df.duplicated(subset=['Column1', 'Column2'], keep=False)]
print("\nDuplicate rows:")
print(duplicate_rows)















duplicates = df.duplicated(subset=['Column1', 'Column2'])
print(duplicates)









import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Split the data into training (non-missing target values) and testing (missing target values) sets
train_df = prepared_df[prepared_df[target_column].notna()]
test_df = prepared_df[prepared_df[target_column].isna()]

# Define features and target
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]
X_test = test_df.drop(columns=[target_column])

# Train the XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)

# Predict the missing values
y_pred = xgb_model.predict(X_test)

# Evaluate the model (Since we don't have true values for test set, we can't compute RMSE here)
# In practice, you would compare y_pred with the true values if they were known

# Add predictions back to the original DataFrame
prepared_df.loc[prepared_df[target_column].isna(), target_column] = y_pred

# Display the results
print("Predicted values for the missing target variable:")
print(prepared_df[prepared_df[target_column].isna()][[target_column]])

# Optionally, if we had true values, we would compute the RMSE like this:
# y_true = true_values_array_for_missing_entries
# rmse = np.sqrt(mean_squared_error(y_true, y_pred))
# print(f"RMSE for XGBoost predictions: {rmse}")













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    
    # Extract the rows with missing values in the target column
    missing_df = df[df[target_column].isna()]
    
    # Extract the rows with non-missing values in the target column
    non_missing_df = df[df[target_column].notna()]
    
    # Extract the true values of the target column for evaluation
    y_true = non_missing_df[target_column].copy()
    
    for method_name, imputer in imputation_methods.items():
        df_copy = non_missing_df.dropna(axis=1, how='any')
        df_copy[target_column] = non_missing_df[target_column]
        
        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        
        y_pred = imputed_df[target_column]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")























import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    y_true = df[target_column][df[target_column].isna()]

    for method_name, imputer in imputation_methods.items():
        df_copy = df.copy()
        df_copy = df_copy.dropna(axis=1, how='any')
        df_copy[target_column] = df[target_column]

        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)

        y_pred = imputed_df[target_column][df[target_column].isna()]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    # Create a copy of the DataFrame
    df_copy = df.copy()
    
    # Separate the target column
    target = df_copy[[target_column]].copy()
    
    # Drop columns with missing values
    df_copy = df_copy.dropna(axis=1, how='any')
    
    # Add the target column back
    df_copy = pd.concat([df_copy, target], axis=1)
    
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Display the prepared DataFrame
print(prepared_df.head())













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        # Ensure the training data does not contain NaNs
        X_train_imputed = imputer.fit_transform(X_train)
        imputed_train_df = pd.DataFrame(X_train_imputed, columns=X_train.columns)
        
        # Impute missing values in the test set using the trained imputer
        X_test = test_df.drop(columns=[target_column])
        X_test_imputed = imputer.transform(X_test)
        imputed_test_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)
        imputed_test_df[target_column] = test_df[target_column]
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")



















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Drop columns with missing values except for the target column
        df_copy = df_copy.dropna(axis=1, how='any', subset=[col for col in df.columns if col != target_column])
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")






















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques using cross-validation
def evaluate_imputation_methods(df, imputation_methods):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    results = {method: [] for method in imputation_methods.keys()}

    for train_index, test_index in kf.split(df):
        train_df, test_df = df.iloc[train_index], df.iloc[test_index]

        for method_name, imputer in imputation_methods.items():
            # Fit the imputer on the training set
            imputer.fit(train_df)
            
            # Transform both the training and test sets
            imputed_train_df = pd.DataFrame(imputer.transform(train_df), columns=df.columns)
            imputed_test_df = pd.DataFrame(imputer.transform(test_df), columns=df.columns)
            
            for col in df.columns:
                if train_df[col].isna().sum() > 0 or test_df[col].isna().sum() > 0:
                    # Calculate RMSE for each column with missing values
                    y_true = test_df[col].dropna()
                    y_pred = imputed_test_df.loc[y_true.index, col]
                    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                    results[method_name].append(rmse)
    
    # Calculate the mean RMSE for each imputation method
    mean_results = {method: np.mean(rmses) for method, rmses in results.items()}
    
    return mean_results

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, imputation_methods)

# Display the results
print("Mean RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")

















Handle Missing Values: Impute missing values to avoid complications in subsequent steps.
Handle Outliers: Address outliers to ensure they do not skew the scaling and modeling.
Handle Categorical Data: Encode categorical variables.
Scale Data: Normalize or standardize the data.
Handle Multicollinearity: Check for and address multicollinearity.










import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")
























import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5,
    'Target': np.random.rand(100) * 100
})

# Calculate correlation matrix
corr_matrix = df.corr()

# Display correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# Calculate R-squared values
target_var = 'Target'
r2_results = {}

for col in df.columns:
    if col != target_var:
        X = df[[col]]
        y = df[target_var]
        model = LinearRegression().fit(X, y)
        y_pred = model.predict(X)
        r2 = r2_score(y, y_pred)
        r2_results[col] = r2

# Display R-squared results
r2_df = pd.DataFrame.from_dict(r2_results, orient='index', columns=['R-Squared']).sort_values(by='R-Squared', ascending=False)
print(r2_df)

# Plot R-squared values
plt.figure(figsize=(10, 6))
sns.barplot(x=r2_df.index, y='R-Squared', data=r2_df, palette='viridis')
plt.title('R-Squared Values of Variables Explaining Target')
plt.xlabel('Variables')
plt.ylabel('R-Squared')
plt.xticks(rotation=45)
plt.show()














import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to plot distribution and detect outliers
def plot_distribution_and_outliers(df):
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in num_cols:
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Histogram and KDE Plot
        sns.histplot(df[col], kde=True, ax=axes[0], color='blue')
        axes[0].set_title(f'Distribution of {col}')
        axes[0].set_xlabel(col)
        
        # Box Plot for outliers
        sns.boxplot(y=df[col], ax=axes[1], color='orange')
        axes[1].set_title(f'Box Plot of {col}')
        
        plt.tight_layout()
        plt.show()
        
        # Detect outliers using IQR method
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        
        print(f'{col}:')
        print(f'Number of outliers: {outliers.shape[0]}')
        if not outliers.empty:
            print('Outliers:')
            print(outliers[[col]])
        print('\n')

# Plot distributions and outliers
plot_distribution_and_outliers(df)








# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Set font size for the plot
plt.rcParams.update({'font.size': 14})

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5, annot_kws={"size": 14})

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black', fontsize=14)

plt.title('Correlation Matrix with p-values (Filtered)', fontsize=18)
plt.xlabel('Variables', fontsize=16)
plt.ylabel('Variables', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()














import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()
















# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.xticks(rotation=45)
plt.show()
















import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_vif(df):
    # Add a constant to the dataframe
    df_with_const = add_constant(df)
    
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = df_with_const.columns
    vif_data["VIF"] = [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])]
    
    return vif_data

# Calculate VIF
vif_data = calculate_vif(df)

# Display the VIF result
print(vif_data)


import matplotlib.pyplot as plt

# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.show()













import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j:
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values')
plt.show()
































import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Store the results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Store the metrics
    results['Model'].append(model_name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1-Score'].append(f1)
    
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")

# Plot the results
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Comparison')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
axes = axes.flatten()

for i, metric in enumerate(metrics):
    axes[i].bar(results['Model'], results[metric], color=['blue', 'green', 'red'])
    axes[i].set_title(metric)
    axes[i].set_ylim([0, 1])
    axes[i].set_xlabel('Model')
    axes[i].set_ylabel(metric)
    for j, val in enumerate(results[metric]):
        axes[i].text(j, val + 0.02, f'{val:.2f}', ha='center')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()




import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Display the matrices
print("Correlation Matrix:")
print(corr_matrix)
print("\nP-Value Matrix:")
print(p_value_matrix)

# Combine the correlation matrix and p-value matrix into one DataFrame for display
combined_matrix = corr_matrix.astype(str) + " (p=" + p_value_matrix.round(4).astype(str) + ")"

print("\nCombined Matrix:")
print(combined_matrix)
















import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")
