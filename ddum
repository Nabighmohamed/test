import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with categorical variables (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'Category1': np.random.choice(['A', 'B', 'C'], size=100),
    'Category2': np.random.choice(['X', 'Y', 'Z'], size=100),
    'Category3': np.random.choice(['M', 'N'], size=100)
})

# Function to generate statistical summaries for categorical variables
def categorical_summary(df):
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in cat_cols:
        print(f"Summary for {col}:")
        print(df[col].value_counts())
        print("\n")
        
        # Plotting the distribution of the categorical variable
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x=col, palette='viridis')
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.show()

# Generate and plot summaries for categorical variables
categorical_summary(df)




















import pandas as pd
import numpy as np

# Sample data
data = {'values': [1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]}
df = pd.DataFrame(data)

# Central Tendency
mean = df['values'].mean()
median = df['values'].median()
mode = df['values'].mode()

# Variability
range_ = df['values'].max() - df['values'].min()
variance = df['values'].var()
std_dev = df['values'].std()
iqr = df['values'].quantile(0.75) - df['values'].quantile(0.25)

# Shape
skewness = df['values'].skew()
kurtosis = df['values'].kurt()

# Summary
summary = df['values'].describe()

print("Mean:", mean)
print("Median:", median)
print("Mode:", mode[0])
print("Range:", range_)
print("Variance:", variance)
print("Standard Deviation:", std_dev)
print("Interquartile Range (IQR):", iqr)
print("Skewness:", skewness)
print("Kurtosis:", kurtosis)
print("\nSummary:\n", summary)




















import pandas as pd

# Sample data
data = {
    'Column1': ['A', 'B', 'A', 'C', 'B', 'A', 'B', 'C'],
    'Column2': [1, 2, 1, 3, 2, 1, 2, 3]
}

df = pd.DataFrame(data)

# Check for duplicates based on Column1 and Column2
duplicates = df.duplicated(subset=['Column1', 'Column2'])
print("Boolean Series indicating duplicates:")
print(duplicates)

# Get only the rows that are duplicates
duplicate_rows = df[df.duplicated(subset=['Column1', 'Column2'], keep=False)]
print("\nDuplicate rows:")
print(duplicate_rows)















duplicates = df.duplicated(subset=['Column1', 'Column2'])
print(duplicates)









import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Split the data into training (non-missing target values) and testing (missing target values) sets
train_df = prepared_df[prepared_df[target_column].notna()]
test_df = prepared_df[prepared_df[target_column].isna()]

# Define features and target
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]
X_test = test_df.drop(columns=[target_column])

# Train the XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)

# Predict the missing values
y_pred = xgb_model.predict(X_test)

# Evaluate the model (Since we don't have true values for test set, we can't compute RMSE here)
# In practice, you would compare y_pred with the true values if they were known

# Add predictions back to the original DataFrame
prepared_df.loc[prepared_df[target_column].isna(), target_column] = y_pred

# Display the results
print("Predicted values for the missing target variable:")
print(prepared_df[prepared_df[target_column].isna()][[target_column]])

# Optionally, if we had true values, we would compute the RMSE like this:
# y_true = true_values_array_for_missing_entries
# rmse = np.sqrt(mean_squared_error(y_true, y_pred))
# print(f"RMSE for XGBoost predictions: {rmse}")













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    
    # Extract the rows with missing values in the target column
    missing_df = df[df[target_column].isna()]
    
    # Extract the rows with non-missing values in the target column
    non_missing_df = df[df[target_column].notna()]
    
    # Extract the true values of the target column for evaluation
    y_true = non_missing_df[target_column].copy()
    
    for method_name, imputer in imputation_methods.items():
        df_copy = non_missing_df.dropna(axis=1, how='any')
        df_copy[target_column] = non_missing_df[target_column]
        
        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        
        y_pred = imputed_df[target_column]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")























import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import warnings
from fancyimpute import IterativeSVD, MatrixFactorization, SoftImpute, BiScaler, KNN

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    df_copy = df.copy()
    target = df_copy[[target_column]].copy()
    df_copy = df_copy.dropna(axis=1, how='any')
    df_copy = pd.concat([df_copy, target], axis=1)
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Define imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'Most_frequent': SimpleImputer(strategy='most_frequent'),
    'KNN_5': KNNImputer(n_neighbors=5),
    'KNN_10': KNNImputer(n_neighbors=10),
    'Iterative_Ridge': IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42),
    'Iterative_RF': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42),
    'IterativeSVD': IterativeSVD(),
    'MatrixFactorization': MatrixFactorization(),
    'SoftImpute': SoftImpute(),
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}
    y_true = df[target_column][df[target_column].isna()]

    for method_name, imputer in imputation_methods.items():
        df_copy = df.copy()
        df_copy = df_copy.dropna(axis=1, how='any')
        df_copy[target_column] = df[target_column]

        if isinstance(imputer, SimpleImputer) or isinstance(imputer, KNNImputer) or isinstance(imputer, IterativeImputer):
            imputed_data = imputer.fit_transform(df_copy)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)
        else:
            imputed_data = imputer.fit_transform(df_copy.values)
            imputed_df = pd.DataFrame(imputed_data, columns=df_copy.columns)

        y_pred = imputed_df[target_column][df[target_column].isna()]
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results[method_name] = rmse

    return results

# Evaluate the imputation methods
results = evaluate_imputation_methods(prepared_df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Function to copy target column, drop columns with missing values, and add target column back
def prepare_dataframe(df, target_column):
    # Create a copy of the DataFrame
    df_copy = df.copy()
    
    # Separate the target column
    target = df_copy[[target_column]].copy()
    
    # Drop columns with missing values
    df_copy = df_copy.dropna(axis=1, how='any')
    
    # Add the target column back
    df_copy = pd.concat([df_copy, target], axis=1)
    
    return df_copy

# Specify the target column
target_column = 'A'

# Prepare the DataFrame
prepared_df = prepare_dataframe(df, target_column)

# Display the prepared DataFrame
print(prepared_df.head())













import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        # Ensure the training data does not contain NaNs
        X_train_imputed = imputer.fit_transform(X_train)
        imputed_train_df = pd.DataFrame(X_train_imputed, columns=X_train.columns)
        
        # Impute missing values in the test set using the trained imputer
        X_test = test_df.drop(columns=[target_column])
        X_test_imputed = imputer.transform(X_test)
        imputed_test_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)
        imputed_test_df[target_column] = test_df[target_column]
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")












import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Separate the target column
        target = df_copy[target_column]
        
        # Drop columns with missing values
        df_copy = df_copy.dropna(axis=1, how='any')
        
        # Add the target column back
        df_copy[target_column] = target
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")



















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Drop columns with missing values except for the target column
        df_copy = df_copy.dropna(axis=1, how='any', subset=[col for col in df.columns if col != target_column])
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")






















import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques using cross-validation
def evaluate_imputation_methods(df, imputation_methods):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    results = {method: [] for method in imputation_methods.keys()}

    for train_index, test_index in kf.split(df):
        train_df, test_df = df.iloc[train_index], df.iloc[test_index]

        for method_name, imputer in imputation_methods.items():
            # Fit the imputer on the training set
            imputer.fit(train_df)
            
            # Transform both the training and test sets
            imputed_train_df = pd.DataFrame(imputer.transform(train_df), columns=df.columns)
            imputed_test_df = pd.DataFrame(imputer.transform(test_df), columns=df.columns)
            
            for col in df.columns:
                if train_df[col].isna().sum() > 0 or test_df[col].isna().sum() > 0:
                    # Calculate RMSE for each column with missing values
                    y_true = test_df[col].dropna()
                    y_pred = imputed_test_df.loc[y_true.index, col]
                    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                    results[method_name].append(rmse)
    
    # Calculate the mean RMSE for each imputation method
    mean_results = {method: np.mean(rmses) for method, rmses in results.items()}
    
    return mean_results

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, imputation_methods)

# Display the results
print("Mean RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")

















Handle Missing Values: Impute missing values to avoid complications in subsequent steps.
Handle Outliers: Address outliers to ensure they do not skew the scaling and modeling.
Handle Categorical Data: Encode categorical variables.
Scale Data: Normalize or standardize the data.
Handle Multicollinearity: Check for and address multicollinearity.










import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings("ignore")

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

# Introduce some missing values for demonstration
for col in df.columns:
    df.loc[df.sample(frac=0.2).index, col] = np.nan

# Define the imputation methods
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5),
    'MICE': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)
}

# Function to evaluate imputation techniques
def evaluate_imputation_methods(df, target_column, imputation_methods):
    results = {}

    for method_name, imputer in imputation_methods.items():
        # Create a copy of the DataFrame
        df_copy = df.copy()
        
        # Split the data into train and test sets, including only non-missing target values
        train_df = df_copy[df_copy[target_column].notna()]
        test_df = df_copy[df_copy[target_column].isna()]
        
        if test_df.empty:
            raise ValueError("No missing values in the target column to impute.")
        
        X_train = train_df.drop(columns=[target_column])
        y_train = train_df[target_column]
        
        X_test = test_df.drop(columns=[target_column])
        
        # Impute missing values in the training set
        imputed_train = imputer.fit_transform(train_df)
        imputed_train_df = pd.DataFrame(imputed_train, columns=df_copy.columns)
        
        # Impute missing values in the test set
        imputed_test = imputer.transform(test_df)
        imputed_test_df = pd.DataFrame(imputed_test, columns=df_copy.columns)
        
        # Evaluate imputation
        if target_column in imputed_test_df.columns:
            # Compute RMSE for continuous variables
            y_true = df_copy[target_column][df_copy[target_column].isna()]
            y_pred = imputed_test_df[target_column]
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            results[method_name] = rmse
        else:
            # For categorical data, use an appropriate metric (e.g., accuracy)
            pass

    return results

# Specify the target column (column with missing values to be imputed)
target_column = 'A'

# Evaluate the imputation methods
results = evaluate_imputation_methods(df, target_column, imputation_methods)

# Display the results
print("RMSE for different imputation methods:")
for method, score in results.items():
    print(f"{method}: {score}")

# Determine the best imputation method
best_method = min(results, key=results.get)
print(f"\nBest imputation method: {best_method} with RMSE: {results[best_method]}")
























import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Sample DataFrame (replace this with your actual data)
np.random.seed(42)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5,
    'Target': np.random.rand(100) * 100
})

# Calculate correlation matrix
corr_matrix = df.corr()

# Display correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# Calculate R-squared values
target_var = 'Target'
r2_results = {}

for col in df.columns:
    if col != target_var:
        X = df[[col]]
        y = df[target_var]
        model = LinearRegression().fit(X, y)
        y_pred = model.predict(X)
        r2 = r2_score(y, y_pred)
        r2_results[col] = r2

# Display R-squared results
r2_df = pd.DataFrame.from_dict(r2_results, orient='index', columns=['R-Squared']).sort_values(by='R-Squared', ascending=False)
print(r2_df)

# Plot R-squared values
plt.figure(figsize=(10, 6))
sns.barplot(x=r2_df.index, y='R-Squared', data=r2_df, palette='viridis')
plt.title('R-Squared Values of Variables Explaining Target')
plt.xlabel('Variables')
plt.ylabel('R-Squared')
plt.xticks(rotation=45)
plt.show()














import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100) * 100,
    'B': np.random.rand(100) * 50 + 50,
    'C': np.random.rand(100) * 20 - 10,
    'D': np.random.randn(100) * 5
})

# Function to plot distribution and detect outliers
def plot_distribution_and_outliers(df):
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in num_cols:
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Histogram and KDE Plot
        sns.histplot(df[col], kde=True, ax=axes[0], color='blue')
        axes[0].set_title(f'Distribution of {col}')
        axes[0].set_xlabel(col)
        
        # Box Plot for outliers
        sns.boxplot(y=df[col], ax=axes[1], color='orange')
        axes[1].set_title(f'Box Plot of {col}')
        
        plt.tight_layout()
        plt.show()
        
        # Detect outliers using IQR method
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        
        print(f'{col}:')
        print(f'Number of outliers: {outliers.shape[0]}')
        if not outliers.empty:
            print('Outliers:')
            print(outliers[[col]])
        print('\n')

# Plot distributions and outliers
plot_distribution_and_outliers(df)








# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Set font size for the plot
plt.rcParams.update({'font.size': 14})

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5, annot_kws={"size": 14})

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black', fontsize=14)

plt.title('Correlation Matrix with p-values (Filtered)', fontsize=18)
plt.xlabel('Variables', fontsize=16)
plt.ylabel('Variables', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()














import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_correlation_matrix(df, threshold=0.70):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
    
    # Filter pairs with correlation above the threshold
    corr_matrix_filtered = corr_matrix.copy()
    p_value_matrix_filtered = p_value_matrix.copy()
    
    for i in range(n):
        for j in range(n):
            if abs(corr_matrix.iloc[i, j]) < threshold:
                corr_matrix_filtered.iloc[i, j] = np.nan
                p_value_matrix_filtered.iloc[i, j] = np.nan
    
    return corr_matrix_filtered, p_value_matrix_filtered

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df, threshold=0.70)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j and not np.isnan(corr_matrix.iloc[i, j]):
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values (Filtered)')
plt.show()
















# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.xticks(rotation=45)
plt.show()
















import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100),
    'E': np.random.rand(100),
    'F': np.random.rand(100)
})

def calculate_vif(df):
    # Add a constant to the dataframe
    df_with_const = add_constant(df)
    
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = df_with_const.columns
    vif_data["VIF"] = [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])]
    
    return vif_data

# Calculate VIF
vif_data = calculate_vif(df)

# Display the VIF result
print(vif_data)


import matplotlib.pyplot as plt

# Plotting the VIF values
plt.figure(figsize=(10, 6))
plt.bar(vif_data["Variable"], vif_data["VIF"], color='skyblue')
plt.xlabel('Variables')
plt.ylabel('VIF')
plt.title('Variance Inflation Factor (VIF)')
plt.axhline(y=5, color='r', linestyle='--')
plt.axhline(y=10, color='r', linestyle='--')
plt.show()













import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1,
            cbar_kws={"shrink": .8}, linewidths=0.5)

# Annotate the heatmap with p-values
for i in range(len(corr_matrix)):
    for j in range(len(corr_matrix)):
        if i < j:
            plt.text(j + 0.5, i + 0.5, f"p={p_value_matrix.iloc[i, j]:.4f}", ha='center', va='center', color='black')

plt.title('Correlation Matrix with p-values')
plt.show()
































import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Store the results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # Store the metrics
    results['Model'].append(model_name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1-Score'].append(f1)
    
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")

# Plot the results
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Comparison')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
axes = axes.flatten()

for i, metric in enumerate(metrics):
    axes[i].bar(results['Model'], results[metric], color=['blue', 'green', 'red'])
    axes[i].set_title(metric)
    axes[i].set_ylim([0, 1])
    axes[i].set_xlabel('Model')
    axes[i].set_ylabel(metric)
    for j, val in enumerate(results[metric]):
        axes[i].text(j, val + 0.02, f'{val:.2f}', ha='center')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()




import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Sample DataFrame (replace this with your actual data)
df = pd.DataFrame({
    'A': np.random.rand(100),
    'B': np.random.rand(100),
    'C': np.random.rand(100),
    'D': np.random.rand(100)
})

def calculate_correlation_matrix(df):
    # Get the list of columns
    cols = df.columns
    n = len(cols)
    
    # Initialize matrices to store correlation coefficients and p-values
    corr_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    p_value_matrix = pd.DataFrame(np.zeros((n, n)), columns=cols, index=cols)
    
    # Calculate correlation coefficients and p-values
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix.iloc[i, j] = 1.0
                p_value_matrix.iloc[i, j] = 0.0
            else:
                corr, p_value = pearsonr(df.iloc[:, i], df.iloc[:, j])
                corr_matrix.iloc[i, j] = corr
                p_value_matrix.iloc[i, j] = p_value
                
    return corr_matrix, p_value_matrix

# Calculate the correlation matrix and p-value matrix
corr_matrix, p_value_matrix = calculate_correlation_matrix(df)

# Display the matrices
print("Correlation Matrix:")
print(corr_matrix)
print("\nP-Value Matrix:")
print(p_value_matrix)

# Combine the correlation matrix and p-value matrix into one DataFrame for display
combined_matrix = corr_matrix.astype(str) + " (p=" + p_value_matrix.round(4).astype(str) + ")"

print("\nCombined Matrix:")
print(combined_matrix)
















import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load your dataset
df = pd.read_csv('your_dataset.csv')

# Identify the categorical variable with missing values
target_var = 'your_categorical_variable'

# Drop all columns with missing values except the target variable
columns_with_missing_values = df.columns[df.isna().any()].tolist()
columns_with_missing_values.remove(target_var)
df_clean = df.drop(columns=columns_with_missing_values)

# Ensure target variable is of integer type
df_clean[target_var] = df_clean[target_var].astype(int)

# Split the data into training, testing, and target (missing values) sets
df_not_missing = df_clean.dropna(subset=[target_var])
df_missing = df_clean[df_clean[target_var].isna()]

# Further split the not_missing data into training and testing sets
train, test = train_test_split(df_not_missing, test_size=0.2, random_state=42)

# Define the features and target
X_train = train.drop(columns=[target_var])
y_train = train[target_var].astype(int)
X_test = test.drop(columns=[target_var])
y_test = test[target_var].astype(int)

# Handle class imbalance using SMOTE with n_neighbors set to 1
smote = SMOTE(random_state=42, k_neighbors=1)

# Build the preprocessing step
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object', 'category']).columns)
    ],
    remainder='passthrough'  # Pass through the remaining numerical columns without transformation
)

# Define the models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Build the pipeline with SMOTE and preprocessor
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', smote),
        ('classifier', model)
    ])
    
    # Train the classifier on the training set
    pipeline.fit(X_train, y_train)
    
    # Validate the model on the testing set
    y_pred = pipeline.predict(X_test)
    print(f"Classification report for {model_name}:")
    print(classification_report(y_test, y_pred))
    
    # Impute the missing values in the target dataset
    X_missing = df_missing.drop(columns=[target_var])
    df_missing[target_var] = pipeline.predict(X_missing).astype(int)
    
    # Print the imputed values for the missing data
    print(f"Imputed missing values using {model_name}:")
    print(df_missing)

# Save the imputed dataset to a CSV file
df_filled = pd.concat([df_not_missing, df_missing])
df_filled.to_csv('filled_dataset.csv', index=False)
print("The dataset with imputed values has been saved to 'filled_dataset.csv'.")
