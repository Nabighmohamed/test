{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14ec69a",
   "metadata": {},
   "source": [
    "# Predicting Total PNB using XGBoost and AGBoost Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we aim to build predictive models for total PNB using XGBoost and AGBoost algorithms for the 'Rest' dataset. We will preprocess the data, handle outliers, split the data into training and testing sets, and compare the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6b09b",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "First, we need to install the necessary packages. If they are already installed, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AGBoost (requires authentication)\n",
    "!pip install agboost --no-cache-dir --extra-index-url https://${ARTIFACTORY_USERNAME}:${ARTIFACTORY_TOKEN}@repo.artifactory-dogen.group.echonet/artifactory/api/pypi/ap12261-pypi-local/simple\n",
    "\n",
    "# Install other required packages\n",
    "!pip install xgboost matplotlib scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf0c5a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import agboost as agb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e94800",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "We load the dataset and perform initial preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('df_rest.csv', sep=',', encoding='latin-1')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['pnb_annuel', 'pnb_an_rep', 'MACMPROF', 'CNOUVSEG', 'CTYPCLI', 'CENSEIGNE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3c6b3",
   "metadata": {},
   "source": [
    "## Handle Outliers and Data Cleaning\n",
    "\n",
    "We identify and remove outliers based on the Interquartile Range (IQR) method. We also transform certain variables to categorical types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "target = 'total_pnb'\n",
    "\n",
    "# Calculate the IQR to identify outliers\n",
    "Q1 = df[target].quantile(0.25)\n",
    "Q3 = df[target].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the bounds for identifying outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter outliers and negative target values\n",
    "outliers_negative = df[(df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0)]\n",
    "non_outliers_og = df[~((df[target] < lower_bound) | (df[target] > upper_bound) | (df[target] < 0))]\n",
    "\n",
    "# Remove entries with AGE less than 18\n",
    "non_outliers_og = non_outliers_og[non_outliers_og['AGE'] >= 18]\n",
    "\n",
    "# Keep entries with CA between 0 and 900,000\n",
    "non_outliers_og = non_outliers_og[(non_outliers_og['CA'] <= 900000) & (non_outliers_og['CA'] >= 0)]\n",
    "\n",
    "# Cap AGE at 70\n",
    "from random import randrange\n",
    "for i in non_outliers_og.index:\n",
    "    if non_outliers_og.loc[i, 'AGE'] > 70:\n",
    "        non_outliers_og.loc[i, 'AGE'] = randrange(70, 72)\n",
    "\n",
    "# Convert certain numerical variables to categorical\n",
    "non_outliers_og['CMOTENT1'] = non_outliers_og['CMOTENT1'].astype(str) + '_cat'\n",
    "non_outliers_og['Cartes de Paiement'] = non_outliers_og['Cartes de Paiement'].astype(str) + '_cat'\n",
    "non_outliers_og['carte_business'] = non_outliers_og['carte_business'].astype(str) + '_cat'\n",
    "non_outliers_og['crédit équipement'] = non_outliers_og['crédit équipement'].astype(str) + '_cat'\n",
    "\n",
    "# Update the dataset\n",
    "df = non_outliers_og.copy()\n",
    "X = df.copy()\n",
    "y = df['total_pnb']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b76506",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_df, test_df, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e18f84",
   "metadata": {},
   "source": [
    "## Data Preprocessing with AGBoost\n",
    "\n",
    "We preprocess the data using AGBoost's `DataPreprocessing` class, which includes imputing missing values and target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to use\n",
    "feat = ['MACMREP', 'CA', 'CMOTENT1', 'Cartes de Paiement', 'carte_business', 'crédit équipement', 'AGE']\n",
    "\n",
    "# Data preprocessing\n",
    "data_proc = agb.DataPreprocessing(\n",
    "    label_name='total_pnb',\n",
    "    feature_names=feat,\n",
    "    impute_na=True,\n",
    "    threshold_na=0.05,\n",
    "    target_encode=True,\n",
    "    threshold_cat=0,\n",
    "    encoding_algo=\"rank\"\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_proc = data_proc.fit_transform(data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0603f7b",
   "metadata": {},
   "source": [
    "## Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e26236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training set\n",
    "print(\"Missing values in the training set:\")\n",
    "print(train_proc.isna().sum())\n",
    "\n",
    "# View imputation table\n",
    "data_proc.imputation_table\n",
    "\n",
    "# Get updated feature names\n",
    "feature_names = data_proc.features_cache['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f1d34",
   "metadata": {},
   "source": [
    "## Transform the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "test_proc = data_proc.transform(data=test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b97e7b",
   "metadata": {},
   "source": [
    "## Benchmark Model: XGBoost\n",
    "\n",
    "We train an XGBoost model as a benchmark to compare against the AGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1932561",
   "metadata": {},
   "source": [
    "### Prepare Data for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df = train_proc\n",
    "df_ohe = pd.get_dummies(df[feature_names], prefix_sep=\"_ohe_\")\n",
    "df_xgb = xgb.DMatrix(data=df_ohe.values, label=df['total_pnb'].values, feature_names=list(df_ohe.columns))\n",
    "train_xgb = df_xgb\n",
    "\n",
    "df = test_proc\n",
    "df_ohe = pd.get_dummies(df[feature_names], prefix_sep=\"_ohe_\")\n",
    "df_xgb = xgb.DMatrix(data=df_ohe.values, label=df['total_pnb'].values, feature_names=list(df_ohe.columns))\n",
    "test_xgb = df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1342e4",
   "metadata": {},
   "source": [
    "### Cross-Validation to Find Optimal Number of Boosting Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5443fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Hyperparameters\n",
    "e, s, md, boos = 0.02, 0.8, 3, 1000  # Hyperparameters found by AGBoost\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': e,\n",
    "    'seed': 1989,\n",
    "    'subsample': s,\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': md,\n",
    "    'colsample_bytree': 1\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "xgb_cv = xgb.cv(\n",
    "    params=xgb_params,\n",
    "    dtrain=train_xgb,\n",
    "    num_boost_round=boos,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=0\n",
    ")\n",
    "\n",
    "# Plot RMSE vs. number of boosting rounds\n",
    "xgb_cv.iloc[:, [0, 2]].plot(xlabel='Number of Boosting Rounds', ylabel='RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff347b05",
   "metadata": {},
   "source": [
    "### Train the Optimized XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal number of trees\n",
    "best_ntrees = agb.find_best_ntrees(eval_history_df=xgb_cv, maximize=False)\n",
    "print(f\"Optimal number of trees: {best_ntrees}\")\n",
    "\n",
    "# Train the final XGBoost model\n",
    "xgb_final = xgb.train(params=xgb_params, dtrain=train_xgb, num_boost_round=best_ntrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc1cc1",
   "metadata": {},
   "source": [
    "### Evaluate the XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb240c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "df = train_df\n",
    "y_actual_train = df['total_pnb']\n",
    "y_pred_train = xgb_final.predict(data=train_xgb)\n",
    "rmse_xgb_train = mean_squared_error(y_actual_train, y_pred_train, squared=False)\n",
    "\n",
    "# Evaluate on test set\n",
    "df = test_df\n",
    "y_actual_test = df['total_pnb']\n",
    "y_pred_test = xgb_final.predict(data=test_xgb)\n",
    "rmse_xgb_test = mean_squared_error(y_actual_test, y_pred_test, squared=False)\n",
    "r2_xgb_test = r2_score(y_actual_test, y_pred_test)\n",
    "\n",
    "# Compile model scores\n",
    "score = pd.DataFrame({'model': ['XGBoost'], 'RMSE train': [rmse_xgb_train], 'RMSE test': [rmse_xgb_test], 'r2 test:': [r2_xgb_test]})\n",
    "model_scores = score\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2068b3d",
   "metadata": {},
   "source": [
    "## Analyze Residuals for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6540820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_actual_test - y_pred_test\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "data = {'res': residuals, 'Actual': y_actual_test, 'Predicted': y_pred_test}\n",
    "dff = pd.DataFrame(data)\n",
    "\n",
    "# Plot residuals and predictions\n",
    "plt.figure(figsize=(28, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot(dff['res'], shade=True)\n",
    "plt.title('Residuals Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"res\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Residuals vs. Actual')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"Predicted\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Predicted vs. Actual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19adcd2",
   "metadata": {},
   "source": [
    "## AGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c4324",
   "metadata": {},
   "source": [
    "### Initialize AGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20924031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGBoost hyperparameters\n",
    "e, s, md, boos = 0.02, 0.8, 3, 1000\n",
    "\n",
    "params_xgbooster = {\n",
    "    'eta': e,\n",
    "    'seed': 1989,\n",
    "    'subsample': 0.8,\n",
    "    'booster': 'gbtree',\n",
    "    'min_child_weight': 50,\n",
    "    'colsample_bytree': 1\n",
    "}\n",
    "\n",
    "# Initialize AGBoost model\n",
    "agbooster = agb.AGBooster(\n",
    "    label_name='total_pnb',\n",
    "    feature_names=feature_names,\n",
    "    params_xgbooster=params_xgbooster,\n",
    "    prefix_sep=\"_ohe_\",\n",
    "    target_encoding_dict=data_proc.target_encoding_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e53e3",
   "metadata": {},
   "source": [
    "### Feature Selection with AGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select individual features\n",
    "agbooster.select_features(\n",
    "    data=train_proc,\n",
    "    cv_imp=True,\n",
    "    threshold=5,  # Note: Threshold is set high for illustration\n",
    "    num_boost_round_min=51,\n",
    "    num_boost_round_max=200,\n",
    "    num_boost_round_start=200,\n",
    "    early_stopping_rounds=10,\n",
    "    eta_start=0.1\n",
    ")\n",
    "\n",
    "# View feature importance\n",
    "agbooster.feature_importance['importance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf0e56",
   "metadata": {},
   "source": [
    "### Update Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1053d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the list of selected features with a new threshold\n",
    "agbooster.update_selected_features(threshold=0)\n",
    "agbooster.selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d47bb",
   "metadata": {},
   "source": [
    "### Fit Univariate GAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a86a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit univariate GAM\n",
    "agbooster.fit_univariate_gam(\n",
    "    data=train_proc,\n",
    "    num_boost_round_min=51,\n",
    "    num_boost_round_max=200,\n",
    "    num_boost_round_start=200,\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f536c1d",
   "metadata": {},
   "source": [
    "## Evaluate AGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ab27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "df = train_proc\n",
    "y_actual_train = df['total_pnb']\n",
    "y_pred_train = agbooster.gam_univariate_model.predict(data=df)\n",
    "rmse_gam_univar_train = mean_squared_error(y_actual_train, y_pred_train, squared=False)\n",
    "\n",
    "# Evaluate on test set\n",
    "df = test_proc\n",
    "y_actual_test = df['total_pnb']\n",
    "y_pred_test = agbooster.gam_univariate_model.predict(data=df)\n",
    "rmse_gam_univar_test = mean_squared_error(y_actual_test, y_pred_test, squared=False)\n",
    "r2_gam_univar_test = r2_score(y_actual_test, y_pred_test)\n",
    "\n",
    "# Update model scores\n",
    "score = pd.DataFrame({'model': ['GAM Univariate'], 'RMSE train': [rmse_gam_univar_train], 'RMSE test': [rmse_gam_univar_test], 'r2 test:': [r2_gam_univar_test]})\n",
    "model_scores = pd.concat([model_scores, score], ignore_index=True)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a64560",
   "metadata": {},
   "source": [
    "## Analyze Residuals for AGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_actual_test - y_pred_test\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "data = {'res': residuals, 'Actual': y_actual_test, 'Predicted': y_pred_test}\n",
    "dff = pd.DataFrame(data)\n",
    "\n",
    "# Plot residuals and predictions\n",
    "plt.figure(figsize=(28, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot(dff['res'], shade=True)\n",
    "plt.title('Residuals Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"res\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Residuals vs. Actual')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=dff, x=\"Actual\", y=\"Predicted\", hue=\"res\", size=np.abs(dff['res']))\n",
    "plt.title('Predicted vs. Actual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a6160",
   "metadata": {},
   "source": [
    "## Extract Model Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d44c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the GAM model\n",
    "model = agbooster.gam_univariate_model\n",
    "\n",
    "# Create a table of coefficients\n",
    "table = pd.DataFrame([{\n",
    "    'variable': 'Intercept',\n",
    "    'group': '',\n",
    "    'coef': model.gam_model['intercept'],\n",
    "    'nb': '',\n",
    "    'min': '',\n",
    "    'max': ''\n",
    "}])\n",
    "\n",
    "# Append coefficients for each feature\n",
    "for feature in model.gam_model[\"coefs_stump\"].keys():\n",
    "    table_int = pd.DataFrame(model.gam_model[\"coefs_stump\"][feature][['group', 'coef', 'nb', 'min', 'max']])\n",
    "    table_int['variable'] = feature\n",
    "    table = pd.concat([table, table_int], axis=0)\n",
    "\n",
    "# Rearrange columns\n",
    "table = table[['variable', 'group', 'coef', 'nb', 'min', 'max']]\n",
    "\n",
    "print(f\"Total number of coefficients: {table.shape[0]}\")\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
