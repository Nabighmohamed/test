import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())

# Split the data into features and target
X = df[['Value']]
y = df['Adjusted']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train XGBoost model
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate the model
y_train_pred = xgb_reg.predict(X_train)
y_test_pred = xgb_reg.predict(X_test)

mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f'XGBoost - MSE Train: {mse_train}, MSE Test: {mse_test}, R^2 Train: {r2_train}, R^2 Test: {r2_test}')

# Plotting train vs test results
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], '--r')
plt.xlabel('Actual Values (Train)')
plt.ylabel('Predicted Values (Train)')
plt.title('XGBoost - Train')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
plt.xlabel('Actual Values (Test)')
plt.ylabel('Predicted Values (Test)')
plt.title('XGBoost - Test')

plt.suptitle('Actual vs Predicted Values (XGBoost)')
plt.show()








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'Value': np.random.randn(100) * 20  # Generating a dataset with potential outliers
})

# Introduce extreme outliers
df.loc[::10, 'Value'] = df['Value'] * 100

# Function to apply Isolation Forest and adjust outliers
def apply_isolation_forest(df, contamination, n_estimators):
    iso_forest = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=0)
    df['Outlier'] = iso_forest.fit_predict(df[['Value']])
    df['Outlier'] = df['Outlier'].apply(lambda x: 'Outlier' if x == -1 else 'Inlier')
    return df

# Apply Isolation Forest
contamination = 0.1
n_estimators = 100
df = apply_isolation_forest(df, contamination, n_estimators)

# Calculate the replacement values
mean_value = df[df['Outlier'] == 'Inlier']['Value'].mean()
max_value = df[df['Outlier'] == 'Inlier']['Value'].max()
quantile_5th = df[df['Outlier'] == 'Inlier']['Value'].quantile(0.05)

# Replace outliers based on specified rules
def replace_outliers(row, mean_value, max_value, quantile_5th):
    if row['Outlier'] == 'Outlier':
        noise = np.random.normal(0, 1)
        if row['Value'] > mean_value:
            return 0.1 * max_value + noise
        else:
            return quantile_5th + noise
    else:
        return row['Value']

df['Adjusted'] = df.apply(lambda row: replace_outliers(row, mean_value, max_value, quantile_5th), axis=1)

# Plot original data with outliers highlighted
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x=range(len(df)), y='Value', hue='Outlier', palette='tab10', legend='full')
plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Index')
plt.ylabel('Value')
plt.show()

# Plot original and adjusted data distributions
plt.figure(figsize=(12, 8))
sns.displot(df, x='Value', hue='Outlier', kind='kde', fill=True, height=5, aspect=2)
plt.title('Original Data with Outliers Highlighted')

sns.displot(df, x='Adjusted', kind='kde', fill=True, height=5, aspect=2)
plt.title('Data after Outlier Adjustment')

plt.show()

# Show the impact of adjusting outliers
print("Original Data:")
print(df['Value'].describe())
print("\nAdjusted Data:")
print(df['Adjusted'].describe())












import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor, plot_tree
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree and plot the tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group

        # Plot the Decision Tree
        plt.figure(figsize=(12, 8))
        plot_tree(dt_reg, feature_names=[cat_var], filled=True, rounded=True, fontsize=10)
        plt.title(f'Decision Tree for {cat_var}')
        plt.show()
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()









import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame creation for demonstration
np.random.seed(0)
df = pd.DataFrame({
    'CatVar1': np.random.choice(['A', 'B', 'C', np.nan], size=100),
    'CatVar2': np.random.choice(['X', 'Y', 'Z', np.nan], size=100),
    'Target': np.random.uniform(0, 100, size=100)
})

# Replace missing values with a new category "missing" and convert all to strings
df.fillna('missing', inplace=True)
df['CatVar1'] = df['CatVar1'].astype(str)
df['CatVar2'] = df['CatVar2'].astype(str)

# Function to group using Decision Tree
def group_classes_with_decision_tree(df, cat_vars, target_var):
    group_mappings = {}
    for cat_var in cat_vars:
        le = LabelEncoder()
        df[f'{cat_var}_encoded'] = le.fit_transform(df[cat_var])
        X = df[[f'{cat_var}_encoded']]
        y = df[target_var]
        
        dt_reg = DecisionTreeRegressor(max_depth=3, random_state=0)
        dt_reg.fit(X, y)
        
        # Predict the leaves for each sample
        leaf_ids = dt_reg.apply(X).reshape(-1)  # Flatten to 1D array
        unique_leaf_ids = np.unique(leaf_ids)
        leaf_id_to_group = {leaf_id: idx for idx, leaf_id in enumerate(unique_leaf_ids)}
        
        df[f'Grouped_{cat_var}'] = pd.Series(leaf_ids).map(leaf_id_to_group)
        
        # Create a mapping from original categories to new groups
        original_to_group = {}
        for category in le.classes_:
            category_encoded = le.transform([category])[0]
            group = df[df[f'{cat_var}_encoded'] == category_encoded][f'Grouped_{cat_var}'].mode()[0]
            original_to_group[category] = group
        
        group_mappings[cat_var] = original_to_group
        
    return df, group_mappings

# Apply Decision Tree grouping
cat_vars = ['CatVar1', 'CatVar2']
df_grouped, group_mappings = group_classes_with_decision_tree(df, cat_vars, 'Target')

# Display the grouped DataFrame
print(df_grouped.head())

# Display the mappings from original to new groups
for cat_var, mapping in group_mappings.items():
    print(f'\nMapping for {cat_var}:')
    groups = {}
    for original, group in mapping.items():
        if group not in groups:
            groups[group] = []
        groups[group].append(original)
    for group, categories in groups.items():
        print(f'  Group {group + 1}: {categories}')

# Plot the distribution of the target variable by the grouped categories
for cat_var in cat_vars:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=f'Grouped_{cat_var}', y='Target', data=df_grouped)
    plt.title(f'Distribution of Target by Grouped {cat_var}')
    plt.show()
